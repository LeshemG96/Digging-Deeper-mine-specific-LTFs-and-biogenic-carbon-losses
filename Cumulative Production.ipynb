{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df76bdf",
   "metadata": {},
   "source": [
    "Outlier detection   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3df5ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leshe\\AppData\\Local\\Temp\\ipykernel_21740\\1718636777.py:63: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "TARGET = 'Production'\n",
    "ID_COL = 'PROP_ID'\n",
    "NAME_COL = 'PROP_NAME'\n",
    "START_COL = 'START_UP_YR'\n",
    "CLOSE_COL = 'PROJ_CLOSURE_YR'\n",
    "YEAR_COL = 'Year'\n",
    "SAVE_DIR = 'Visualizations/Commodity Production/'\n",
    "\n",
    "OUTLIER_THRESHOLD = 1.2\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# === FUNCTIONS ===\n",
    "def filter_by_lifetime(df):\n",
    "    return df[(df[YEAR_COL] >= df[START_COL]) & (df[YEAR_COL] <= df[CLOSE_COL])]\n",
    "\n",
    "def dynamic_window(series, min_window=3, max_window=10, fraction=0.4):\n",
    "    length = len(series)\n",
    "    return max(min_window, min(int(length * fraction), max_window))\n",
    "\n",
    "def detect_and_replace_outliers(series, threshold=1.2):\n",
    "    window = dynamic_window(series)\n",
    "    rolling_mean = series.rolling(window=window, center=True, min_periods=1).mean()\n",
    "    rolling_std = series.rolling(window=window, center=True, min_periods=1).std()\n",
    "    outliers = (np.abs(series - rolling_mean) > threshold * rolling_std)\n",
    "    cleaned = series.copy()\n",
    "    cleaned[outliers] = rolling_mean[outliers]\n",
    "    return cleaned, outliers\n",
    "\n",
    "def generate_full_lifetime_heatmap(df):\n",
    "    mine_lifetimes = df[[ID_COL, START_COL, CLOSE_COL]].drop_duplicates()\n",
    "    mine_lifetimes['Year_range'] = mine_lifetimes.apply(\n",
    "        lambda row: list(range(row[START_COL], row[CLOSE_COL] + 1)), axis=1\n",
    "    )\n",
    "    df_lifetime_full = mine_lifetimes.explode('Year_range').rename(columns={'Year_range': 'Year'})\n",
    "    df_lifetime_full['Year'] = df_lifetime_full['Year'].astype(int)\n",
    "\n",
    "    df_renamed = df.rename(columns={YEAR_COL: 'Prod_Year'})\n",
    "    df_expanded = pd.merge(\n",
    "        df_lifetime_full,\n",
    "        df_renamed[[ID_COL, 'Prod_Year', 'Production_Original']],\n",
    "        left_on=[ID_COL, 'Year'],\n",
    "        right_on=[ID_COL, 'Prod_Year'],\n",
    "        how='left'\n",
    "    ).drop(columns=['Prod_Year'])\n",
    "\n",
    "    df_expanded['Missing'] = df_expanded['Production_Original'].isna().astype(int)\n",
    "    df_pivot = df_expanded.pivot_table(index=ID_COL, columns='Year', values='Missing', fill_value=-1)\n",
    "    df_melt = df_pivot.reset_index().melt(id_vars=ID_COL, var_name='Year', value_name='Status')\n",
    "    df_melt['Category'] = df_melt['Status'].map({-1: 'Not Applicable', 0: 'Value Present', 1: 'Missing'})\n",
    "\n",
    "    sample_ids_heatmap = df_melt[ID_COL].unique()[:10]\n",
    "    df_heatmap_pivot = df_melt[df_melt[ID_COL].isin(sample_ids_heatmap)].pivot(\n",
    "        index=ID_COL, columns='Year', values='Category'\n",
    "    )\n",
    "    category_to_num = {'Not Applicable': 0, 'Missing': 1, 'Value Present': 2}\n",
    "    heatmap_matrix = df_heatmap_pivot.replace(category_to_num)\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.heatmap(\n",
    "        heatmap_matrix,\n",
    "        cmap=sns.color_palette([\"white\", \"#c51b7d\", \"#4d9221\"]),\n",
    "        cbar_kws={'ticks': [0.5, 1.5, 2.5], 'format': '%d'},\n",
    "        linewidths=0.1,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "    plt.title(\"Heatmap of Missing Data Categories (Full Mine Lifetime, Sample Mines)\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Mine ID\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"heatmap_missing_values_full_lifetime.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def generate_completeness_summary(df_full_lifetime, outlier_counts):\n",
    "    mine_lifetimes = df_full_lifetime[[ID_COL, START_COL, CLOSE_COL]].drop_duplicates()\n",
    "    mine_lifetimes['Year_range'] = mine_lifetimes.apply(\n",
    "        lambda row: list(range(row[START_COL], row[CLOSE_COL] + 1)), axis=1\n",
    "    )\n",
    "    df_lifetime_full = mine_lifetimes.explode('Year_range').rename(columns={'Year_range': 'Year'})\n",
    "    df_lifetime_full['Year'] = df_lifetime_full['Year'].astype(int)\n",
    "\n",
    "    df_renamed = df_full_lifetime.rename(columns={YEAR_COL: 'Prod_Year'})\n",
    "    df_expanded = pd.merge(\n",
    "        df_lifetime_full,\n",
    "        df_renamed[[ID_COL, 'Prod_Year', 'Production_Original']],\n",
    "        left_on=[ID_COL, 'Year'],\n",
    "        right_on=[ID_COL, 'Prod_Year'],\n",
    "        how='left'\n",
    "    ).drop(columns=['Prod_Year'])\n",
    "\n",
    "    completeness = (\n",
    "        df_expanded.groupby(ID_COL)['Production_Original']\n",
    "        .apply(lambda x: x.notna().sum() / len(x))\n",
    "        .reset_index(name='Completeness')\n",
    "    )\n",
    "    completeness = completeness.merge(outlier_counts, on=ID_COL, how='left')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(completeness['Completeness'], bins=30, kde=False, color=\"steelblue\")\n",
    "    plt.axvline(0.25, color=\"red\", linestyle=\"--\")\n",
    "    plt.axvline(0.5, color=\"red\", linestyle=\"--\")\n",
    "    plt.axvline(0.75, color=\"red\", linestyle=\"--\")\n",
    "    plt.title(\"Histogram of Production Data Completeness per Mine\")\n",
    "    plt.xlabel(\"Proportion of Non-Missing Data\")\n",
    "    plt.ylabel(\"Number of Mines\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"histplot_completeness.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    completeness.to_excel(\"Data Output/Commodity Production/Completeness_Summary.xlsx\", index=False)\n",
    "\n",
    "def plot_timeseries_with_outliers(df, id_list, original_col='Production_Original', cleaned_col='Production'):\n",
    "    sample = df[df[ID_COL].isin(id_list)]\n",
    "    g = sample.groupby(ID_COL)\n",
    "    n = len(id_list)\n",
    "    ncols = 4\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 4), sharex=False, sharey=False)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, (mine_id, group) in enumerate(g):\n",
    "        ax = axs[i]\n",
    "        ax.plot(group[YEAR_COL], group[original_col], label=\"Original\", marker='o', linestyle='-')\n",
    "        ax.plot(group[YEAR_COL], group[cleaned_col], label=\"Cleaned\", marker='x', linestyle='--')\n",
    "        ax.set_title(f\"Mine {mine_id}\")\n",
    "        ax.set_xlabel(\"Year\")\n",
    "        ax.set_ylabel(\"Production\")\n",
    "        ax.legend()\n",
    "\n",
    "    for j in range(i+1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"timeseries_outliers_grid.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    path = 'Data Output/Commodity Production/Commodity_Production-1980_2023_long.xlsx'\n",
    "    df = pd.read_excel(path)\n",
    "\n",
    "    # Rename original Production column to Production_Original at the start\n",
    "    df = df.rename(columns={'Production': 'Production_Original'})\n",
    "    \n",
    "    # Update TARGET to use the original column for filtering and processing\n",
    "    TARGET_ORIGINAL = 'Production_Original'\n",
    "\n",
    "    # Filter by mine lifetime\n",
    "    df_filtered = filter_by_lifetime(df)\n",
    "\n",
    "    # Clean and detect outliers using dynamic window\n",
    "    df_outliers = df_filtered[(df_filtered[TARGET_ORIGINAL].notna()) & (df_filtered[TARGET_ORIGINAL] > 0)]\n",
    "    df_outliers = df_outliers.groupby(ID_COL).filter(lambda x: (x[TARGET_ORIGINAL].notna() & (x[TARGET_ORIGINAL] > 0)).sum() >= 5)\n",
    "    df_cleaned = df_outliers.copy()\n",
    "    df_cleaned = df_cleaned.sort_values([ID_COL, YEAR_COL])\n",
    "\n",
    "    cleaned_list = []\n",
    "    outlier_list = []\n",
    "\n",
    "    for mine_id, group in df_cleaned.groupby(ID_COL):\n",
    "        series = group[TARGET_ORIGINAL]\n",
    "        cleaned, outliers = detect_and_replace_outliers(series, threshold=OUTLIER_THRESHOLD)\n",
    "        cleaned_list.extend(cleaned)\n",
    "        outlier_list.extend(outliers)\n",
    "\n",
    "    # Create the new Production column with cleaned data\n",
    "    df_cleaned['Production'] = cleaned_list\n",
    "    df_cleaned['Outlier'] = outlier_list\n",
    "\n",
    "    # Save cleaned file\n",
    "    df_cleaned.to_excel(\"Data Output/Commodity Production/Commodity_Production-1980_2023_long_final.xlsx\", index=False)\n",
    "\n",
    "    # Generate outlier summary\n",
    "    outlier_counts = df_cleaned.groupby(ID_COL)['Outlier'].sum().reset_index(name='Num_Outliers')\n",
    "\n",
    "    # Visualizations\n",
    "    sample_ids = df_cleaned[ID_COL].unique()[:20]\n",
    "    plot_timeseries_with_outliers(df_cleaned, sample_ids)\n",
    "    generate_full_lifetime_heatmap(df)\n",
    "    generate_completeness_summary(df, outlier_counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe7ce5",
   "metadata": {},
   "source": [
    "Hubbert and Femp imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "96f9f268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting models to production data: 100%|██████████| 723/723 [01:17<00:00,  9.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "############################################################Parameters############################################################\n",
    "# Changed to only include 'production' as the target variable\n",
    "targets = ['Production']\n",
    "\n",
    "lower_bounds = {\n",
    "    'hubbert': (0, 0, 0),  # log-transformed L and k\n",
    "    'femp': (0, 0)  # log-transformed R0, logit-transformed C\n",
    "}\n",
    "\n",
    "upper_bounds = {\n",
    "    'hubbert': (np.inf, np.inf, np.inf),\n",
    "    'femp': (np.inf, 1)  # No upper bound needed for log-transformed variables\n",
    "}\n",
    "min_sample_size = 5\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "############################################################Functions############################################################\n",
    "def safe_exp(x):\n",
    "    try:\n",
    "        return np.exp(x)\n",
    "    except OverflowError:\n",
    "        return np.inf  # Or a reasonable max value\n",
    "    \n",
    "def hubbert_transformed(t, log_L, log_k, t0):\n",
    "    \"\"\"\n",
    "    Hubbert model with log-transformed parameters.\n",
    "    \n",
    "    Parameters:\n",
    "        t (np.array): Time\n",
    "        log_L (float): Log of maximum production\n",
    "        log_k (float): Log of growth rate\n",
    "        t0 (float): Time of peak production\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Production at time\n",
    "    \"\"\"\n",
    "    L = safe_exp(log_L)  # Ensure L is strictly positive\n",
    "    k = safe_exp(log_k)  # Ensure k is strictly positive\n",
    "    return L / (1 + np.exp(-k * (t - t0)))\n",
    "\n",
    "def femp_transformed(t, log_R0, C_transformed):\n",
    "    \"\"\"\n",
    "    FEMP model with log and logit-transformed parameters.\n",
    "    \n",
    "    Parameters:\n",
    "        t (np.array): Time\n",
    "        log_R0 (float): Log of initial reserves\n",
    "        C_transformed (float): Logit of production-to-reserve ratio\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Production at time\n",
    "    \"\"\"\n",
    "    R0 = safe_exp(log_R0)  # Ensure R0 is strictly positive\n",
    "    C = 1 / (1 + safe_exp(-C_transformed))  # Ensure C stays between 0 and 1\n",
    "    return R0 * (1 - (1 - C)**t)\n",
    "\n",
    "def hubbert_deriv_transformed(t, log_L, log_k, t0):\n",
    "    \"\"\"\n",
    "    Hubbert model derivative with log-transformed parameters.\n",
    "    \"\"\"\n",
    "    L = safe_exp(log_L)  # Ensure L is strictly positive\n",
    "    k = safe_exp(log_k)  # Ensure k is strictly positive\n",
    "    return L * k * np.exp(-k * (t - t0)) / (1 + np.exp(-k * (t - t0)))**2\n",
    "\n",
    "def femp_deriv_transformed(t, log_R0, C_transformed):\n",
    "    \"\"\"\n",
    "    FEMP model derivative with log and logit-transformed parameters.\n",
    "    \"\"\"\n",
    "    R0 = safe_exp(log_R0)  # Ensure R0 is strictly positive\n",
    "    C = 1 / (1 + safe_exp(-C_transformed))  # Ensure C stays between 0 and 1\n",
    "    return -R0 * np.log(1 - C) * (1 - C)**t\n",
    "\n",
    "def hubbert_model(t, L, k, t0):\n",
    "    '''\n",
    "    Hubbert model for production\n",
    "    \n",
    "    Parameters:\n",
    "        t (np.array): Time\n",
    "        L (float): Maximum production\n",
    "        k (float): Growth rate\n",
    "        t0 (float): Time of peak production\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Production at time\n",
    "    \n",
    "    '''\n",
    "    return L / (1 + np.exp(-k * (t - t0)))\n",
    "\n",
    "def femp(t, R0, C):\n",
    "    '''\n",
    "    Exponential model for production\n",
    "    \n",
    "    Parameters:\n",
    "        t (np.array): Time\n",
    "        R0 (float): Initial reserves\n",
    "        C (float): Production to reserve ratio\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Production at time\n",
    "    \n",
    "    '''\n",
    "    return R0 * (1-(1-C)**t)\n",
    "\n",
    "def femp_deriv(t, R0, C):\n",
    "    '''\n",
    "    Derivative of the exponential model for production\n",
    "    \n",
    "    Parameters:\n",
    "        t (np.array): Time\n",
    "        R0 (float): Initial reserves\n",
    "        C (float): Production to reserve ratio\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Production at time\n",
    "    \n",
    "    '''\n",
    "    return - R0 * np.log(1-C) * (1-C)**t\n",
    "\n",
    "def hubbert_deriv(t, L, k, t0):\n",
    "    '''\n",
    "    Derivative of the Hubbert model for production\n",
    "    \n",
    "    Parameters:\n",
    "        t (np.array): Time\n",
    "        L (float): Maximum production\n",
    "        k (float): Growth rate\n",
    "        t0 (float): Time of peak production\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Production at time\n",
    "    \n",
    "    '''\n",
    "    return L * k * safe_exp(-k * (t - t0)) / (1 + safe_exp(-k * (t - t0)))**2\n",
    "\n",
    "def prep_init_guesses(model_name, sample, t):\n",
    "    if model_name == 'hubbert':\n",
    "        L_guess = sample[t].cumsum().max()  \n",
    "        k_guess = sample[t].mean() / L_guess  \n",
    "        t0_guess = sample.loc[sample[t].idxmax(), 'Year_diff'].astype(int) \n",
    "        return (L_guess, k_guess, t0_guess)\n",
    "\n",
    "    elif model_name == 'femp':\n",
    "        R0_guess = sample[t].cumsum().max()\n",
    "        C_guess = sample[t].mean() / R0_guess\n",
    "        return (R0_guess, C_guess)\n",
    "\n",
    "def fit_prod_model(mine, prod_data, models, targets, lower_bounds, upper_bounds, min_sample_size):\n",
    "    results = []\n",
    "    data_records = []\n",
    "\n",
    "    for t in targets:\n",
    "        sample = prod_data.dropna(subset=t)\n",
    "\n",
    "        sample_size = len(sample)\n",
    "        if sample_size < min_sample_size:\n",
    "            continue\n",
    "\n",
    "        # Get the projected closure year and startup year for this mine\n",
    "        proj_closure_yr = sample['PROJ_CLOSURE_YR'].iloc[0]\n",
    "        start_up_yr = sample['START_UP_YR'].iloc[0]\n",
    "        \n",
    "        # Calculate mine lifetime using closure year\n",
    "        mine_lifetime = int(proj_closure_yr - start_up_yr)\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            years = sample['Year_diff']\n",
    "            init_guess = prep_init_guesses(model_name, sample, t)\n",
    "\n",
    "            try:\n",
    "                # Fit model with transformed parameters\n",
    "                popt, pcov = curve_fit(\n",
    "                    model,\n",
    "                    years.astype(int),\n",
    "                    sample[t],\n",
    "                    p0=init_guess,\n",
    "                    maxfev=10000,\n",
    "                    bounds=(lower_bounds[model_name], upper_bounds[model_name])\n",
    "                )\n",
    "\n",
    "                perr = np.sqrt(np.diag(pcov))  # Standard errors\n",
    "\n",
    "                # Transform parameters back to original scale\n",
    "                if model_name == 'hubbert':\n",
    "                    L_est, k_est, t0_est = popt[0], popt[1], popt[2]\n",
    "                    L_err, k_err, t0_err = perr[0], perr[1], perr[2]\n",
    "\n",
    "                else:  # FEMP model\n",
    "                    R0_est, C_est = popt[0], popt[1]\n",
    "                    R0_err, C_err = perr[0], perr[1]\n",
    "\n",
    "                # Predictions on actual data points\n",
    "                pred = model(years.astype(int), *popt)\n",
    "\n",
    "                # p-values\n",
    "                t_stats = popt / perr\n",
    "                p_values = [2 * (1 - stats.t.cdf(np.abs(t), sample_size - len(popt))) for t in t_stats]\n",
    "                \n",
    "                r2 = np.corrcoef(sample[t], pred)[0, 1] ** 2\n",
    "                rmse = np.sqrt(np.mean((sample[t] - pred) ** 2))\n",
    "                nrmse = rmse / sample[t].max()\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'PROP_ID': mine,\n",
    "                    'Target_var': t,\n",
    "                    'Model': model_name,\n",
    "                    'START_UP_YR': start_up_yr,\n",
    "                    'PROJ_CLOSURE_YR': proj_closure_yr,\n",
    "                    'Mine_Lifetime': mine_lifetime,\n",
    "                    'R2': r2,\n",
    "                    'RMSE': rmse,\n",
    "                    'NRMSE': nrmse,\n",
    "                    'P1_value': L_est if model_name == 'hubbert' else R0_est,\n",
    "                    'P2_value': k_est if model_name == 'hubbert' else C_est,\n",
    "                    'P3_value': t0_est if model_name == 'hubbert' else np.nan,\n",
    "                    'P1_err': L_err if model_name == 'hubbert' else R0_err,\n",
    "                    'P2_err': k_err if model_name == 'hubbert' else C_err,\n",
    "                    'P3_err': t0_err if model_name == 'hubbert' else np.nan,\n",
    "                    'P1_pval': p_values[0],\n",
    "                    'P2_pval': p_values[1],\n",
    "                    'P3_pval': p_values[2] if model_name == 'hubbert' else np.nan\n",
    "                })\n",
    "\n",
    "                # Collect actual observed data and predictions\n",
    "                obs_data_records = pd.DataFrame({\n",
    "                    'PROP_ID': mine,\n",
    "                    'Year_diff': years,\n",
    "                    'Year': years + start_up_yr,\n",
    "                    'Target_var': t,\n",
    "                    'START_UP_YR': start_up_yr,\n",
    "                    'PROJ_CLOSURE_YR': proj_closure_yr,\n",
    "                    'Observed': sample[t],\n",
    "                    'Predicted': pred,\n",
    "                    'Residual': sample[t] - pred,\n",
    "                    'Model': model_name,\n",
    "                    'Data_Type': 'Observed'\n",
    "                })\n",
    "                \n",
    "                # Add predictions up to closure year\n",
    "                # Create prediction data for all years up to closure\n",
    "                full_years = np.arange(0, mine_lifetime + 1)\n",
    "                \n",
    "                # Filter out years that are already in the observed data\n",
    "                future_years = np.setdiff1d(full_years, years)\n",
    "                \n",
    "                if len(future_years) > 0:\n",
    "                    future_pred = model(future_years, *popt)\n",
    "                    \n",
    "                    # Create dataframe for future predictions\n",
    "                    future_data_records = pd.DataFrame({\n",
    "                        'PROP_ID': mine,\n",
    "                        'Year_diff': future_years,\n",
    "                        'Year': future_years + start_up_yr,\n",
    "                        'Target_var': t,\n",
    "                        'START_UP_YR': start_up_yr,\n",
    "                        'PROJ_CLOSURE_YR': proj_closure_yr,\n",
    "                        'Observed': np.nan,\n",
    "                        'Predicted': future_pred,\n",
    "                        'Residual': np.nan,\n",
    "                        'Model': model_name,\n",
    "                        'Data_Type': 'Predicted_Future'\n",
    "                    })\n",
    "                    \n",
    "                    # Combine observed and future predictions\n",
    "                    all_data_records = pd.concat([obs_data_records, future_data_records])\n",
    "                else:\n",
    "                    all_data_records = obs_data_records\n",
    "                \n",
    "                data_records.append(all_data_records)\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                # Handle fitting failures\n",
    "                print(f\"Fitting error for mine {mine}, target {t}, model {model_name}: {e}\")\n",
    "                results.append({\n",
    "                    'PROP_ID': mine,\n",
    "                    'Target_var': t,\n",
    "                    'Model': model_name,\n",
    "                    'START_UP_YR': start_up_yr,\n",
    "                    'PROJ_CLOSURE_YR': proj_closure_yr,\n",
    "                    'Mine_Lifetime': mine_lifetime,\n",
    "                    'R2': np.nan,\n",
    "                    'RMSE': np.nan,\n",
    "                    'NRMSE': np.nan,\n",
    "                    'P1_value': np.nan,\n",
    "                    'P2_value': np.nan,\n",
    "                    'P3_value': np.nan,\n",
    "                    'P1_err': np.nan,\n",
    "                    'P2_err': np.nan,\n",
    "                    'P3_err': np.nan,\n",
    "                    'P1_pval': np.nan,\n",
    "                    'P2_pval': np.nan,\n",
    "                    'P3_pval': np.nan\n",
    "                })\n",
    "\n",
    "    return results, data_records\n",
    "\n",
    "def prep_data(file_path):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the production data Excel file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Prepared production data\n",
    "    \"\"\"\n",
    "    prod_data = pd.read_excel(file_path)\n",
    "    \n",
    "    # Calculate year difference\n",
    "    if 'Year' in prod_data.columns:\n",
    "        prod_data['Year_diff'] = prod_data['Year'] - prod_data['START_UP_YR']\n",
    "    else:\n",
    "        # If no Year column, assuming sequential years are provided\n",
    "        prod_data['Year_diff'] = prod_data.groupby('PROP_ID').cumcount()\n",
    "    \n",
    "    assert prod_data['Year_diff'].min() >= 0, 'Negative year difference'\n",
    "    \n",
    "    return prod_data\n",
    "\n",
    "############################################################Main############################################################\n",
    "def main_fitting_loop(data_path, output_dir='Data Output/Cumulative Production/'):\n",
    "    \"\"\"\n",
    "    Fit models to production data\n",
    "    \n",
    "    Parameters:\n",
    "        data_path (str): Path to the production data Excel file\n",
    "        output_dir (str): Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of model fits\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Use derivative models for fitting\n",
    "    models = {'hubbert': hubbert_deriv, 'femp': femp_deriv}\n",
    "    prod_data = prep_data(data_path)\n",
    "    \n",
    "    p_group = prod_data.groupby('PROP_ID')\n",
    "\n",
    "    # Parallelize over mines\n",
    "    results_data = Parallel(n_jobs=-1)(delayed(fit_prod_model)(\n",
    "        mine, group, models, targets, lower_bounds, upper_bounds, min_sample_size\n",
    "    ) for mine, group in tqdm(p_group, desc='Fitting models to production data'))\n",
    "\n",
    "    # Unpack parallelized results\n",
    "    all_results, all_data_records = zip(*results_data)\n",
    "\n",
    "    # Combine results if not empty\n",
    "    res_df = pd.concat([pd.DataFrame(res) for res in all_results if res])\n",
    "    \n",
    "    # Properly handle data records - flatten the list of dataframes\n",
    "    flattened_data_records = []\n",
    "    for sublist in all_data_records:\n",
    "        if isinstance(sublist, list):\n",
    "            for item in sublist:\n",
    "                if isinstance(item, pd.DataFrame) and not item.empty:\n",
    "                    flattened_data_records.append(item)\n",
    "        elif isinstance(sublist, pd.DataFrame) and not sublist.empty:\n",
    "            flattened_data_records.append(sublist)\n",
    "    \n",
    "    data_records = pd.concat(flattened_data_records) if flattened_data_records else pd.DataFrame()\n",
    "    \n",
    "    # Save results as Excel files\n",
    "    res_df.to_excel(f'{output_dir}/production_model_fits_lifetime.xlsx', index=False)\n",
    "    data_records.to_excel(f'{output_dir}/data_records_lifetime.xlsx', index=False)\n",
    "    \n",
    "    \n",
    "    return res_df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Use the specified input file path\n",
    "    data_path = \"Data Output/Commodity Production/Commodity_Production-1980_2023_long_final.xlsx\"\n",
    "    main_fitting_loop(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "825b6253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from Data Output/Cumulative Production/production_model_fits_lifetime.xlsx\n",
      "Initial: 1446 rows, 723 unique mines\n",
      "No missing p-values found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def clean_empty_p_values():\n",
    "    \"\"\"\n",
    "    Deletes all PROP_IDs from both production_model_fits_lifetime.xlsx and\n",
    "    data_records_lifetime.xlsx if they have any empty p-values.\n",
    "    Prints summary and saves cleaned files back to their original paths.\n",
    "    \"\"\"\n",
    "    fits_path = 'Data Output/Cumulative Production/production_model_fits_lifetime.xlsx'\n",
    "    records_path = 'Data Output/Cumulative Production/data_records_lifetime.xlsx'\n",
    "\n",
    "    # Check both files exist\n",
    "    for path in [fits_path, records_path]:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"File not found: {path}\")\n",
    "            return\n",
    "\n",
    "    # Load fits data\n",
    "    print(f\"Loading data from {fits_path}\")\n",
    "    df_fits = pd.read_excel(fits_path)\n",
    "\n",
    "    # Check required columns\n",
    "    required_cols = ['PROP_ID', 'P1_value', 'P2_value']\n",
    "    if not all(col in df_fits.columns for col in required_cols):\n",
    "        print(f\"Missing required columns in {fits_path}\")\n",
    "        return\n",
    "\n",
    "    # Initial stats\n",
    "    total_mines = df_fits['PROP_ID'].nunique()\n",
    "    total_rows = len(df_fits)\n",
    "    print(f\"Initial: {total_rows} rows, {total_mines} unique mines\")\n",
    "\n",
    "    # Identify mines with empty P values\n",
    "    mask_p1 = df_fits['P1_value'].isna()\n",
    "    mask_p2 = df_fits['P2_value'].isna()\n",
    "    mask_any_empty = mask_p1 | mask_p2\n",
    "    mines_to_drop = df_fits.loc[mask_any_empty, 'PROP_ID'].unique()\n",
    "\n",
    "    if len(mines_to_drop) == 0:\n",
    "        print(\"No missing p-values found.\")\n",
    "        return\n",
    "\n",
    "    # Print details\n",
    "    print(f\"\\nDropping {len(mines_to_drop)} mines with missing p-values:\")\n",
    "    for mine_id in mines_to_drop:\n",
    "        missing = []\n",
    "        if df_fits.loc[(df_fits['PROP_ID'] == mine_id) & mask_p1].any().any():\n",
    "            missing.append(\"P1\")\n",
    "        if df_fits.loc[(df_fits['PROP_ID'] == mine_id) & mask_p2].any().any():\n",
    "            missing.append(\"P2\")\n",
    "        print(f\"  - PROP_ID {mine_id}: missing {', '.join(missing)}\")\n",
    "\n",
    "    # Filter both datasets\n",
    "    df_fits_clean = df_fits[~df_fits['PROP_ID'].isin(mines_to_drop)]\n",
    "\n",
    "    print(f\"\\nLoading corresponding records from {records_path}\")\n",
    "    df_records = pd.read_excel(records_path)\n",
    "    if 'PROP_ID' not in df_records.columns:\n",
    "        print(f\"Missing PROP_ID column in {records_path}\")\n",
    "        return\n",
    "\n",
    "    df_records_clean = df_records[~df_records['PROP_ID'].isin(mines_to_drop)]\n",
    "\n",
    "    # Save cleaned datasets\n",
    "    df_fits_clean.to_excel(fits_path, index=False)\n",
    "    df_records_clean.to_excel(records_path, index=False)\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\nCleaned data saved:\")\n",
    "    print(f\"  - {len(df_fits) - len(df_fits_clean)} rows removed from fits file\")\n",
    "    print(f\"  - {len(df_records) - len(df_records_clean)} rows removed from records file\")\n",
    "    print(f\"  - {len(mines_to_drop)} unique mines removed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_empty_p_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250de02",
   "metadata": {},
   "source": [
    "Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "52043d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           R2                                   RMSE                        \\\n",
      "         mean   std median  skew kurtosis       mean         std    median   \n",
      "Model                                                                        \n",
      "femp     0.44  0.30   0.45  0.02    -1.36  706087.11  2828305.11  25844.73   \n",
      "hubbert  0.62  0.28   0.72 -0.76    -0.61  426145.47  3075961.52  13487.98   \n",
      "\n",
      "                        NRMSE                              \n",
      "          skew kurtosis  mean   std median  skew kurtosis  \n",
      "Model                                                      \n",
      "femp     10.31   154.17  0.24  0.13   0.22  1.10     1.53  \n",
      "hubbert  19.59   448.06  0.12  0.07   0.11  1.91     5.55  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leshe\\AppData\\Local\\Temp\\ipykernel_21740\\349211397.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import skew, kurtosis\n",
    "import os\n",
    "\n",
    "# Load your data\n",
    "modelres = pd.read_excel(\"Data Output/Cumulative Production/production_model_fits_lifetime.xlsx\")\n",
    "data_records = pd.read_excel(\"Data Output/Cumulative Production/data_records_lifetime.xlsx\")\n",
    "\n",
    "# Constants\n",
    "sig = 0.05\n",
    "\n",
    "# ------- Classification Function --------\n",
    "def classify_model_results(df):\n",
    "    df['femp_significant'] = (df['Model'] == 'femp') & (df[['P1_pval', 'P2_pval']] < sig).all(axis=1)\n",
    "    df['hubbert_significant'] = (df['Model'] == 'hubbert') & (df[['P1_pval', 'P2_pval', 'P3_pval']] < sig).all(axis=1)\n",
    "\n",
    "    agg = df.groupby(['PROP_ID']).agg({\n",
    "        'femp_significant': 'any',\n",
    "        'hubbert_significant': 'any',\n",
    "        'RMSE': lambda x: x.iloc[1] - x.iloc[0] if len(x) == 2 else np.nan,\n",
    "        'R2': lambda x: x.iloc[1] - x.iloc[0] if len(x) == 2 else np.nan\n",
    "    }).reset_index()\n",
    "\n",
    "    def choose_class(row):\n",
    "        if row['femp_significant'] and row['hubbert_significant']:\n",
    "            if row['RMSE'] < 0 and row['R2'] > 0:\n",
    "                return 'F'\n",
    "            elif row['RMSE'] > 0 and row['R2'] < 0:\n",
    "                return 'H'\n",
    "            else:\n",
    "                return 'H'\n",
    "        elif row['femp_significant']:\n",
    "            return 'F'\n",
    "        elif row['hubbert_significant']:\n",
    "            return 'H'\n",
    "        else:\n",
    "            return 'N'\n",
    "\n",
    "    agg['Class'] = agg.apply(choose_class, axis=1)\n",
    "    return df.merge(agg[['PROP_ID', 'Class']], on='PROP_ID')\n",
    "\n",
    "# ------- Summary Statistics --------\n",
    "def summarize_fit_stats(df):\n",
    "    summary = df.groupby(['Model'])[['R2', 'RMSE', 'NRMSE']].agg(['mean', 'std', 'median', skew, kurtosis]).round(2)\n",
    "    summary.to_excel(\"Data Output/Cumulative Production/production_model_summary.xlsx\")\n",
    "    return summary\n",
    "\n",
    "# ------- Matplotlib Plots --------\n",
    "def plot_histograms(df):\n",
    "    for metric in ['R2', 'RMSE', 'NRMSE']:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for model in df['Model'].unique():\n",
    "            subset = df[df['Model'] == model]\n",
    "            values = subset[metric] / 1e6 if metric == 'RMSE' else subset[metric]\n",
    "            plt.hist(values, bins=20, alpha=0.6, label=model)\n",
    "        plt.title(f\"Histogram of {metric}\")\n",
    "        plt.xlabel(metric + (\" (Mt)\" if metric == \"RMSE\" else \"\"))\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"Visualizations/Cumulative Production/{metric}_histogram.png\")\n",
    "        plt.close()\n",
    "\n",
    "def plot_class_bar(df):\n",
    "    class_counts = df['Class'].value_counts(normalize=True) * 100\n",
    "    class_counts.plot(kind='bar', color='skyblue')\n",
    "    plt.title(\"Model Classification Share\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Visualizations/Cumulative Production/class_bar_chart.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_error_scatter(df):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model in df['Model'].unique():\n",
    "        subset = df[df['Model'] == model]\n",
    "        plt.scatter(subset['Observed'], subset['Predicted'], alpha=0.3, label=model)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.plot([1, 1e8], [1, 1e8], 'k--')\n",
    "    plt.xlabel(\"Observed\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Observed vs Predicted (log-log)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Visualizations/Cumulative Production/error_scatter.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_error_time_series(df):\n",
    "    df['Error'] = df['Predicted'] - df['Observed']\n",
    "    df = df.dropna(subset=['Error'])\n",
    "    df['Error'] = StandardScaler().fit_transform(df['Error'].values.reshape(-1, 1))\n",
    "    df = df[df['Year'] > 1950]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for model in df['Model'].unique():\n",
    "        subset = df[df['Model'] == model]\n",
    "        plt.plot(subset['Year'], subset['Error'], '.', alpha=0.3, label=model)\n",
    "    plt.axhline(0, color='black', linestyle='--')\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Standardized Error\")\n",
    "    plt.title(\"Error Over Time\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Visualizations/Cumulative Production/error_time_series.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_boxplot_fit_metrics(df, metric):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    df.boxplot(column=metric, by='Model')\n",
    "    plt.title(f\"Boxplot of {metric} by Model\")\n",
    "    plt.suptitle(\"\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Visualizations/Cumulative Production/{metric}_boxplot.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ------- Main Execution --------\n",
    "if __name__ == '__main__':\n",
    "    modelres = classify_model_results(modelres)\n",
    "    print(summarize_fit_stats(modelres))\n",
    "\n",
    "    data_records = data_records.merge(modelres[['PROP_ID', 'Model', 'Class']], on=['PROP_ID', 'Model'], how='left')\n",
    "\n",
    "    plot_histograms(modelres)\n",
    "    plot_class_bar(modelres)\n",
    "    plot_error_scatter(data_records)\n",
    "    plot_error_time_series(data_records)\n",
    "    plot_boxplot_fit_metrics(modelres, 'R2')\n",
    "    plot_boxplot_fit_metrics(modelres, 'RMSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a2624",
   "metadata": {},
   "source": [
    "Cumulative Production calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af30ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: Data Output/Cumulative Production/Cumulative_Production.xlsx\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "INPUT_FILE = \"Data Output/Cumulative Production/production_model_fits_lifetime.xlsx\"\n",
    "OUTPUT_FILE = \"Data Output/Cumulative Production/Cumulative_Production.xlsx\"\n",
    "\n",
    "# ----------------------------\n",
    "# Models\n",
    "# ----------------------------\n",
    "def hubbert_model(t, L, k, t0):\n",
    "    return L / (1 + np.exp(-k * (t - t0)))\n",
    "\n",
    "def femp(t, R0, C):\n",
    "    return R0 * (1 - (1 - C) ** t)\n",
    "\n",
    "# ----------------------------\n",
    "# Load data\n",
    "# ----------------------------\n",
    "df = pd.read_excel(INPUT_FILE)\n",
    "df[\"Model\"] = df[\"Model\"].str.lower()\n",
    "\n",
    "# ----------------------------\n",
    "# Calculate cumulative production\n",
    "# ----------------------------\n",
    "results = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    t = row[\"Mine_Lifetime\"]\n",
    "\n",
    "    if row[\"Model\"] == \"hubbert\":\n",
    "        L, k, t0 = row[\"P1_value\"], row[\"P2_value\"], row[\"P3_value\"]\n",
    "        cum_prod = hubbert_model(t, L, k, t0)\n",
    "\n",
    "    elif row[\"Model\"] == \"femp\":\n",
    "        R0, C = row[\"P1_value\"], row[\"P2_value\"]\n",
    "        cum_prod = femp(t, R0, C)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    results.append([row[\"PROP_ID\"], row[\"Model\"], t, cum_prod])\n",
    "\n",
    "# ----------------------------\n",
    "# Save results\n",
    "# ----------------------------\n",
    "out = pd.DataFrame(results, columns=[\"PROP_ID\", \"Model\", \"Mine_Lifetime\", \"Cumulative_Production\"])\n",
    "Path(OUTPUT_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_excel(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"Saved results to: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9501720c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Cumulative_Production'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m hubbert_df \u001b[38;5;241m=\u001b[39m cumprod_df[cumprod_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhubbert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Keep only the relevant columns and rename\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m hubbert_df \u001b[38;5;241m=\u001b[39m \u001b[43mhubbert_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPROP_ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCumulative_Production\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mrename(\n\u001b[0;32m     16\u001b[0m     columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCumulative_Production\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCumProd_Tonnes\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Load commodity production data\u001b[39;00m\n\u001b[0;32m     20\u001b[0m commodity_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(commodity_path)\n",
      "File \u001b[1;32mc:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Cumulative_Production'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "cumprod_path = \"Data Output/Cumulative Production/Cumulative_Production.xlsx\"\n",
    "commodity_path = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "output_path = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "\n",
    "# Load cumulative production data\n",
    "cumprod_df = pd.read_excel(cumprod_path)\n",
    "\n",
    "# Keep only rows for Hubbert model\n",
    "hubbert_df = cumprod_df[cumprod_df[\"Model\"] == \"hubbert\"]\n",
    "\n",
    "# Keep only the relevant columns and rename\n",
    "hubbert_df = hubbert_df[[\"PROP_ID\", \"Cumulative_Production\"]].rename(\n",
    "    columns={\"Cumulative_Production\": \"CumProd_Tonnes\"}\n",
    ")\n",
    "\n",
    "# Load commodity production data\n",
    "commodity_df = pd.read_excel(commodity_path)\n",
    "\n",
    "# Merge on PROP_ID\n",
    "merged_df = commodity_df.merge(hubbert_df, on=\"PROP_ID\", how=\"left\")\n",
    "\n",
    "# Save to a new file\n",
    "merged_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Merged file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c41587",
   "metadata": {},
   "source": [
    "Cumulative production vs reserves comparison\n",
    "\n",
    "3) compare to reserves metal, except bauxite where you compare to reserves ore\n",
    "4) drop mines that are >3x the reserves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "daa74a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cumulative production vs reserves comparison (single file)...\n",
      "Saved results to Data Output/Cumulative Production/Comparison_Results/Reserves_Comparison\\cumulative_vs_reserves.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Compare cumulative production with reserves and save results to Excel.\n",
    "\n",
    "- Uses ONLY the commodity production file.\n",
    "- Cumulative: CumProd_Tonnes\n",
    "- Reserves (Bauxite): Reserves_Tonnage\n",
    "- Reserves (all others): Reserves_Contained\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "PRODUCTION_FILE = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "\n",
    "OUTPUT_DIR = \"Data Output/Cumulative Production/Comparison_Results/Reserves_Comparison\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"cumulative_vs_reserves.xlsx\")\n",
    "\n",
    "# Column names\n",
    "PROP_ID_COL = \"PROP_ID\"\n",
    "PRIMARY_COMMODITY_COL = \"PRIMARY_COMMODITY\"\n",
    "CUMULATIVE_COL = \"CumProd_Tonnes\"\n",
    "RESERVES_CONTAINED_COL = \"Reserves_Contained\"\n",
    "RESERVES_TONNAGE_COL = \"Reserves_Tonnage\"\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "def main():\n",
    "    print(\"Running cumulative production vs reserves comparison (single file)...\")\n",
    "\n",
    "    # Load from the commodity production file\n",
    "    df = pd.read_excel(PRODUCTION_FILE, engine=\"openpyxl\")\n",
    "\n",
    "    # Keep only needed columns and one row per PROP_ID\n",
    "    keep_cols = [\n",
    "        PROP_ID_COL,\n",
    "        PRIMARY_COMMODITY_COL,\n",
    "        CUMULATIVE_COL,\n",
    "        RESERVES_CONTAINED_COL,\n",
    "        RESERVES_TONNAGE_COL,\n",
    "    ]\n",
    "    df = df[keep_cols].groupby(PROP_ID_COL, as_index=False).agg(\"first\")\n",
    "\n",
    "    # Ensure numeric where needed\n",
    "    for col in [CUMULATIVE_COL, RESERVES_CONTAINED_COL, RESERVES_TONNAGE_COL]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Choose reserves column based on commodity\n",
    "    is_bauxite = df[PRIMARY_COMMODITY_COL].astype(str).str.strip().str.lower().eq(\"bauxite\")\n",
    "    df[\"Reserves_Compare\"] = np.where(\n",
    "        is_bauxite,\n",
    "        df[RESERVES_TONNAGE_COL],\n",
    "        df[RESERVES_CONTAINED_COL],\n",
    "    )\n",
    "\n",
    "    # Drop rows missing either cumulative or reserves\n",
    "    df = df.dropna(subset=[CUMULATIVE_COL, \"Reserves_Compare\"])\n",
    "\n",
    "    # Compute metrics\n",
    "    df[\"Absolute_Difference\"] = (df[CUMULATIVE_COL] - df[\"Reserves_Compare\"]).abs()\n",
    "    df[\"Raw_Difference\"] = df[CUMULATIVE_COL] - df[\"Reserves_Compare\"]\n",
    "    df[\"Percent_Difference\"] = (df[\"Raw_Difference\"] / df[\"Reserves_Compare\"]) * 100\n",
    "    df[\"Cumulative_to_Reserves_Ratio\"] = df[CUMULATIVE_COL] / df[\"Reserves_Compare\"]\n",
    "\n",
    "    # Save\n",
    "    df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"Saved results to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0d8f339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Main dataset: 723 records\n",
      "Mines to remove: 122\n",
      "PROP_IDs: [24470, 24474, 24503, 24505, 24732, 24733, 24774, 24783, 24815, 25687, 25699, 25715, 25720, 25727, 25751, 25753, 25754, 25831, 26073, 26495, 26575, 26711, 26722, 27027, 27134, 27139, 27145, 27146, 27163, 27165, 27166, 27167, 27213, 27243, 27280, 27283, 27291, 27303, 27345, 27384, 27539, 27542, 27543, 27612, 27616, 27632, 27653, 27747, 27830, 27868, 27971, 27981, 28004, 28021, 28245, 28250, 28264, 28283, 28296, 28316, 28362, 28363, 28364, 28428, 28585, 28596, 28703, 28955, 29057, 29073, 29195, 29246, 29347, 29377, 29438, 29544, 29792, 29980, 30277, 30526, 30656, 31300, 31316, 31560, 31563, 31631, 31651, 31758, 31818, 31827, 31833, 31895, 31907, 31934, 32142, 32169, 32642, 32779, 32931, 35185, 35378, 35755, 35780, 36464, 36896, 37326, 38727, 54759, 56047, 57368, 57500, 59345, 64042, 66063, 66682, 66683, 68175, 74744, 74757, 74761, 76468, 80388]\n",
      "Updated dataset: 601 records\n",
      "Removed: 122 records\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple script to remove mines with high Cumulative_to_Reserves_Ratio (>3.4) \n",
    "from the main dataset.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "MAIN_DATASET_FILE = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "COMPARISON_FILE = \"Data Output/Cumulative Production/Comparison_Results/Reserves_Comparison/cumulative_vs_reserves.xlsx\"\n",
    "\n",
    "# Filtering criteria\n",
    "RATIO_THRESHOLD = 3.4\n",
    "\n",
    "# Column names\n",
    "PROP_ID_COL = 'PROP_ID'\n",
    "RATIO_COL = 'Cumulative_to_Reserves_Ratio'\n",
    "\n",
    "def main():\n",
    "    \"\"\"Remove high-ratio mines from the main dataset.\"\"\"\n",
    "    \n",
    "    print(\"Loading datasets...\")\n",
    "    \n",
    "    # Load main dataset\n",
    "    main_df = pd.read_excel(MAIN_DATASET_FILE, engine='openpyxl')\n",
    "    print(f\"Main dataset: {len(main_df):,} records\")\n",
    "    \n",
    "    # Load comparison results (cumulative vs reserves)\n",
    "    comp_df = pd.read_excel(COMPARISON_FILE, engine='openpyxl')\n",
    "    \n",
    "    # Ensure ratio is numeric\n",
    "    comp_df[RATIO_COL] = pd.to_numeric(comp_df[RATIO_COL], errors='coerce')\n",
    "    \n",
    "    # Find mines with high ratios\n",
    "    high_ratio_mines = comp_df[comp_df[RATIO_COL] > RATIO_THRESHOLD]\n",
    "    prop_ids_to_remove = set(high_ratio_mines[PROP_ID_COL].dropna().unique())\n",
    "    \n",
    "    print(f\"Mines to remove: {len(prop_ids_to_remove)}\")\n",
    "    print(f\"PROP_IDs: {sorted(prop_ids_to_remove)}\")\n",
    "    \n",
    "    # Filter main dataset\n",
    "    filtered_df = main_df[~main_df[PROP_ID_COL].isin(prop_ids_to_remove)]\n",
    "    \n",
    "    # Update the main file\n",
    "    filtered_df.to_excel(MAIN_DATASET_FILE, index=False)\n",
    "    \n",
    "    print(f\"Updated dataset: {len(filtered_df):,} records\")\n",
    "    print(f\"Removed: {len(main_df) - len(filtered_df):,} records\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca91009",
   "metadata": {},
   "source": [
    "National average comparison\n",
    "\n",
    "2) do the comparison\n",
    "2) convert bauxite after comparison to aluminium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a35e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processed 609 mines\n",
      "Results saved to: annual_production_by_commodity_country.xlsx\n",
      "Top 5 combinations:\n",
      "  1. Iron Ore in Brazil: 1,167,063,898 tonnes/year\n",
      "  2. Iron Ore in Australia: 234,531,400 tonnes/year\n",
      "  3. Iron Ore in Russia: 64,132,909 tonnes/year\n",
      "  4. Iron Ore in India: 49,734,793 tonnes/year\n",
      "  5. Iron Ore in China: 42,331,584 tonnes/year\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple script to calculate average annual production per mine and aggregate\n",
    "total annual production by commodity and country.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "OUTPUT_DIR = \"Data Output/Cumulative Production/Comparison_Results/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Column names\n",
    "PROP_ID_COL = 'PROP_ID'\n",
    "CUMPROD_COL = 'CumProd_Tonnes'\n",
    "START_YEAR_COL = 'START_UP_YR'\n",
    "CLOSURE_YEAR_COL = 'PROJ_CLOSURE_YR'\n",
    "PRIMARY_COMMODITY_COL = 'PRIMARY_COMMODITY'\n",
    "COUNTRY_COL = 'COUNTRY_NAME'\n",
    "\n",
    "def main():\n",
    "    \"\"\"Calculate annual production and aggregate by commodity and country.\"\"\"\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_excel(INPUT_FILE, engine='openpyxl')\n",
    "    \n",
    "    # Calculate mine lifetime\n",
    "    df['Mine_Lifetime_Years'] = df[CLOSURE_YEAR_COL] - df[START_YEAR_COL]\n",
    "    \n",
    "    # Calculate average annual production\n",
    "    df['Average_Annual_Production_Tonnes'] = df[CUMPROD_COL] / df['Mine_Lifetime_Years']\n",
    "    \n",
    "    print(f\"Processed {len(df)} mines\")\n",
    "    \n",
    "    # Aggregate by commodity and country\n",
    "    commodity_country = df.groupby([PRIMARY_COMMODITY_COL, COUNTRY_COL]).agg({\n",
    "        'Average_Annual_Production_Tonnes': 'sum',\n",
    "        PROP_ID_COL: 'count'\n",
    "    }).round(2)\n",
    "    \n",
    "    commodity_country.columns = ['Total_Annual_Production_Tonnes', 'Number_of_Mines']\n",
    "    commodity_country = commodity_country.reset_index().sort_values('Total_Annual_Production_Tonnes', ascending=False)\n",
    "    \n",
    "    # Save results\n",
    "    commodity_country.to_excel(os.path.join(OUTPUT_DIR, 'annual_production_by_commodity_country.xlsx'), index=False)\n",
    "    \n",
    "    print(f\"Results saved to: annual_production_by_commodity_country.xlsx\")\n",
    "    print(f\"Top 5 combinations:\")\n",
    "    for i, (_, row) in enumerate(commodity_country.head(5).iterrows()):\n",
    "        print(f\"  {i+1}. {row[PRIMARY_COMMODITY_COL]} in {row[COUNTRY_COL]}: {row['Total_Annual_Production_Tonnes']:,.0f} tonnes/year\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f7b475c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 3 rows with specified PROP_IDs\n",
      "Remaining rows: 598\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = 'Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# List of PROP_IDs to remove\n",
    "prop_ids_to_remove = [37218, 31921, 29665, 28529]\n",
    "\n",
    "# Remove rows with specified PROP_IDs\n",
    "df_filtered = df[~df['PROP_ID'].isin(prop_ids_to_remove)]\n",
    "\n",
    "# Save back to the same file\n",
    "df_filtered.to_excel(file_path, index=False)\n",
    "\n",
    "print(f\"Removed {len(df) - len(df_filtered)} rows with specified PROP_IDs\")\n",
    "print(f\"Remaining rows: {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6c3bd646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Output/Cumulative Production/data_records_lifetime.xlsx: Removed 13334 rows, 53662 rows remaining\n",
      "Data Output/Cumulative Production/production_model_fits_lifetime.xlsx: Removed 250 rows, 1196 rows remaining\n",
      "All files updated successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file_paths = [\n",
    "    'Data Output/Cumulative Production/data_records_lifetime.xlsx',\n",
    "    'Data Output/Cumulative Production/production_model_fits_lifetime.xlsx'\n",
    "]\n",
    "\n",
    "# PROP_IDs to remove\n",
    "prop_ids_to_remove = [\n",
    "    24470, 24474, 24503, 24505, 24732, 24733, 24774, 24783, 24815, 25687, 25699, \n",
    "    25715, 25720, 25727, 25751, 25753, 25754, 25831, 26073, 26495, 26575, 26711, \n",
    "    26722, 27027, 27134, 27139, 27145, 27146, 27163, 27165, 27166, 27167, 27213, \n",
    "    27243, 27280, 27283, 27291, 27303, 27345, 27384, 27539, 27542, 27543, 27612, \n",
    "    27616, 27632, 27653, 27747, 27825, 27830, 27868, 27971, 27981, 28004, 28021, \n",
    "    28245, 28250, 28264, 28283, 28296, 28316, 28362, 28363, 28364, 28428, 28585, \n",
    "    28596, 28703, 28955, 29057, 29073, 29195, 29246, 29347, 29377, 29438, 29544, \n",
    "    29792, 29980, 30277, 30526, 30656, 31300, 31316, 31560, 31563, 31631, 31651, \n",
    "    31758, 31818, 31827, 31833, 31895, 31907, 31934, 32142, 32169, 32642, 32779, \n",
    "    32931, 35185, 35378, 35755, 35780, 36464, 36896, 37326, 38727, 54759, 56047, \n",
    "    57368, 57500, 59345, 64042, 66063, 66682, 66683, 68175, 74744, 74757, 74761, \n",
    "    76468, 80388, 37218, 31921, 29665, 28529\n",
    "\n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for file_path in file_paths:\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Remove rows with specified PROP_IDs\n",
    "    df_filtered = df[~df['PROP_ID'].isin(prop_ids_to_remove)]\n",
    "    \n",
    "    # Save back to the same file\n",
    "    df_filtered.to_excel(file_path, index=False)\n",
    "    \n",
    "    print(f\"{file_path}: Removed {len(df) - len(df_filtered)} rows, {len(df_filtered)} rows remaining\")\n",
    "\n",
    "print(\"All files updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e75f71da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CumProd_Tonnes for bauxite mines.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "\n",
    "df = pd.read_excel(file)\n",
    "mask = df[\"PRIMARY_COMMODITY\"] == \"Bauxite\"\n",
    "df.loc[mask, \"CumProd_Tonnes\"] = df.loc[mask, \"CumProd_Tonnes\"] * df.loc[mask, \"Ore_Grade\"]\n",
    "df.to_excel(file, index=False)\n",
    "\n",
    "print(\"Updated CumProd_Tonnes for bauxite mines.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c617fb",
   "metadata": {},
   "source": [
    "Per mine analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fd5993a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\ggplot.py:615: PlotnineWarning: Saving 16 x 10 in image.\n",
      "c:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\ggplot.py:616: PlotnineWarning: Filename: Visualizations/Cumulative Production/cumprod_plot_from_model_fits.png\n",
      "c:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\ggplot.py:615: PlotnineWarning: Saving 16 x 10 in image.\n",
      "c:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\ggplot.py:616: PlotnineWarning: Filename: Visualizations/Cumulative Production/prod_obs_vs_pred.png\n",
      "c:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\layer.py:364: PlotnineWarning: geom_point : Removed 1104 rows containing missing values.\n",
      "c:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\ggplot.py:615: PlotnineWarning: Saving 16 x 10 in image.\n",
      "c:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\ggplot.py:616: PlotnineWarning: Filename: Visualizations/Cumulative Production/residuals_plot_hubbert.png\n",
      "c:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\layer.py:364: PlotnineWarning: geom_point : Removed 570 rows containing missing values.\n",
      "c:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\ggplot.py:615: PlotnineWarning: Saving 16 x 10 in image.\n",
      "c:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\ggplot.py:616: PlotnineWarning: Filename: Visualizations/Cumulative Production/residuals_plot_femp.png\n",
      "c:\\Users\\leshe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\layer.py:364: PlotnineWarning: geom_point : Removed 570 rows containing missing values.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotnine import *\n",
    "from scipy import stats\n",
    "\n",
    "#########################################Model Functions##############################################\n",
    "\n",
    "def hubbert_model(t, L, k, t0):\n",
    "    return L / (1 + np.exp(-k * (t - t0)))\n",
    "\n",
    "def femp(t, R0, C):\n",
    "    return R0 * (1 - (1 - C)**t)\n",
    "\n",
    "def femp_deriv(t, R0, C):\n",
    "    return - R0 * np.log(1 - C) * (1 - C)**t\n",
    "\n",
    "def hubbert_deriv(t, L, k, t0):\n",
    "    exp_term = np.exp(-k * (t - t0))\n",
    "    return L * k * exp_term / (1 + exp_term)**2\n",
    "\n",
    "#########################################Utility Functions############################################\n",
    "\n",
    "def save_fig_plotnine(plot, filename, w=10, h=6):\n",
    "    plot.save(filename, width=w, height=h, dpi=300)\n",
    "\n",
    "def save_fig(filename, dpi=300):\n",
    "    plt.savefig(filename, dpi=dpi, bbox_inches='tight')\n",
    "\n",
    "#########################################Data Prep#####################################################\n",
    "\n",
    "def prep_data():\n",
    "    rec = pd.read_excel('Data Output/Cumulative Production/data_records_lifetime.xlsx')\n",
    "    modelres = pd.read_excel('Data Output/Cumulative Production/production_model_fits_lifetime.xlsx')\n",
    "    ua = pd.read_excel('Data Output/Cumulative Production/cumprod_mc_confidence.xlsx')\n",
    "\n",
    "    rec = rec.rename(columns={'PROP_ID': 'Prop_id'})\n",
    "    modelres = modelres.rename(columns={'PROP_ID': 'Prop_id'})\n",
    "    ua = ua.rename(columns={'PROP_ID': 'Prop_id', 'Time_period': 'Year_diff'})\n",
    "\n",
    "    ua['Year'] = ua['START_UP_YR'] + ua['Year_diff']\n",
    "    ua['Target_var'] = 'Production'\n",
    "\n",
    "    rec['Year'] = rec['Year'].astype(int)\n",
    "\n",
    "    merged = ua.merge(rec[['Prop_id', 'Year', 'Observed']], on=['Prop_id', 'Year'], how='left')\n",
    "    merged = merged.merge(modelres[['Prop_id', 'Target_var', 'Model', 'R2', 'NRMSE']], on=['Prop_id', 'Target_var', 'Model'], how='left')\n",
    "\n",
    "    return merged\n",
    "\n",
    "#########################################Main Plot Function###########################################\n",
    "\n",
    "def plot_cumprod():\n",
    "    \"\"\"\n",
    "    Build cumulative production curves directly from the model fits:\n",
    "      - Read 'production_model_fits_lifetime.xlsx'\n",
    "      - Use P1_value, P2_value, P3_value and Mine_Lifetime\n",
    "      - For hubbert:  L=P1, k=P2, t0=P3  -> hubbert_model(t, L, k, t0)\n",
    "      - For femp:     R0=P1, C=P2        -> femp(t, R0, C)\n",
    "      - Year axis uses START_UP_YR if available; otherwise uses t (years since start)\n",
    "      - Values converted to Mt\n",
    "    \"\"\"\n",
    "    fits_path = 'Data Output/Cumulative Production/production_model_fits_lifetime.xlsx'\n",
    "    df = pd.read_excel(fits_path)\n",
    "\n",
    "    # Standardize column names we rely on\n",
    "    df = df.rename(columns={'PROP_ID': 'Prop_id'})\n",
    "    # Keep only the production target if multiple targets exist\n",
    "    if 'Target_var' in df.columns:\n",
    "        df = df[df['Target_var'].str.lower() == 'production']\n",
    "\n",
    "    # Required columns check (be permissive if some exist)\n",
    "    required = ['Prop_id', 'Model', 'P1_value', 'P2_value', 'Mine_Lifetime']\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns in model fits file: {missing}\")\n",
    "\n",
    "    # If t0 exists for hubbert\n",
    "    has_p3 = 'P3_value' in df.columns\n",
    "    has_start = 'START_UP_YR' in df.columns\n",
    "\n",
    "    # Compute cumulative curves row-wise\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        prop = row['Prop_id']\n",
    "        model = str(row['Model']).strip().lower()\n",
    "        p1 = row['P1_value']\n",
    "        p2 = row['P2_value']\n",
    "        p3 = row['P3_value'] if has_p3 and not pd.isna(row.get('P3_value')) else None\n",
    "        lifetime = int(row['Mine_Lifetime']) if not pd.isna(row['Mine_Lifetime']) else 0\n",
    "        if lifetime < 0:\n",
    "            continue\n",
    "\n",
    "        # Build time grid t = 0..lifetime (inclusive)\n",
    "        t = np.arange(0, lifetime + 1, dtype=float)\n",
    "\n",
    "        # Compute cumulative according to model\n",
    "        if model == 'hubbert':\n",
    "            # Assumed mapping: L=p1, k=p2, t0=p3 (if p3 missing, default to lifetime/2)\n",
    "            L = p1\n",
    "            k = p2\n",
    "            t0 = p3 if p3 is not None else lifetime / 2.0\n",
    "            cum = hubbert_model(t, L, k, t0)\n",
    "        elif model == 'femp':\n",
    "            # Assumed mapping: R0=p1, C=p2\n",
    "            R0 = p1\n",
    "            C = p2\n",
    "            cum = femp(t, R0, C)\n",
    "        else:\n",
    "            # Unknown model: skip\n",
    "            continue\n",
    "\n",
    "        # Convert to Mt\n",
    "        cum_mt = cum / 1e6\n",
    "\n",
    "        # Construct Year or t\n",
    "        if has_start and not pd.isna(row['START_UP_YR']):\n",
    "            years = (int(row['START_UP_YR']) + t).astype(int)\n",
    "            x_name = 'Year'\n",
    "        else:\n",
    "            years = t.astype(int)\n",
    "            x_name = 't'\n",
    "\n",
    "        # Store\n",
    "        for xi, yi in zip(years, cum_mt):\n",
    "            records.append({\n",
    "                'Prop_id': prop,\n",
    "                'Model': model,\n",
    "                x_name: int(xi),\n",
    "                'CumProd_Mt': yi\n",
    "            })\n",
    "\n",
    "    if not records:\n",
    "        raise ValueError(\"No cumulative records were generated — check model names/parameters.\")\n",
    "\n",
    "    plot_df = pd.DataFrame(records)\n",
    "\n",
    "    # Choose x-axis depending on what's available\n",
    "    if 'Year' in plot_df.columns:\n",
    "        x_col = 'Year'\n",
    "        x_lab = 'Year'\n",
    "    else:\n",
    "        x_col = 't'\n",
    "        x_lab = 'Years since start'\n",
    "\n",
    "    # Sample up to 20 properties to keep the facet grid readable\n",
    "    sample_ids = plot_df['Prop_id'].dropna().unique()\n",
    "    if len(sample_ids) > 20:\n",
    "        sample_ids = np.random.choice(sample_ids, size=20, replace=False)\n",
    "    plot_df = plot_df[plot_df['Prop_id'].isin(sample_ids)]\n",
    "\n",
    "    plot = (\n",
    "        ggplot(plot_df, aes(x=x_col, y='CumProd_Mt', color='Model')) +\n",
    "        geom_line(size=1) +\n",
    "        facet_wrap('~Prop_id', scales='free') +\n",
    "        labs(\n",
    "            title='Cumulative Production from Model Fits',\n",
    "            x=x_lab,\n",
    "            y='Cumulative production (Mt)'\n",
    "        ) +\n",
    "        theme_minimal() +\n",
    "        theme(\n",
    "            legend_position='bottom',\n",
    "            panel_background=element_rect(fill='white'),\n",
    "            plot_background=element_rect(fill='white'),\n",
    "            strip_background=element_rect(fill='white')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    save_fig_plotnine(\n",
    "        plot,\n",
    "        'Visualizations/Cumulative Production/cumprod_plot_from_model_fits.png',\n",
    "        w=16,\n",
    "        h=10\n",
    "    )\n",
    "\n",
    "def plot_predicted_vs_observed():\n",
    "    data = pd.read_excel(\"Data Output/Cumulative Production/data_records_lifetime.xlsx\")\n",
    "    data = data.rename(columns={'PROP_ID': 'Prop_id'})\n",
    "    data['Observed'] = data['Observed'] / 1e6\n",
    "    data['Predicted'] = data['Predicted'] / 1e6\n",
    "\n",
    "    sample_ids = np.random.choice(data['Prop_id'].unique(), size=min(20, data['Prop_id'].nunique()), replace=False)\n",
    "    subset = data[data['Prop_id'].isin(sample_ids)]\n",
    "\n",
    "    plot = (\n",
    "        ggplot(subset, aes(x='Year')) +\n",
    "        geom_point(aes(y='Observed'), color='black', size=1.5) +\n",
    "        geom_line(aes(y='Predicted', color='Model'), size=1) +\n",
    "        facet_wrap('~Prop_id', scales='free') +\n",
    "        labs(title='Observed vs Predicted Production', x='Year', y='Production (Mt)') +\n",
    "        theme_minimal() +\n",
    "        theme(\n",
    "            legend_position='bottom',\n",
    "            panel_background=element_rect(fill='white'),\n",
    "            plot_background=element_rect(fill='white'),\n",
    "            strip_background=element_rect(fill='white'),\n",
    "            panel_grid_major=element_line(color='lightgray', size=0.5),\n",
    "            panel_grid_minor=element_line(color='lightgray', size=0.25)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    save_fig_plotnine(plot, 'Visualizations/Cumulative Production/prod_obs_vs_pred.png', w=16, h=10)\n",
    "\n",
    "def plot_residuals_by_model():\n",
    "    data = pd.read_excel(\"Data Output/Cumulative Production/data_records_lifetime.xlsx\")\n",
    "    data = data.rename(columns={'PROP_ID': 'Prop_id'})\n",
    "    data['Residual'] = data['Residual'] / 1e6  # Convert to Mt if needed\n",
    "\n",
    "    # Get common sample IDs that exist for both models\n",
    "    hubbert_data = data[data['Model'] == 'hubbert']\n",
    "    femp_data = data[data['Model'] == 'femp']\n",
    "    \n",
    "    # Find intersection of property IDs that exist in both models\n",
    "    common_prop_ids = set(hubbert_data['Prop_id'].unique()) & set(femp_data['Prop_id'].unique())\n",
    "    common_prop_ids = list(common_prop_ids)\n",
    "    \n",
    "    # Sample from the common property IDs\n",
    "    if len(common_prop_ids) > 20:\n",
    "        sample_ids = np.random.choice(common_prop_ids, size=20, replace=False)\n",
    "    else:\n",
    "        sample_ids = common_prop_ids\n",
    "\n",
    "    for model in ['hubbert', 'femp']:\n",
    "        model_data = data[data['Model'] == model]\n",
    "        subset = model_data[model_data['Prop_id'].isin(sample_ids)]\n",
    "\n",
    "        plot = (\n",
    "            ggplot(subset, aes(x='Year', y='Residual')) +\n",
    "            geom_point(alpha=0.7) +\n",
    "            geom_smooth(color='red', method='lm') +\n",
    "            facet_wrap('~Prop_id', scales='free') +\n",
    "            labs(title=f'Residuals Over Time ({model} Model)', x='Year', y='Residual (Mt)') +\n",
    "            theme_minimal() +\n",
    "            theme(\n",
    "                legend_position='none',\n",
    "                panel_background=element_rect(fill='white'),\n",
    "                plot_background=element_rect(fill='white'),\n",
    "                strip_background=element_rect(fill='white'),\n",
    "                panel_grid_major=element_line(color='lightgray', size=0.5),\n",
    "                panel_grid_minor=element_line(color='lightgray', size=0.25)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        filename = f'Visualizations/Cumulative Production/residuals_plot_{model}.png'\n",
    "        save_fig_plotnine(plot, filename, w=16, h=10)\n",
    "\n",
    "def qqplot():\n",
    "    data = pd.read_excel(\"Data Output/Cumulative Production/data_records_lifetime.xlsx\")\n",
    "    data = data.rename(columns={'PROP_ID': 'Prop_id'})\n",
    "    targets = ['Observed', 'Predicted']\n",
    "    subset = data[targets].apply(np.log)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    for i, target in enumerate(targets):\n",
    "        stats.probplot(subset[target].dropna(), dist=\"norm\", plot=ax[i])\n",
    "        ax[i].set_title(f'QQ Plot - {target}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_fig('Visualizations/Cumulative Production/qq_plots.png')\n",
    "    plt.close()\n",
    "\n",
    "def normality_tests():\n",
    "    data = pd.read_excel(\"Data Output/Cumulative Production/data_records_lifetime.xlsx\")\n",
    "    data = data.rename(columns={'PROP_ID': 'Prop_id'})\n",
    "    targets = ['Observed', 'Predicted']\n",
    "    grouped = data.groupby('Prop_id')\n",
    "\n",
    "    results = []\n",
    "    for prop_id, group in grouped:\n",
    "        for var in targets:\n",
    "            values = group[var].dropna()\n",
    "            if len(values) < 10:\n",
    "                continue\n",
    "            shapiro_p = stats.shapiro(values)[1]\n",
    "            ks_p = stats.kstest(values, 'norm')[1]\n",
    "            results.append((prop_id, var, shapiro_p, ks_p))\n",
    "\n",
    "    df = pd.DataFrame(results, columns=['Prop_id', 'Variable', 'Shapiro_p', 'KS_p'])\n",
    "    df.to_excel('Data Output/Cumulative Production/normality_test_results.xlsx', index=False)\n",
    "\n",
    "#########################################Run###########################################################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plot_cumprod()\n",
    "    plot_predicted_vs_observed()\n",
    "    plot_residuals_by_model()\n",
    "    qqplot()\n",
    "    normality_tests()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
