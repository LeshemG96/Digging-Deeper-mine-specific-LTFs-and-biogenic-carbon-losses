{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a1a13f",
   "metadata": {},
   "source": [
    "Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e9b4e67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Bauxite...\n",
      "Saved merged data to Data Input/Commodity Production\\Bauxite-Production-1980-2023.xlsx\n",
      "Processing Copper...\n",
      "Saved merged data to Data Input/Commodity Production\\Copper-Production-1980-2023.xlsx\n",
      "Processing IronOre...\n",
      "Saved merged data to Data Input/Commodity Production\\IronOre-Production-1980-2023.xlsx\n",
      "Processing Lithium...\n",
      "Saved merged data to Data Input/Commodity Production\\Lithium-Production-1980-2023.xlsx\n",
      "Processing Manganese...\n",
      "Saved merged data to Data Input/Commodity Production\\Manganese-Production-1980-2023.xlsx\n",
      "Processing Nickel...\n",
      "Saved merged data to Data Input/Commodity Production\\Nickel-Production-1980-2023.xlsx\n",
      "Processing Zinc...\n",
      "Saved merged data to Data Input/Commodity Production\\Zinc-Production-1980-2023.xlsx\n",
      "All commodities processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def process_commodity_data(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process commodity production data from Excel files and create merged outputs.\n",
    "    \n",
    "    Args:\n",
    "        input_folder (str): Path to the folder containing raw commodity data\n",
    "        output_folder (str): Path to the folder where merged files will be saved\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a list of all commodity folders\n",
    "    commodity_folders = [f for f in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, f))]\n",
    "    \n",
    "    for commodity in commodity_folders:\n",
    "        print(f\"Processing {commodity}...\")\n",
    "        commodity_path = os.path.join(input_folder, commodity)\n",
    "        \n",
    "        # Get all Excel files for this commodity\n",
    "        excel_files = sorted(glob.glob(os.path.join(commodity_path, f\"ComProd-*-{commodity}.xlsx\")))\n",
    "        \n",
    "        # Sort files in chronological order (newest first to oldest last)\n",
    "        excel_files = sorted(excel_files, key=lambda x: int(x.split('-')[1]), reverse=True)\n",
    "        \n",
    "        # Read and process each file\n",
    "        df_list = []\n",
    "        for file_path in excel_files:\n",
    "            df = process_excel_file(file_path)\n",
    "            df_list.append(df)\n",
    "        \n",
    "        # Merge the dataframes\n",
    "        merged_df = merge_dataframes(df_list)\n",
    "        \n",
    "        # Sort columns chronologically\n",
    "        merged_df = sort_columns_chronologically(merged_df)\n",
    "        \n",
    "        # Save the merged data with the specified naming convention\n",
    "        output_path = os.path.join(output_folder, f\"{commodity}-Production-1980-2023.xlsx\")\n",
    "        merged_df.to_excel(output_path, index=False)\n",
    "        \n",
    "        # Remove empty row after header in the output file\n",
    "        remove_empty_row_after_header(output_path)\n",
    "        \n",
    "        print(f\"Saved merged data to {output_path}\")\n",
    "\n",
    "def remove_empty_row_after_header(file_path):\n",
    "    \"\"\"\n",
    "    Remove the empty row after the header in the Excel file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file\n",
    "    \"\"\"\n",
    "    # Read the saved Excel file\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Check if there's at least one row and if the first row is empty\n",
    "    if len(df) > 0 and df.iloc[0].isna().all():\n",
    "        # Remove the empty row\n",
    "        df = df.drop(0).reset_index(drop=True)\n",
    "        # Save the file again\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"Removed empty row after header in {file_path}\")\n",
    "\n",
    "def process_excel_file(file_path):\n",
    "    \"\"\"\n",
    "    Process an individual Excel file:\n",
    "    - Skip the first 4 rows\n",
    "    - Remove the 7th row (which is now the 3rd row after skipping)\n",
    "    - Set the appropriate headers\n",
    "    - Fix year column names\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Processed dataframe with standardized column names\n",
    "    \"\"\"\n",
    "    # Read Excel file, skipping the first 4 rows\n",
    "    df = pd.read_excel(file_path, skiprows=4)\n",
    "    \n",
    "    # Drop the empty row (originally the 7th row, now the 3rd after skipping 4)\n",
    "    df = df.drop(2, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # First, detect the actual property column names\n",
    "    first_col_name = df.columns[0]\n",
    "    second_col_name = df.columns[1]\n",
    "    \n",
    "    # Get the year values from the first row\n",
    "    years = df.iloc[0, 2:].astype(str)\n",
    "    \n",
    "    # Set initial column names preserving the original property column names\n",
    "    df.columns = [first_col_name, second_col_name] + list(years)\n",
    "    \n",
    "    # Remove the first row (which contained the years)\n",
    "    df = df.iloc[1:].reset_index(drop=True)\n",
    "    \n",
    "    # Standardize property column names\n",
    "    column_mapping = {\n",
    "        first_col_name: 'PROP_NAME',\n",
    "        second_col_name: 'PROP_ID'\n",
    "    }\n",
    "    \n",
    "    # Clean year column names and create new column names list\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        if col in [first_col_name, second_col_name]:\n",
    "            new_cols.append(column_mapping[col])\n",
    "        else:\n",
    "            # Remove 'Y' at the end\n",
    "            new_col = col.replace('Y', '')\n",
    "            \n",
    "            # Handle different header formats\n",
    "            if 'COMMODITY_PRODUCTION_TONNE_BY_PERIOD' in new_col:\n",
    "                new_col = new_col.split('_')[-1]  # Extract just the year part\n",
    "            elif any(term in new_col for term in ['TONNE', 'PRODUCTION']):\n",
    "                # Generic handler for any production data columns\n",
    "                parts = new_col.split('_')\n",
    "                # Get the last part which is typically the year\n",
    "                if parts and parts[-1].isdigit():\n",
    "                    new_col = parts[-1]\n",
    "            \n",
    "            new_cols.append(new_col)\n",
    "    \n",
    "    df.columns = new_cols\n",
    "    \n",
    "    # Remove any empty rows\n",
    "    df = df.dropna(how='all').reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_dataframes(df_list):\n",
    "    \"\"\"\n",
    "    Merge multiple dataframes by keeping PROP_NAME and PROP_ID columns from the first dataframe\n",
    "    and appending production data columns from all dataframes.\n",
    "    \n",
    "    Args:\n",
    "        df_list (list): List of dataframes to merge\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Merged dataframe with no empty rows\n",
    "    \"\"\"\n",
    "    if not df_list:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Verify all dataframes have the standardized property columns\n",
    "    for i, df in enumerate(df_list):\n",
    "        if 'PROP_NAME' not in df.columns or 'PROP_ID' not in df.columns:\n",
    "            print(f\"Warning: DataFrame {i} is missing standard property columns. Available columns: {df.columns.tolist()}\")\n",
    "            # This is a defensive measure to prevent errors, but should not be needed with the updated process_excel_file\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # Use the first dataframe as the base\n",
    "    result_df = df_list[0][['PROP_NAME', 'PROP_ID']].copy()\n",
    "    \n",
    "    # Add production data columns from each dataframe\n",
    "    for df in df_list:\n",
    "        production_columns = [col for col in df.columns if col not in ['PROP_NAME', 'PROP_ID']]\n",
    "        if production_columns:  # Only merge if there are production columns\n",
    "            result_df = pd.merge(result_df, df[['PROP_ID'] + production_columns], on='PROP_ID', how='left')\n",
    "    \n",
    "    # Ensure there are no empty rows in the final result\n",
    "    result_df = result_df.dropna(how='all').reset_index(drop=True)\n",
    "    \n",
    "    # Remove any rows that are completely empty (except for PROP_NAME and PROP_ID)\n",
    "    if len(result_df) > 0:\n",
    "        # Check if a row contains data in any column other than PROP_NAME and PROP_ID\n",
    "        data_columns = [col for col in result_df.columns if col not in ['PROP_NAME', 'PROP_ID']]\n",
    "        if data_columns:\n",
    "            has_data = ~result_df[data_columns].isna().all(axis=1)\n",
    "            result_df = result_df[has_data].reset_index(drop=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def sort_columns_chronologically(df):\n",
    "    \"\"\"\n",
    "    Sort the production columns in chronological order (1980-2023).\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe with production data\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe with chronologically sorted columns\n",
    "    \"\"\"\n",
    "    # Separate identification columns and year columns\n",
    "    id_columns = ['PROP_NAME', 'PROP_ID']\n",
    "    year_columns = [col for col in df.columns if col not in id_columns]\n",
    "    \n",
    "    # Sort year columns\n",
    "    year_columns.sort(key=lambda x: int(x) if x.isdigit() else 0)\n",
    "    \n",
    "    # Reorder columns\n",
    "    sorted_df = df[id_columns + year_columns]\n",
    "    \n",
    "    return sorted_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the data processing pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        input_folder = \"Data Input/Commodity Production_Raw\"\n",
    "        output_folder = \"Data Input/Commodity Production\"\n",
    "        process_commodity_data(input_folder, output_folder)\n",
    "        print(\"All commodities processed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing commodity data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1fdad97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting metadata merge process...\n",
      "Loaded metadata file with 34843 rows.\n",
      "Found 7 Excel files to process.\n",
      "Processing Bauxite-Production-1980-2023.xlsx...\n",
      "Merged Bauxite-Production-1980-2023.xlsx - 73 matching rows found.\n",
      "Saved merged data back to Bauxite-Production-1980-2023.xlsx\n",
      "Processing Copper-Production-1980-2023.xlsx...\n",
      "Merged Copper-Production-1980-2023.xlsx - 888 matching rows found.\n",
      "Saved merged data back to Copper-Production-1980-2023.xlsx\n",
      "Processing IronOre-Production-1980-2023.xlsx...\n",
      "Merged IronOre-Production-1980-2023.xlsx - 716 matching rows found.\n",
      "Saved merged data back to IronOre-Production-1980-2023.xlsx\n",
      "Processing Lithium-Production-1980-2023.xlsx...\n",
      "Merged Lithium-Production-1980-2023.xlsx - 38 matching rows found.\n",
      "Saved merged data back to Lithium-Production-1980-2023.xlsx\n",
      "Processing Manganese-Production-1980-2023.xlsx...\n",
      "Merged Manganese-Production-1980-2023.xlsx - 37 matching rows found.\n",
      "Saved merged data back to Manganese-Production-1980-2023.xlsx\n",
      "Processing Nickel-Production-1980-2023.xlsx...\n",
      "Merged Nickel-Production-1980-2023.xlsx - 401 matching rows found.\n",
      "Saved merged data back to Nickel-Production-1980-2023.xlsx\n",
      "Processing Zinc-Production-1980-2023.xlsx...\n",
      "Merged Zinc-Production-1980-2023.xlsx - 534 matching rows found.\n",
      "Saved merged data back to Zinc-Production-1980-2023.xlsx\n",
      "Metadata merge process completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def merge_with_metadata(commodity_folder, metadata_file):\n",
    "    \"\"\"\n",
    "    Merge all Excel files in the commodity folder with metadata file based on PROP_ID.\n",
    "    \n",
    "    Args:\n",
    "        commodity_folder (str): Path to the folder containing commodity files\n",
    "        metadata_file (str): Path to the metadata file\n",
    "    \"\"\"\n",
    "    print(f\"Starting metadata merge process...\")\n",
    "    \n",
    "    # Check if metadata file exists\n",
    "    if not os.path.exists(metadata_file):\n",
    "        print(f\"Error: Metadata file {metadata_file} not found.\")\n",
    "        return\n",
    "    \n",
    "    # Load the metadata file\n",
    "    try:\n",
    "        metadata_df = pd.read_excel(metadata_file)\n",
    "        print(f\"Loaded metadata file with {len(metadata_df)} rows.\")\n",
    "        \n",
    "        # Verify metadata file has PROP_ID column\n",
    "        if 'PROP_ID' not in metadata_df.columns:\n",
    "            print(f\"Error: Metadata file does not contain PROP_ID column.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Get all commodity Excel files\n",
    "    excel_files = glob.glob(os.path.join(commodity_folder, \"*.xlsx\"))\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(f\"No Excel files found in {commodity_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(excel_files)} Excel files to process.\")\n",
    "    \n",
    "    # Process each Excel file\n",
    "    for file_path in excel_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\"Processing {file_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load the commodity file\n",
    "            commodity_df = pd.read_excel(file_path)\n",
    "            \n",
    "            # Verify commodity file has PROP_ID column\n",
    "            if 'PROP_ID' not in commodity_df.columns:\n",
    "                print(f\"Warning: File {file_name} does not contain PROP_ID column. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Identify production columns (year columns from 1980-2023)\n",
    "            prod_columns = []\n",
    "            for col in commodity_df.columns:\n",
    "                # Check if column name is a year between 1980 and 2023\n",
    "                try:\n",
    "                    year = int(col)\n",
    "                    if 1980 <= year <= 2023:\n",
    "                        prod_columns.append(col)\n",
    "                except (ValueError, TypeError):\n",
    "                    # Not a year column\n",
    "                    pass\n",
    "            \n",
    "            # Identify metadata columns (exclude duplicated columns)\n",
    "            metadata_columns = [col for col in metadata_df.columns \n",
    "                               if col != 'PROP_NAME']  # PROP_ID will be used for merge\n",
    "            \n",
    "            # Merge with metadata on PROP_ID, keeping only matching rows\n",
    "            merged_df = pd.merge(commodity_df, metadata_df, on='PROP_ID', how='inner')\n",
    "            \n",
    "            # Check if any rows were matched\n",
    "            if len(merged_df) == 0:\n",
    "                print(f\"Warning: No matching PROP_IDs found in {file_name}. File will be empty.\")\n",
    "            else:\n",
    "                print(f\"Merged {file_name} - {len(merged_df)} matching rows found.\")\n",
    "            \n",
    "            # Remove duplicate PROP_NAME column if it exists in both files\n",
    "            if 'PROP_NAME_x' in merged_df.columns and 'PROP_NAME_y' in merged_df.columns:\n",
    "                # Keep the metadata version (usually PROP_NAME_y)\n",
    "                merged_df = merged_df.drop('PROP_NAME_x', axis=1)\n",
    "                merged_df = merged_df.rename(columns={'PROP_NAME_y': 'PROP_NAME'})\n",
    "            \n",
    "            # Reorder columns: metadata columns first, then production columns\n",
    "            # Start with PROP_ID and PROP_NAME\n",
    "            final_columns = ['PROP_ID', 'PROP_NAME']\n",
    "            \n",
    "            # Add other metadata columns (that aren't already in final_columns)\n",
    "            other_metadata = [col for col in metadata_df.columns \n",
    "                             if col not in final_columns and col != 'PROP_NAME_y']\n",
    "            final_columns.extend(other_metadata)\n",
    "            \n",
    "            # Add production columns at the end\n",
    "            final_columns.extend([col for col in prod_columns if col not in final_columns])\n",
    "            \n",
    "            # Filter to include only columns that exist in merged_df\n",
    "            final_columns = [col for col in final_columns if col in merged_df.columns]\n",
    "            \n",
    "            # Reorder the dataframe\n",
    "            merged_df = merged_df[final_columns]\n",
    "            \n",
    "            # Save back to the original file\n",
    "            merged_df.to_excel(file_path, index=False)\n",
    "            print(f\"Saved merged data back to {file_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "    \n",
    "    print(\"Metadata merge process completed.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the metadata merge process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        commodity_folder = \"Data Input/Commodity Production\"\n",
    "        metadata_file = \"Data Input/Meta Data/Meta_Data_Columns_final.xlsx\"\n",
    "        merge_with_metadata(commodity_folder, metadata_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in metadata merge process: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ee185",
   "metadata": {},
   "source": [
    "Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6a72a0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: Bauxite-Production-1980-2023.xlsx\n",
      "No mismatched rows found in Bauxite-Production-1980-2023.xlsx\n",
      "\n",
      "Processing file: Copper-Production-1980-2023.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leshe\\AppData\\Local\\Temp\\ipykernel_12532\\1813254057.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mismatched_rows['SOURCE_FILE'] = filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 329 rows with mismatched commodity type\n",
      "Remaining rows: 559\n",
      "File saved back to: Data Input/Commodity Production\\Copper-Production-1980-2023.xlsx\n",
      "Mismatched rows saved to: Data Input/Commodity Production/Secondary\\Copper_mismatched.xlsx\n",
      "\n",
      "Processing file: IronOre-Production-1980-2023.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leshe\\AppData\\Local\\Temp\\ipykernel_12532\\1813254057.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mismatched_rows['SOURCE_FILE'] = filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 16 rows with mismatched commodity type\n",
      "Remaining rows: 700\n",
      "File saved back to: Data Input/Commodity Production\\IronOre-Production-1980-2023.xlsx\n",
      "Mismatched rows saved to: Data Input/Commodity Production/Secondary\\IronOre_mismatched.xlsx\n",
      "\n",
      "Processing file: Lithium-Production-1980-2023.xlsx\n",
      "Removed 1 rows with mismatched commodity type\n",
      "Remaining rows: 37\n",
      "File saved back to: Data Input/Commodity Production\\Lithium-Production-1980-2023.xlsx\n",
      "Mismatched rows saved to: Data Input/Commodity Production/Secondary\\Lithium_mismatched.xlsx\n",
      "\n",
      "Processing file: Manganese-Production-1980-2023.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leshe\\AppData\\Local\\Temp\\ipykernel_12532\\1813254057.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mismatched_rows['SOURCE_FILE'] = filename\n",
      "C:\\Users\\leshe\\AppData\\Local\\Temp\\ipykernel_12532\\1813254057.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mismatched_rows['SOURCE_FILE'] = filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 3 rows with mismatched commodity type\n",
      "Remaining rows: 34\n",
      "File saved back to: Data Input/Commodity Production\\Manganese-Production-1980-2023.xlsx\n",
      "Mismatched rows saved to: Data Input/Commodity Production/Secondary\\Manganese_mismatched.xlsx\n",
      "\n",
      "Processing file: Nickel-Production-1980-2023.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leshe\\AppData\\Local\\Temp\\ipykernel_12532\\1813254057.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mismatched_rows['SOURCE_FILE'] = filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 261 rows with mismatched commodity type\n",
      "Remaining rows: 140\n",
      "File saved back to: Data Input/Commodity Production\\Nickel-Production-1980-2023.xlsx\n",
      "Mismatched rows saved to: Data Input/Commodity Production/Secondary\\Nickel_mismatched.xlsx\n",
      "\n",
      "Processing file: Zinc-Production-1980-2023.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leshe\\AppData\\Local\\Temp\\ipykernel_12532\\1813254057.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mismatched_rows['SOURCE_FILE'] = filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 229 rows with mismatched commodity type\n",
      "Remaining rows: 305\n",
      "File saved back to: Data Input/Commodity Production\\Zinc-Production-1980-2023.xlsx\n",
      "Mismatched rows saved to: Data Input/Commodity Production/Secondary\\Zinc_mismatched.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def process_commodity_files(folder_path, output_folder):\n",
    "    \"\"\"\n",
    "    Process all Excel files in a folder, removing rows where the PRIMARY_COMMODITY\n",
    "    doesn't match the commodity indicated in the filename.\n",
    "   \n",
    "    Args:\n",
    "        folder_path: Path to the folder containing Excel files\n",
    "        output_folder: Path to save extracted mismatched rows\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "   \n",
    "    # Get all Excel files in the folder\n",
    "    excel_files = glob.glob(os.path.join(folder_path, \"*.xlsx\"))\n",
    "   \n",
    "    # Process each Excel file\n",
    "    for file_path in excel_files:\n",
    "        # Skip the merged file if it exists in the same folder\n",
    "        if \"All_Commodities\" in file_path:\n",
    "            continue\n",
    "           \n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"\\nProcessing file: {filename}\")\n",
    "       \n",
    "        # Extract the commodity name from the filename\n",
    "        # Format: [primary commodity]-Production-1980-2023.xlsx\n",
    "        commodity_name = filename.split('-')[0]\n",
    "       \n",
    "        try:\n",
    "            # Read the Excel file\n",
    "            df = pd.read_excel(file_path)\n",
    "           \n",
    "            # Check if required columns exist\n",
    "            if 'PRIMARY_COMMODITY' not in df.columns:\n",
    "                print(f\"Warning: PRIMARY_COMMODITY column not found in {filename}\")\n",
    "                continue\n",
    "               \n",
    "            if 'PROP_ID' not in df.columns:\n",
    "                print(f\"Warning: PROP_ID column not found in {filename}\")\n",
    "                continue\n",
    "           \n",
    "            # Get initial row count\n",
    "            initial_count = len(df)\n",
    "           \n",
    "            # Handle special cases\n",
    "            if commodity_name == \"IronOre\":\n",
    "                expected_commodity = \"Iron Ore\"\n",
    "            else:\n",
    "                expected_commodity = commodity_name\n",
    "           \n",
    "            # Find rows where PRIMARY_COMMODITY doesn't match the expected value\n",
    "            mismatched_rows = df[df['PRIMARY_COMMODITY'] != expected_commodity]\n",
    "           \n",
    "            # Add filename column to mismatched rows for reference\n",
    "            mismatched_rows['SOURCE_FILE'] = filename\n",
    "           \n",
    "            # Keep only the matching rows\n",
    "            df_filtered = df[df['PRIMARY_COMMODITY'] == expected_commodity]\n",
    "           \n",
    "            # Count removed rows\n",
    "            removed_count = initial_count - len(df_filtered)\n",
    "           \n",
    "            # Save the filtered data back to the original file\n",
    "            df_filtered.to_excel(file_path, index=False)\n",
    "           \n",
    "            # Save mismatched rows to individual file in the output folder\n",
    "            if not mismatched_rows.empty:\n",
    "                # Create output filename based on the original filename\n",
    "                output_filename = os.path.join(output_folder, f\"{commodity_name}_mismatched.xlsx\")\n",
    "                mismatched_rows.to_excel(output_filename, index=False)\n",
    "               \n",
    "                print(f\"Removed {removed_count} rows with mismatched commodity type\")\n",
    "                print(f\"Remaining rows: {len(df_filtered)}\")\n",
    "                print(f\"File saved back to: {file_path}\")\n",
    "                print(f\"Mismatched rows saved to: {output_filename}\")\n",
    "            else:\n",
    "                print(f\"No mismatched rows found in {filename}\")\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define folder paths\n",
    "    input_folder = \"Data Input/Commodity Production\"\n",
    "    output_folder = \"Data Input/Commodity Production/Secondary\"\n",
    "   \n",
    "    # Run the process\n",
    "    process_commodity_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4e0dc754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process files in Data Input/Commodity Production...\n",
      "Minimum non-zero values required: 5\n",
      "Found 7 Excel files to process.\n",
      "Processing Bauxite-Production-1980-2023.xlsx...\n",
      "Found 44 year columns in Bauxite-Production-1980-2023.xlsx.\n",
      "Removed 10 rows from Bauxite-Production-1980-2023.xlsx:\n",
      "  - Rows with zero production: 0\n",
      "  - Rows with insufficient production (<5 non-zero values): 10\n",
      "  - Remaining rows: 63\n",
      "Saved filtered data back to Bauxite-Production-1980-2023.xlsx\n",
      "Processing Copper-Production-1980-2023.xlsx...\n",
      "Found 44 year columns in Copper-Production-1980-2023.xlsx.\n",
      "Removed 181 rows from Copper-Production-1980-2023.xlsx:\n",
      "  - Rows with zero production: 71\n",
      "  - Rows with insufficient production (<5 non-zero values): 110\n",
      "  - Remaining rows: 378\n",
      "Saved filtered data back to Copper-Production-1980-2023.xlsx\n",
      "Processing IronOre-Production-1980-2023.xlsx...\n",
      "Found 44 year columns in IronOre-Production-1980-2023.xlsx.\n",
      "Removed 182 rows from IronOre-Production-1980-2023.xlsx:\n",
      "  - Rows with zero production: 0\n",
      "  - Rows with insufficient production (<5 non-zero values): 182\n",
      "  - Remaining rows: 518\n",
      "Saved filtered data back to IronOre-Production-1980-2023.xlsx\n",
      "Processing Lithium-Production-1980-2023.xlsx...\n",
      "Found 44 year columns in Lithium-Production-1980-2023.xlsx.\n",
      "Removed 16 rows from Lithium-Production-1980-2023.xlsx:\n",
      "  - Rows with zero production: 0\n",
      "  - Rows with insufficient production (<5 non-zero values): 16\n",
      "  - Remaining rows: 21\n",
      "Saved filtered data back to Lithium-Production-1980-2023.xlsx\n",
      "Processing Manganese-Production-1980-2023.xlsx...\n",
      "Found 44 year columns in Manganese-Production-1980-2023.xlsx.\n",
      "Removed 6 rows from Manganese-Production-1980-2023.xlsx:\n",
      "  - Rows with zero production: 0\n",
      "  - Rows with insufficient production (<5 non-zero values): 6\n",
      "  - Remaining rows: 28\n",
      "Saved filtered data back to Manganese-Production-1980-2023.xlsx\n",
      "Processing Nickel-Production-1980-2023.xlsx...\n",
      "Found 44 year columns in Nickel-Production-1980-2023.xlsx.\n",
      "Removed 51 rows from Nickel-Production-1980-2023.xlsx:\n",
      "  - Rows with zero production: 3\n",
      "  - Rows with insufficient production (<5 non-zero values): 48\n",
      "  - Remaining rows: 89\n",
      "Saved filtered data back to Nickel-Production-1980-2023.xlsx\n",
      "Processing Zinc-Production-1980-2023.xlsx...\n",
      "Found 44 year columns in Zinc-Production-1980-2023.xlsx.\n",
      "Removed 90 rows from Zinc-Production-1980-2023.xlsx:\n",
      "  - Rows with zero production: 4\n",
      "  - Rows with insufficient production (<5 non-zero values): 86\n",
      "  - Remaining rows: 215\n",
      "Saved filtered data back to Zinc-Production-1980-2023.xlsx\n",
      "Row removal process completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def remove_zero_rows(folder_path, min_nonzero_values):\n",
    "    \"\"\"\n",
    "    Process all Excel files in the specified folder and remove rows where:\n",
    "    1. All year columns (1980-2023) contain only zeros or empty values, OR\n",
    "    2. The row has fewer than the minimum required non-zero production values\n",
    "   \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing Excel files\n",
    "        min_nonzero_values (int): Minimum number of non-zero values required to keep a row\n",
    "    \"\"\"\n",
    "    print(f\"Starting to process files in {folder_path}...\")\n",
    "    print(f\"Minimum non-zero values required: {min_nonzero_values}\")\n",
    "   \n",
    "    # Get all Excel files in the folder\n",
    "    excel_files = glob.glob(os.path.join(folder_path, \"*.xlsx\"))\n",
    "   \n",
    "    if not excel_files:\n",
    "        print(f\"No Excel files found in {folder_path}\")\n",
    "        return\n",
    "   \n",
    "    print(f\"Found {len(excel_files)} Excel files to process.\")\n",
    "   \n",
    "    # Process each Excel file\n",
    "    for file_path in excel_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\"Processing {file_name}...\")\n",
    "       \n",
    "        try:\n",
    "            # Load the file\n",
    "            df = pd.read_excel(file_path)\n",
    "            initial_rows = len(df)\n",
    "           \n",
    "            # Identify year columns (1980-2023)\n",
    "            year_columns = []\n",
    "            for col in df.columns:\n",
    "                try:\n",
    "                    year = int(col)\n",
    "                    if 1980 <= year <= 2023:\n",
    "                        year_columns.append(col)\n",
    "                except (ValueError, TypeError):\n",
    "                    # Not a year column\n",
    "                    pass\n",
    "           \n",
    "            if not year_columns:\n",
    "                print(f\"Warning: No year columns found in {file_name}. Skipping file.\")\n",
    "                continue\n",
    "           \n",
    "            print(f\"Found {len(year_columns)} year columns in {file_name}.\")\n",
    "           \n",
    "            # Count non-zero, non-empty values for each row\n",
    "            nonzero_counts = df[year_columns].apply(lambda row:\n",
    "                sum(\n",
    "                    1 for value in row \n",
    "                    if (not pd.isna(value)) and (value != 0) and (value != '0') and (value != '')\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "           \n",
    "            # Create a mask to identify rows that meet the minimum threshold\n",
    "            meets_threshold = nonzero_counts >= min_nonzero_values\n",
    "           \n",
    "            # Keep only rows that meet the minimum threshold\n",
    "            df_filtered = df[meets_threshold].reset_index(drop=True)\n",
    "           \n",
    "            # Calculate how many rows were removed\n",
    "            removed_rows = initial_rows - len(df_filtered)\n",
    "            \n",
    "            # Show breakdown of removed rows\n",
    "            zero_rows = sum(nonzero_counts == 0)\n",
    "            insufficient_rows = sum((nonzero_counts > 0) & (nonzero_counts < min_nonzero_values))\n",
    "            \n",
    "            print(f\"Removed {removed_rows} rows from {file_name}:\")\n",
    "            print(f\"  - Rows with zero production: {zero_rows}\")\n",
    "            print(f\"  - Rows with insufficient production (<{min_nonzero_values} non-zero values): {insufficient_rows}\")\n",
    "            print(f\"  - Remaining rows: {len(df_filtered)}\")\n",
    "           \n",
    "            # Save the filtered data back to the original file\n",
    "            df_filtered.to_excel(file_path, index=False)\n",
    "            print(f\"Saved filtered data back to {file_name}\")\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "   \n",
    "    print(\"Row removal process completed.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the row removal process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        folder_path = \"Data Input/Commodity Production\"\n",
    "        # Set minimum non-zero values required (default is 5)\n",
    "        min_nonzero_threshold = 5\n",
    "        remove_zero_rows(folder_path, min_nonzero_threshold)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in row removal process: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "97afb104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Bauxite-Production-1980-2023.xlsx...\n",
      "  Updated Bauxite-Production-1980-2023.xlsx:\n",
      "    - Removed 0 rows due to development stage\n",
      "    - Removed 0 rows due to mine type\n",
      "    - Removed 1 rows due to empty mine type\n",
      "    - Removed 4 rows due to Closed + (Inactive or Care and Maintenance) combination\n",
      "    - Total removed: 5 rows\n",
      "    - Remaining rows: 58\n",
      "Processing Copper-Production-1980-2023.xlsx...\n",
      "  Updated Copper-Production-1980-2023.xlsx:\n",
      "    - Removed 25 rows due to development stage\n",
      "    - Removed 3 rows due to mine type\n",
      "    - Removed 2 rows due to empty mine type\n",
      "    - Removed 35 rows due to Closed + (Inactive or Care and Maintenance) combination\n",
      "    - Total removed: 65 rows\n",
      "    - Remaining rows: 313\n",
      "Processing IronOre-Production-1980-2023.xlsx...\n",
      "  Updated IronOre-Production-1980-2023.xlsx:\n",
      "    - Removed 9 rows due to development stage\n",
      "    - Removed 5 rows due to mine type\n",
      "    - Removed 50 rows due to empty mine type\n",
      "    - Removed 31 rows due to Closed + (Inactive or Care and Maintenance) combination\n",
      "    - Total removed: 95 rows\n",
      "    - Remaining rows: 423\n",
      "Processing Lithium-Production-1980-2023.xlsx...\n",
      "  Updated Lithium-Production-1980-2023.xlsx:\n",
      "    - Removed 1 rows due to development stage\n",
      "    - Removed 0 rows due to mine type\n",
      "    - Removed 1 rows due to empty mine type\n",
      "    - Removed 0 rows due to Closed + (Inactive or Care and Maintenance) combination\n",
      "    - Total removed: 2 rows\n",
      "    - Remaining rows: 19\n",
      "Processing Manganese-Production-1980-2023.xlsx...\n",
      "  Updated Manganese-Production-1980-2023.xlsx:\n",
      "    - Removed 3 rows due to development stage\n",
      "    - Removed 0 rows due to mine type\n",
      "    - Removed 0 rows due to empty mine type\n",
      "    - Removed 0 rows due to Closed + (Inactive or Care and Maintenance) combination\n",
      "    - Total removed: 3 rows\n",
      "    - Remaining rows: 25\n",
      "Processing Nickel-Production-1980-2023.xlsx...\n",
      "  Updated Nickel-Production-1980-2023.xlsx:\n",
      "    - Removed 12 rows due to development stage\n",
      "    - Removed 0 rows due to mine type\n",
      "    - Removed 4 rows due to empty mine type\n",
      "    - Removed 9 rows due to Closed + (Inactive or Care and Maintenance) combination\n",
      "    - Total removed: 25 rows\n",
      "    - Remaining rows: 64\n",
      "Processing Zinc-Production-1980-2023.xlsx...\n",
      "  Updated Zinc-Production-1980-2023.xlsx:\n",
      "    - Removed 3 rows due to development stage\n",
      "    - Removed 1 rows due to mine type\n",
      "    - Removed 3 rows due to empty mine type\n",
      "    - Removed 25 rows due to Closed + (Inactive or Care and Maintenance) combination\n",
      "    - Total removed: 32 rows\n",
      "    - Remaining rows: 183\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_xlsx_files(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all xlsx files in the input folder by:\n",
    "    1. Removing rows with specified values in DEV_STAGE column\n",
    "    2. Removing rows with specified values in MINE_TYPE1 column\n",
    "    3. Removing rows with empty/null values in MINE_TYPE1 column\n",
    "    4. Removing rows that have 'Closed' in DEV_STAGE AND ('Inactive' OR 'Care and Maintenance') in ACTV_STATUS\n",
    "   \n",
    "    Args:\n",
    "        input_folder (str): Path to folder with input xlsx files\n",
    "        output_folder (str): Path to save processed xlsx files (not used, as we're saving to the same file)\n",
    "    \"\"\"\n",
    "    # List of DEV_STAGE values to filter out\n",
    "    dev_stages_to_remove = [\n",
    "        'Grassroots',\n",
    "        'Exploration',\n",
    "        'Advanced Exploration',\n",
    "        'Target Outline',\n",
    "        'Reserves Development',\n",
    "        'Construction Planned',\n",
    "        'Construction Started',\n",
    "        'Prefeas/Scoping',\n",
    "        'Feasibility Started',\n",
    "        'Feasibility Complete',\n",
    "        'Feasibility'\n",
    "    ]\n",
    "   \n",
    "    # List of MINE_TYPE1 values to filter out\n",
    "    mine_types_to_remove = [\n",
    "        'Dredging',\n",
    "        'Dump',\n",
    "        'Tailings',\n",
    "        'Stock Pile',\n",
    "        'Placer',\n",
    "        'Ocean',\n",
    "        'In-Situ Leach'\n",
    "    ]\n",
    "   \n",
    "    # Get all xlsx files in the input folder\n",
    "    try:\n",
    "        files = [f for f in os.listdir(input_folder) if f.endswith('.xlsx')]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input folder '{input_folder}' not found.\")\n",
    "        return\n",
    "   \n",
    "    if not files:\n",
    "        print(f\"No xlsx files found in '{input_folder}'.\")\n",
    "        return\n",
    "   \n",
    "    # Process each file\n",
    "    for file in files:\n",
    "        input_path = os.path.join(input_folder, file)\n",
    "       \n",
    "        print(f\"Processing {file}...\")\n",
    "       \n",
    "        try:\n",
    "            # Read the Excel file\n",
    "            df = pd.read_excel(input_path)\n",
    "            original_rows = len(df)\n",
    "           \n",
    "            # Check if required columns exist\n",
    "            required_columns = ['DEV_STAGE', 'START_UP_YR', 'MINE_TYPE1', 'ACTV_STATUS']\n",
    "            missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "           \n",
    "            if missing_columns:\n",
    "                print(f\"Warning: Required columns {missing_columns} not found in {file}, skipping.\")\n",
    "                continue\n",
    "           \n",
    "            # Filter out rows with specified DEV_STAGE values\n",
    "            df_filtered_stage = df[~df['DEV_STAGE'].isin(dev_stages_to_remove)]\n",
    "           \n",
    "            # Filter out rows with specified MINE_TYPE1 values\n",
    "            df_filtered_minetype = df_filtered_stage[~df_filtered_stage['MINE_TYPE1'].isin(mine_types_to_remove)]\n",
    "            \n",
    "            # Filter out rows with empty/null MINE_TYPE1 values\n",
    "            df_filtered_minetype_nonempty = df_filtered_minetype[df_filtered_minetype['MINE_TYPE1'].notna() & (df_filtered_minetype['MINE_TYPE1'] != '')]\n",
    "            \n",
    "            # Filter out rows that have 'Closed' in DEV_STAGE AND ('Inactive' OR 'Care and Maintenance') in ACTV_STATUS\n",
    "            closed_and_inactive_mask = (df_filtered_minetype_nonempty['DEV_STAGE'] == 'Closed') & (\n",
    "                (df_filtered_minetype_nonempty['ACTV_STATUS'] == 'Inactive') | \n",
    "                (df_filtered_minetype_nonempty['ACTV_STATUS'] == 'Care and Maintenance')\n",
    "            )\n",
    "            df_filtered = df_filtered_minetype_nonempty[~closed_and_inactive_mask]\n",
    "           \n",
    "            # Save the filtered DataFrame back to the same Excel file\n",
    "            df_filtered.to_excel(input_path, index=False)\n",
    "           \n",
    "            # Report results\n",
    "            rows_removed_stage = original_rows - len(df_filtered_stage)\n",
    "            rows_removed_mine_type = len(df_filtered_stage) - len(df_filtered_minetype)\n",
    "            rows_removed_empty_minetype = len(df_filtered_minetype) - len(df_filtered_minetype_nonempty)\n",
    "            rows_removed_closed_inactive = len(df_filtered_minetype_nonempty) - len(df_filtered)\n",
    "            total_rows_removed = original_rows - len(df_filtered)\n",
    "           \n",
    "            print(f\"  Updated {file}:\")\n",
    "            print(f\"    - Removed {rows_removed_stage} rows due to development stage\")\n",
    "            print(f\"    - Removed {rows_removed_mine_type} rows due to mine type\")\n",
    "            print(f\"    - Removed {rows_removed_empty_minetype} rows due to empty mine type\")\n",
    "            print(f\"    - Removed {rows_removed_closed_inactive} rows due to Closed + (Inactive or Care and Maintenance) combination\")\n",
    "            print(f\"    - Total removed: {total_rows_removed} rows\")\n",
    "            print(f\"    - Remaining rows: {len(df_filtered)}\")\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "   \n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"Data Input/Commodity Production\"\n",
    "    output_folder = \"Data Input/Commodity Production\"  # Not used but kept for compatibility\n",
    "    process_xlsx_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1694cb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging commodity files...\n",
      "Reading file: Bauxite-Production-1980-2023.xlsx...\n",
      "Reading file: Copper-Production-1980-2023.xlsx...\n",
      "Reading file: IronOre-Production-1980-2023.xlsx...\n",
      "Reading file: Lithium-Production-1980-2023.xlsx...\n",
      "Reading file: Manganese-Production-1980-2023.xlsx...\n",
      "Reading file: Nickel-Production-1980-2023.xlsx...\n",
      "Reading file: Zinc-Production-1980-2023.xlsx...\n",
      "Successfully merged 7 commodity files with 1085 total rows.\n",
      "\n",
      "Merging operation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_main_commodity_files():\n",
    "    \"\"\"\n",
    "    Merges all xlsx files in the main commodity folder\n",
    "   \n",
    "    Returns:\n",
    "        str: Path to the merged commodities file\n",
    "    \"\"\"\n",
    "    # Define paths\n",
    "    main_folder = \"Data Input/Commodity Production\"\n",
    "    merged_folder = \"Data Output/Commodity Production\"\n",
    "   \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(merged_folder, exist_ok=True)\n",
    "   \n",
    "    # Define output file\n",
    "    all_commodities_file = os.path.join(merged_folder, \"Commodity_Production-1980_2023.xlsx\")\n",
    "   \n",
    "    print(\"Merging commodity files...\")\n",
    "   \n",
    "    # Initialize list to store all dataframes\n",
    "    all_dfs = []\n",
    "   \n",
    "    # Process main folder files\n",
    "    main_files = [f for f in os.listdir(main_folder) if f.endswith('.xlsx')]\n",
    "    main_file_count = 0\n",
    "   \n",
    "    for file in main_files:\n",
    "        file_path = os.path.join(main_folder, file)\n",
    "        # Skip directories (like subdirectories)\n",
    "        if os.path.isdir(file_path):\n",
    "            continue\n",
    "           \n",
    "        try:\n",
    "            print(f\"Reading file: {file}...\")\n",
    "            df = pd.read_excel(file_path)\n",
    "            all_dfs.append(df)\n",
    "            main_file_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "       \n",
    "    # Merge all dataframes if we have any\n",
    "    if all_dfs:\n",
    "        merged_main = pd.concat(all_dfs, ignore_index=True)\n",
    "        merged_main.to_excel(all_commodities_file, index=False)\n",
    "        print(f\"Successfully merged {main_file_count} commodity files \"\n",
    "              f\"with {len(merged_main)} total rows.\")\n",
    "    else:\n",
    "        print(\"No valid files to merge.\")\n",
    "   \n",
    "    return all_commodities_file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the merge operation for commodity files\n",
    "    commodities_file = merge_main_commodity_files()\n",
    "   \n",
    "    print(\"\\nMerging operation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca12962",
   "metadata": {},
   "source": [
    "Processing part \n",
    "    1) Merge reserves data\n",
    "    2) convert iron ore, manganese ore, to metal content and merge ore grade column into dataset\n",
    "    3) literature review based start up year imputation --> there are 76 --> DO THIS FIRST\n",
    "    4) literature review based closure year imputaiton --> DO THIS FIRST --> it is done for mines where there is production after the closure year\n",
    "    5) if there is a non-zero production value before the start up year, change start up year to that year\n",
    "    6) If there is a non-zero proudction value after projected closure year, change closure year to that year --> still doing this\n",
    "    7) if start up year has 0 as production change it to the first year with non-zero production\n",
    "    8) conduct hieararchical median based imputation for projected closure year of mines with missing closure years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b84808e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Excel file merge process...\n",
      "==================================================\n",
      "Reading reserves file...\n",
      "Reserves file loaded: 9206 rows, 49 columns\n",
      "Reading production file...\n",
      "Production file loaded: 1085 rows, 57 columns\n",
      "\n",
      "Reserves file columns:\n",
      "['PROP_NAME_x', 'PROP_ID', 'PRIMARY_COMMODITY_x', 'PROP_NAME_y', 'PRIMARY_COMMODITY_y', 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
      "\n",
      "Production file columns:\n",
      "['PROP_ID', 'PROP_NAME', 'PRIMARY_COMMODITY', 'COMMODITIES_LIST', 'COUNTRY_NAME', 'LATITUDE', 'LONGITUDE', 'DEV_STAGE', 'ACTV_STATUS', 'MINE_TYPE1', 'GEOLOGIC_ORE_BODY_TYPE', 'START_UP_YR', 'PROJ_CLOSURE_YR', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "\n",
      "Found 44 year columns: [1980, 1981, 1982, 1983, 1984]...\n",
      "\n",
      "Calculating maximum reserves per PROP_ID...\n",
      "Calculated maximum reserves for 3243 unique PROP_IDs\n",
      "Sample reserves data:\n",
      "   PROP_ID  Reserves_Tonnage\n",
      "0    24463      1.009700e+08\n",
      "1    24469      4.980000e+08\n",
      "2    24470      5.650000e+08\n",
      "3    24473      1.230300e+10\n",
      "4    24474      1.647000e+08\n",
      "\n",
      "Will insert Reserves_Tonnage column at position 11 (before START_UP_YR)\n",
      "\n",
      "Merging data...\n",
      "Merge completed: 1085 rows, 58 columns\n",
      "Records with reserves data: 569\n",
      "Records without reserves data: 516\n",
      "\n",
      "Saving merged file to: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n",
      "File saved successfully!\n",
      "\n",
      "==================================================\n",
      "MERGE SUMMARY\n",
      "==================================================\n",
      "Original production records: 1085\n",
      "Unique PROP_IDs in production: 1085\n",
      "Unique PROP_IDs in reserves: 9206\n",
      "Matched records with reserves: 569\n",
      "Unmatched records: 516\n",
      "\n",
      "Reserves statistics:\n",
      "Min reserves: 200,000\n",
      "Max reserves: 12,303,000,000\n",
      "Mean reserves: 542,547,278\n",
      "\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_reserves_to_production():\n",
    "    \"\"\"\n",
    "    Merges reserves data to production data based on PROP_ID.\n",
    "    Finds the highest reserves value per mine across all years (1980-2023).\n",
    "    \"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    reserves_file = \"Data Input/Reserves/Reserves_Ore_Tonnage/Reserves_Tonnage-1980_2023.xlsx\"\n",
    "    production_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        # Read the reserves file\n",
    "        print(\"Reading reserves file...\")\n",
    "        reserves_df = pd.read_excel(reserves_file)\n",
    "        print(f\"Reserves file loaded: {reserves_df.shape[0]} rows, {reserves_df.shape[1]} columns\")\n",
    "        \n",
    "        # Read the production file\n",
    "        print(\"Reading production file...\")\n",
    "        production_df = pd.read_excel(production_file)\n",
    "        print(f\"Production file loaded: {production_df.shape[0]} rows, {production_df.shape[1]} columns\")\n",
    "        \n",
    "        # Display column names for verification\n",
    "        print(\"\\nReserves file columns:\")\n",
    "        print(reserves_df.columns.tolist())\n",
    "        print(\"\\nProduction file columns:\")\n",
    "        print(production_df.columns.tolist())\n",
    "        \n",
    "        # Check if PROP_ID exists in both files\n",
    "        if 'PROP_ID' not in reserves_df.columns:\n",
    "            raise ValueError(\"PROP_ID column not found in reserves file\")\n",
    "        if 'PROP_ID' not in production_df.columns:\n",
    "            raise ValueError(\"PROP_ID column not found in production file\")\n",
    "        \n",
    "        # Identify year columns (1980-2023) in reserves data\n",
    "        year_columns = []\n",
    "        for col in reserves_df.columns:\n",
    "            # Check if column name is a year between 1980-2023\n",
    "            try:\n",
    "                year = int(str(col))\n",
    "                if 1980 <= year <= 2023:\n",
    "                    year_columns.append(col)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nFound {len(year_columns)} year columns: {year_columns[:5]}...\" if len(year_columns) > 5 else f\"\\nFound year columns: {year_columns}\")\n",
    "        \n",
    "        if not year_columns:\n",
    "            print(\"Warning: No year columns found. Checking for columns containing years...\")\n",
    "            # Alternative approach - look for columns that might contain year data\n",
    "            year_columns = [col for col in reserves_df.columns \n",
    "                          if any(str(year) in str(col) for year in range(1980, 2024))]\n",
    "            print(f\"Alternative year columns found: {year_columns}\")\n",
    "        \n",
    "        # Calculate maximum reserves value for each PROP_ID\n",
    "        print(\"\\nCalculating maximum reserves per PROP_ID...\")\n",
    "        \n",
    "        # Create a copy of reserves data for processing\n",
    "        reserves_work = reserves_df.copy()\n",
    "        \n",
    "        # Convert year columns to numeric, replacing non-numeric values with NaN\n",
    "        for col in year_columns:\n",
    "            reserves_work[col] = pd.to_numeric(reserves_work[col], errors='coerce')\n",
    "        \n",
    "        # Calculate max reserves for each PROP_ID across all years\n",
    "        # First, calculate the maximum value across year columns for each row\n",
    "        reserves_work['Reserves_Tonnage'] = reserves_work[year_columns].max(axis=1, skipna=True)\n",
    "        \n",
    "        # Filter out rows where the maximum value is 0, NaN, or negative\n",
    "        reserves_work = reserves_work[\n",
    "            (reserves_work['Reserves_Tonnage'] > 0) & \n",
    "            (reserves_work['Reserves_Tonnage'].notna()) &\n",
    "            (reserves_work['Reserves_Tonnage'] != np.inf)\n",
    "        ]\n",
    "        \n",
    "        # Then group by PROP_ID and take the maximum reserves value per property\n",
    "        reserves_summary = reserves_work.groupby('PROP_ID')['Reserves_Tonnage'].max().reset_index()\n",
    "        \n",
    "        print(f\"Calculated maximum reserves for {len(reserves_summary)} unique PROP_IDs\")\n",
    "        print(f\"Sample reserves data:\")\n",
    "        print(reserves_summary.head())\n",
    "        \n",
    "        # Find the position to insert the new column (before START_UP_YR)\n",
    "        if 'START_UP_YR' in production_df.columns:\n",
    "            insert_position = production_df.columns.get_loc('START_UP_YR')\n",
    "            print(f\"\\nWill insert Reserves_Tonnage column at position {insert_position} (before START_UP_YR)\")\n",
    "        else:\n",
    "            print(\"\\nSTART_UP_YR column not found. Will append Reserves_Tonnage at the end.\")\n",
    "            insert_position = len(production_df.columns)\n",
    "        \n",
    "        # Merge reserves data with production data\n",
    "        print(\"\\nMerging data...\")\n",
    "        merged_df = production_df.merge(reserves_summary, on='PROP_ID', how='left')\n",
    "        \n",
    "        # Reorder columns to place Reserves_Tonnage in the correct position\n",
    "        columns = list(production_df.columns)\n",
    "        if 'START_UP_YR' in columns:\n",
    "            # Insert before START_UP_YR\n",
    "            columns.insert(insert_position, 'Reserves_Tonnage')\n",
    "            merged_df = merged_df[columns]\n",
    "        \n",
    "        print(f\"Merge completed: {merged_df.shape[0]} rows, {merged_df.shape[1]} columns\")\n",
    "        print(f\"Records with reserves data: {merged_df['Reserves_Tonnage'].notna().sum()}\")\n",
    "        print(f\"Records without reserves data: {merged_df['Reserves_Tonnage'].isna().sum()}\")\n",
    "        \n",
    "        # Save the merged file\n",
    "        output_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "        print(f\"\\nSaving merged file to: {output_file}\")\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        merged_df.to_excel(output_file, index=False)\n",
    "        print(\"File saved successfully!\")\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MERGE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Original production records: {len(production_df)}\")\n",
    "        print(f\"Unique PROP_IDs in production: {production_df['PROP_ID'].nunique()}\")\n",
    "        print(f\"Unique PROP_IDs in reserves: {reserves_df['PROP_ID'].nunique()}\")\n",
    "        print(f\"Matched records with reserves: {merged_df['Reserves_Tonnage'].notna().sum()}\")\n",
    "        print(f\"Unmatched records: {merged_df['Reserves_Tonnage'].isna().sum()}\")\n",
    "        \n",
    "        if merged_df['Reserves_Tonnage'].notna().sum() > 0:\n",
    "            print(f\"\\nReserves statistics:\")\n",
    "            print(f\"Min reserves: {merged_df['Reserves_Tonnage'].min():,.0f}\")\n",
    "            print(f\"Max reserves: {merged_df['Reserves_Tonnage'].max():,.0f}\")\n",
    "            print(f\"Mean reserves: {merged_df['Reserves_Tonnage'].mean():,.0f}\")\n",
    "        \n",
    "        return merged_df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Excel file merge process...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        result = merge_reserves_to_production()\n",
    "        print(\"\\nProcess completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nProcess failed with error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "33415c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged file to Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "reserves_file = \"Data Input/Reserves/Reserves_Contained/Commodities_Reserves_Contained.xlsx\"\n",
    "commodity_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "output_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "\n",
    "# Load data\n",
    "reserves_df = pd.read_excel(reserves_file, engine=\"openpyxl\")\n",
    "commodity_df = pd.read_excel(commodity_file, engine=\"openpyxl\")\n",
    "\n",
    "# Keep only relevant columns from reserves\n",
    "reserves_df = reserves_df[[\"PROP_ID\", \"Reserves_Contained\"]]\n",
    "\n",
    "# Merge on PROP_ID\n",
    "merged_df = commodity_df.merge(reserves_df, on=\"PROP_ID\", how=\"left\")\n",
    "\n",
    "# Reorder columns: place Reserves_Contained after Reserves_Tonnage\n",
    "cols = list(merged_df.columns)\n",
    "if \"Reserves_Tonnage\" in cols and \"Reserves_Contained\" in cols:\n",
    "    idx = cols.index(\"Reserves_Tonnage\") + 1\n",
    "    # Remove and reinsert at the right place\n",
    "    cols.insert(idx, cols.pop(cols.index(\"Reserves_Contained\")))\n",
    "    merged_df = merged_df[cols]\n",
    "\n",
    "# Save result\n",
    "merged_df.to_excel(output_file, index=False)\n",
    "print(f\"Saved merged file to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d2e2114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge complete. Applied 0.529 multiplier to bauxite grades from ore-grade file (before %→decimal).\n",
      " - From ore-grade file (targets): 161\n",
      " - Filled Iron Ore via country/default: 306\n",
      " - Filled Manganese (0.445): 11\n",
      " - Filled Bauxite default (0.245): 28\n",
      "Saved to: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- File paths ---\n",
    "ore_grade_path = \"Data Input/Ore Grade/Commodities_Ore_Grade.xlsx\"\n",
    "prod_path = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "\n",
    "# --- Load data ---\n",
    "grades = pd.read_excel(ore_grade_path, dtype={\"PROP_ID\": str})\n",
    "prod = pd.read_excel(prod_path, dtype={\"PROP_ID\": str})\n",
    "\n",
    "# Keep only needed cols (retain PRIMARY_COMMODITY if present so we can detect bauxite)\n",
    "keep_cols = [c for c in [\"PROP_ID\", \"Ore_Grade\", \"PRIMARY_COMMODITY\"] if c in grades.columns]\n",
    "grades = grades[keep_cols].drop_duplicates(subset=[\"PROP_ID\"])\n",
    "\n",
    "# If PRIMARY_COMMODITY not in ore-grade file, map it in from production (for tagging bauxite rows)\n",
    "if \"PRIMARY_COMMODITY\" not in grades.columns:\n",
    "    grades = grades.merge(\n",
    "        prod[[\"PROP_ID\", \"PRIMARY_COMMODITY\"]].drop_duplicates(\"PROP_ID\"),\n",
    "        on=\"PROP_ID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "# --- Adjust bauxite grades in the ore-grade file: multiply by 0.529 BEFORE percent→decimal ---\n",
    "# (Leaves defaults/fallbacks unchanged; only rows that already have a bauxite grade are adjusted.)\n",
    "baux_mask_in_grades = grades[\"PRIMARY_COMMODITY\"].eq(\"Bauxite\")\n",
    "grades.loc[baux_mask_in_grades, \"Ore_Grade\"] = pd.to_numeric(\n",
    "    grades.loc[baux_mask_in_grades, \"Ore_Grade\"], errors=\"coerce\"\n",
    ") * 0.529\n",
    "\n",
    "# Convert Ore_Grade from percent to decimal (e.g., 45 -> 0.45)\n",
    "grades[\"Ore_Grade\"] = pd.to_numeric(grades[\"Ore_Grade\"], errors=\"coerce\") / 100.0\n",
    "\n",
    "# Drop helper column to avoid duplicate columns on merge\n",
    "grades = grades[[\"PROP_ID\", \"Ore_Grade\"]]\n",
    "\n",
    "# --- Merge Ore_Grade into production (left join on PROP_ID) ---\n",
    "prod = prod.merge(grades, on=\"PROP_ID\", how=\"left\")\n",
    "\n",
    "# --- Fill missing Ore_Grade for specific commodities (fallbacks) ---\n",
    "iron_ore_factors = {\n",
    "    \"China\": 0.27, \"Australia\": 0.54, \"Brazil\": 0.64, \"India\": 0.61, \"Russia\": 0.60,\n",
    "    \"Ukraine\": 0.69, \"South Africa\": 0.58, \"Canada\": 0.62, \"United States\": 0.57, \"Sweden\": 0.60\n",
    "}\n",
    "default_iron_ore_factor = 0.53\n",
    "\n",
    "missing_grade = prod[\"Ore_Grade\"].isna()\n",
    "\n",
    "# Iron Ore: country-specific or default\n",
    "mask_fe = missing_grade & (prod[\"PRIMARY_COMMODITY\"] == \"Iron Ore\")\n",
    "prod.loc[mask_fe, \"Ore_Grade\"] = prod.loc[mask_fe, \"COUNTRY_NAME\"].map(iron_ore_factors).fillna(default_iron_ore_factor)\n",
    "\n",
    "# Manganese: 0.445\n",
    "mask_mn = missing_grade & (prod[\"PRIMARY_COMMODITY\"] == \"Manganese\")\n",
    "prod.loc[mask_mn, \"Ore_Grade\"] = 0.445\n",
    "\n",
    "# Bauxite: keep default fallback as-is (0.245) — only applies where grade was missing\n",
    "mask_bx = missing_grade & (prod[\"PRIMARY_COMMODITY\"] == \"Bauxite\")\n",
    "prod.loc[mask_bx, \"Ore_Grade\"] = 0.245\n",
    "\n",
    "# --- Place Ore_Grade column right after Reserves_Contained (if present) ---\n",
    "if \"Reserves_Contained\" in prod.columns and \"Ore_Grade\" in prod.columns:\n",
    "    cols = prod.columns.tolist()\n",
    "    cols.remove(\"Ore_Grade\")\n",
    "    insert_at = cols.index(\"Reserves_Contained\") + 1\n",
    "    cols[insert_at:insert_at] = [\"Ore_Grade\"]\n",
    "    prod = prod[cols]\n",
    "\n",
    "# --- Save (overwrites) ---\n",
    "prod.to_excel(prod_path, index=False)\n",
    "\n",
    "# --- Optional: quick summary prints ---\n",
    "filled_fe = mask_fe.sum()\n",
    "filled_mn = mask_mn.sum()\n",
    "filled_bx = mask_bx.sum()\n",
    "merged_from_file = (~missing_grade & prod[\"PRIMARY_COMMODITY\"].isin([\"Iron Ore\", \"Bauxite\", \"Manganese\"])).sum()\n",
    "\n",
    "print(\"Merge complete. Applied 0.529 multiplier to bauxite grades from ore-grade file (before %→decimal).\")\n",
    "print(f\" - From ore-grade file (targets): {merged_from_file}\")\n",
    "print(f\" - Filled Iron Ore via country/default: {filled_fe}\")\n",
    "print(f\" - Filled Manganese (0.445): {filled_mn}\")\n",
    "print(f\" - Filled Bauxite default (0.245): {filled_bx}\")\n",
    "print(f\"Saved to: {prod_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a72fc161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Ore_Grade to year columns for Iron Ore and Manganese mines.\n",
      "Targets (rows): 448\n",
      "Rows with Ore_Grade applied: 448\n",
      "Saved to: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "in_path = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "out_path = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "\n",
    "# Load\n",
    "df = pd.read_excel(in_path, dtype={\"PROP_ID\": str})\n",
    "\n",
    "# Ensure Ore_Grade exists and is numeric (convert % -> decimal if needed)\n",
    "if \"Ore_Grade\" not in df.columns:\n",
    "    raise KeyError(\"Column 'Ore_Grade' not found in the input file.\")\n",
    "df[\"Ore_Grade\"] = pd.to_numeric(df[\"Ore_Grade\"], errors=\"coerce\")\n",
    "df[\"Ore_Grade\"] = df[\"Ore_Grade\"].where(df[\"Ore_Grade\"] <= 1, df[\"Ore_Grade\"] / 100.0)\n",
    "\n",
    "# Identify year columns 1980..2023\n",
    "year_cols = [c for c in df.columns\n",
    "             if re.fullmatch(r\"\\d{4}\", str(c)) and 1980 <= int(str(c)) <= 2023]\n",
    "\n",
    "# Make sure year columns are numeric\n",
    "for c in year_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Target mines: Iron Ore and Manganese\n",
    "mask = df[\"PRIMARY_COMMODITY\"].isin([\"Iron Ore\", \"Manganese\"])\n",
    "\n",
    "# Multiply year columns by Ore_Grade (rows with missing grade remain unchanged)\n",
    "before_nonnull = df.loc[mask, year_cols].notna().sum().sum()\n",
    "df.loc[mask, year_cols] = df.loc[mask, year_cols].mul(df.loc[mask, \"Ore_Grade\"].fillna(1.0), axis=0)\n",
    "after_nonnull = df.loc[mask, year_cols].notna().sum().sum()\n",
    "\n",
    "# Save\n",
    "df.to_excel(out_path, index=False)\n",
    "\n",
    "# Simple summary\n",
    "targets = mask.sum()\n",
    "used_grades = df.loc[mask, \"Ore_Grade\"].notna().sum()\n",
    "print(\"Applied Ore_Grade to year columns for Iron Ore and Manganese mines.\")\n",
    "print(f\"Targets (rows): {targets}\")\n",
    "print(f\"Rows with Ore_Grade applied: {used_grades}\")\n",
    "print(f\"Saved to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "65559c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading metadata file: Data Input/Meta Data/Lit_Missing_Start_Yr.xlsx\n",
      "Metadata file loaded: 150 rows\n",
      "Reading production file: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n",
      "Production file loaded: 1085 rows\n",
      "Empty START_UP_YR cells in production file: 161\n",
      "Valid START_UP_YEAR values in metadata: 150\n",
      "PROP_ID 62580: Filled empty START_UP_YR with 1970\n",
      "PROP_ID 55209: Filled empty START_UP_YR with 1940\n",
      "PROP_ID 65193: Filled empty START_UP_YR with 1970\n",
      "PROP_ID 68363: Filled empty START_UP_YR with 1985\n",
      "PROP_ID 66249: Filled empty START_UP_YR with 1958\n",
      "PROP_ID 68362: Filled empty START_UP_YR with 1983\n",
      "PROP_ID 57501: Filled empty START_UP_YR with 1962\n",
      "PROP_ID 68527: Filled empty START_UP_YR with 1987\n",
      "PROP_ID 57368: Filled empty START_UP_YR with 1965\n",
      "PROP_ID 69079: Filled empty START_UP_YR with 1948\n",
      "PROP_ID 65191: Filled empty START_UP_YR with 1934\n",
      "PROP_ID 64560: Filled empty START_UP_YR with 1965\n",
      "PROP_ID 68360: Filled empty START_UP_YR with 1990\n",
      "PROP_ID 57084: Filled empty START_UP_YR with 1963\n",
      "PROP_ID 54759: Filled empty START_UP_YR with 1952\n",
      "PROP_ID 80829: Filled empty START_UP_YR with 1950\n",
      "PROP_ID 81205: Filled empty START_UP_YR with 1966\n",
      "PROP_ID 80801: Filled empty START_UP_YR with 2011\n",
      "PROP_ID 38217: Filled empty START_UP_YR with 1998\n",
      "PROP_ID 28955: Filled empty START_UP_YR with 1975\n",
      "PROP_ID 66154: Filled empty START_UP_YR with 2010\n",
      "PROP_ID 28369: Filled empty START_UP_YR with 1989\n",
      "PROP_ID 62900: Filled empty START_UP_YR with 1989\n",
      "PROP_ID 37169: Filled empty START_UP_YR with 1960\n",
      "PROP_ID 37405: Filled empty START_UP_YR with 2010\n",
      "PROP_ID 59116: Filled empty START_UP_YR with 2023\n",
      "PROP_ID 39426: Filled empty START_UP_YR with 1941\n",
      "PROP_ID 31918: Filled empty START_UP_YR with 1994\n",
      "PROP_ID 37171: Filled empty START_UP_YR with 1919\n",
      "PROP_ID 31953: Filled empty START_UP_YR with 1965\n",
      "PROP_ID 37172: Filled empty START_UP_YR with 1966\n",
      "PROP_ID 31924: Filled empty START_UP_YR with 1982\n",
      "PROP_ID 59175: Filled empty START_UP_YR with 2021\n",
      "PROP_ID 68530: Filled empty START_UP_YR with 1934\n",
      "PROP_ID 68538: Filled empty START_UP_YR with 1933\n",
      "PROP_ID 33003: Filled empty START_UP_YR with 2018\n",
      "PROP_ID 31914: Filled empty START_UP_YR with 1933\n",
      "PROP_ID 32201: Filled empty START_UP_YR with 1934\n",
      "PROP_ID 59242: Filled empty START_UP_YR with 1960\n",
      "PROP_ID 31950: Filled empty START_UP_YR with 1972\n",
      "PROP_ID 54755: Filled empty START_UP_YR with 1973\n",
      "PROP_ID 64378: Filled empty START_UP_YR with 1991\n",
      "PROP_ID 31911: Filled empty START_UP_YR with 1963\n",
      "PROP_ID 59243: Filled empty START_UP_YR with 1942\n",
      "PROP_ID 70039: Filled empty START_UP_YR with 1957\n",
      "PROP_ID 59270: Filled empty START_UP_YR with 1961\n",
      "PROP_ID 68072: Filled empty START_UP_YR with 2008\n",
      "PROP_ID 37218: Filled empty START_UP_YR with 2012\n",
      "PROP_ID 69426: Filled empty START_UP_YR with 1885\n",
      "PROP_ID 32175: Filled empty START_UP_YR with 1970\n",
      "PROP_ID 38557: Filled empty START_UP_YR with 1950\n",
      "PROP_ID 88897: Filled empty START_UP_YR with 2018\n",
      "PROP_ID 62246: Filled empty START_UP_YR with 2010\n",
      "PROP_ID 57500: Filled empty START_UP_YR with 1997\n",
      "PROP_ID 74801: Filled empty START_UP_YR with 1962\n",
      "PROP_ID 69416: Filled empty START_UP_YR with 1958\n",
      "PROP_ID 52849: Filled empty START_UP_YR with 1950\n",
      "PROP_ID 57502: Filled empty START_UP_YR with 1997\n",
      "PROP_ID 66021: Filled empty START_UP_YR with 2007\n",
      "PROP_ID 52231: Filled empty START_UP_YR with 2012\n",
      "PROP_ID 57030: Filled empty START_UP_YR with 2016\n",
      "PROP_ID 33172: Filled empty START_UP_YR with 1964\n",
      "PROP_ID 31651: Filled empty START_UP_YR with 1950\n",
      "PROP_ID 29246: Filled empty START_UP_YR with 1907\n",
      "PROP_ID 86806: Filled empty START_UP_YR with 2016\n",
      "PROP_ID 77877: Filled empty START_UP_YR with 1924\n",
      "PROP_ID 34161: Filled empty START_UP_YR with 2005\n",
      "Deleted 94 rows with empty START_UP_YR after merging\n",
      "\n",
      "=== MERGE RESULTS ===\n",
      "Empty START_UP_YR cells before merge: 161\n",
      "Empty START_UP_YR cells after merge: 94\n",
      "Cells filled: 67\n",
      "Rows deleted due to empty START_UP_YR: 94\n",
      "Final rows remaining: 991\n",
      "Total rows removed from dataset: 94\n",
      "\n",
      "Matching statistics:\n",
      "PROP_IDs in metadata file: 76\n",
      "PROP_IDs in production file: 991\n",
      "Matching PROP_IDs: 68\n",
      "\n",
      "Updated production file saved: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_startup_years():\n",
    "    \"\"\"\n",
    "    Merges START_UP_YEAR from the metadata file into START_UP_YR in the production file.\n",
    "    Only fills empty cells in the production file with values from the metadata file.\n",
    "    Merge is based on the shared PROP_ID column.\n",
    "    \"\"\"\n",
    "    # Define file paths\n",
    "    metadata_file = \"Data Input/Meta Data/Lit_Missing_Start_Yr.xlsx\"\n",
    "    production_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "    \n",
    "    print(f\"Reading metadata file: {metadata_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the metadata file\n",
    "        metadata_df = pd.read_excel(metadata_file)\n",
    "        print(f\"Metadata file loaded: {len(metadata_df)} rows\")\n",
    "        \n",
    "        # Check required columns in metadata file\n",
    "        if 'PROP_ID' not in metadata_df.columns:\n",
    "            print(\"Error: PROP_ID column not found in metadata file\")\n",
    "            return\n",
    "        if 'START_UP_YEAR' not in metadata_df.columns:\n",
    "            print(\"Error: START_UP_YEAR column not found in metadata file\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Reading production file: {production_file}\")\n",
    "        \n",
    "        # Read the production file\n",
    "        production_df = pd.read_excel(production_file)\n",
    "        print(f\"Production file loaded: {len(production_df)} rows\")\n",
    "        \n",
    "        # Check required columns in production file\n",
    "        if 'PROP_ID' not in production_df.columns:\n",
    "            print(\"Error: PROP_ID column not found in production file\")\n",
    "            return\n",
    "        if 'START_UP_YR' not in production_df.columns:\n",
    "            print(\"Error: START_UP_YR column not found in production file\")\n",
    "            return\n",
    "        \n",
    "        # Count empty cells in production file before merge\n",
    "        empty_startup_before = production_df['START_UP_YR'].isna().sum()\n",
    "        print(f\"Empty START_UP_YR cells in production file: {empty_startup_before}\")\n",
    "        \n",
    "        # Create lookup dictionary from metadata file\n",
    "        # Only include non-null values\n",
    "        metadata_lookup = metadata_df[metadata_df['START_UP_YEAR'].notna()].set_index('PROP_ID')['START_UP_YEAR'].to_dict()\n",
    "        print(f\"Valid START_UP_YEAR values in metadata: {len(metadata_lookup)}\")\n",
    "        \n",
    "        # Track changes\n",
    "        cells_filled = 0\n",
    "        \n",
    "        # Fill empty cells in production file\n",
    "        for idx, row in production_df.iterrows():\n",
    "            prop_id = row['PROP_ID']\n",
    "            current_startup = row['START_UP_YR']\n",
    "            \n",
    "            # Only fill if current cell is empty and we have data in metadata\n",
    "            if pd.isna(current_startup) and prop_id in metadata_lookup:\n",
    "                new_startup = metadata_lookup[prop_id]\n",
    "                production_df.at[idx, 'START_UP_YR'] = new_startup\n",
    "                cells_filled += 1\n",
    "                print(f\"PROP_ID {prop_id}: Filled empty START_UP_YR with {new_startup}\")\n",
    "        \n",
    "        # Count empty cells after merge\n",
    "        empty_startup_after = production_df['START_UP_YR'].isna().sum()\n",
    "        \n",
    "        # Delete rows that still have empty START_UP_YR after merging\n",
    "        rows_before_deletion = len(production_df)\n",
    "        production_df = production_df[production_df['START_UP_YR'].notna()]\n",
    "        rows_after_deletion = len(production_df)\n",
    "        rows_deleted = rows_before_deletion - rows_after_deletion\n",
    "        \n",
    "        print(f\"Deleted {rows_deleted} rows with empty START_UP_YR after merging\")\n",
    "        \n",
    "        # Save the updated production file\n",
    "        production_df.to_excel(production_file, index=False)\n",
    "        \n",
    "        # Report results\n",
    "        print(f\"\\n=== MERGE RESULTS ===\")\n",
    "        print(f\"Empty START_UP_YR cells before merge: {empty_startup_before}\")\n",
    "        print(f\"Empty START_UP_YR cells after merge: {empty_startup_after}\")\n",
    "        print(f\"Cells filled: {cells_filled}\")\n",
    "        print(f\"Rows deleted due to empty START_UP_YR: {rows_deleted}\")\n",
    "        print(f\"Final rows remaining: {rows_after_deletion}\")\n",
    "        print(f\"Total rows removed from dataset: {rows_before_deletion - rows_after_deletion}\")\n",
    "        \n",
    "        # Show matching statistics\n",
    "        metadata_prop_ids = set(metadata_df['PROP_ID'].dropna())\n",
    "        production_prop_ids = set(production_df['PROP_ID'].dropna())\n",
    "        matching_prop_ids = metadata_prop_ids.intersection(production_prop_ids)\n",
    "        \n",
    "        print(f\"\\nMatching statistics:\")\n",
    "        print(f\"PROP_IDs in metadata file: {len(metadata_prop_ids)}\")\n",
    "        print(f\"PROP_IDs in production file: {len(production_prop_ids)}\")\n",
    "        print(f\"Matching PROP_IDs: {len(matching_prop_ids)}\")\n",
    "        \n",
    "        print(f\"\\nUpdated production file saved: {production_file}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during merge process: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_startup_years()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "38e0cce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading metadata file: Data Input/Meta Data/Property_Closure_Analysis - Copy.xlsx\n",
      "Metadata file loaded: 62 rows\n",
      "Reading production file: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n",
      "Production file loaded: 991 rows\n",
      "Valid PROJ_CLOSURE_YR_Updated values in metadata: 26\n",
      "Existing PROJ_CLOSURE_YR values in production file: 591\n",
      "PROP_ID 29291: Updated closure year from 2009 to 2030\n",
      "PROP_ID 27589: Updated closure year from 1997 to 2034\n",
      "PROP_ID 28268: Updated closure year from 1987 to 2028\n",
      "PROP_ID 31323: Updated closure year from 2022 to 2021\n",
      "PROP_ID 28286: Updated closure year from 2022 to 2027\n",
      "PROP_ID 27249: Updated closure year from 1999 to 2035\n",
      "PROP_ID 27030: Updated closure year from 2020 to 2036\n",
      "PROP_ID 30969: Updated closure year from 2016 to 2023\n",
      "PROP_ID 27752: Updated closure year from 2015 to 2024\n",
      "PROP_ID 33208: Updated closure year from 2021 to 2026\n",
      "PROP_ID 31818: Updated closure year from 2012 to 2015\n",
      "PROP_ID 31560: Updated closure year from 2019 to 2024\n",
      "PROP_ID 36392: Updated closure year from 2021 to 2034\n",
      "PROP_ID 76468: Updated closure year from 2021 to 2120\n",
      "PROP_ID 26731: Updated closure year from 2022 to 2023\n",
      "PROP_ID 27453: Updated closure year from 2018 to 2031\n",
      "PROP_ID 27053: Updated closure year from 2012 to 2016\n",
      "PROP_ID 35422: Updated closure year from 2021 to 2008\n",
      "PROP_ID 27539: Updated closure year from 2022 to 2030\n",
      "PROP_ID 27612: Updated closure year from 2014 to 2024\n",
      "PROP_ID 31563: Updated closure year from 2022 to 2027\n",
      "PROP_ID 30711: Updated closure year from 1996 to 2025\n",
      "\n",
      "=== MERGE RESULTS ===\n",
      "PROJ_CLOSURE_YR values before merge: 591\n",
      "PROJ_CLOSURE_YR values after merge: 591\n",
      "Rows with new closure years added: 0\n",
      "Rows with closure years updated: 22\n",
      "Total changes made: 22\n",
      "Net increase in closure data: 0\n",
      "\n",
      "Matching statistics:\n",
      "PROP_IDs in metadata file: 62\n",
      "PROP_IDs in production file: 991\n",
      "Matching PROP_IDs: 53\n",
      "PROP_IDs with valid closure updates: 26\n",
      "\n",
      "Updated production file saved: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_closure_years():\n",
    "    \"\"\"\n",
    "    Merges PROJ_CLOSURE_YR_Updated from the metadata file into PROJ_CLOSURE_YR \n",
    "    in the production file. Only merges valid years from the source dataset.\n",
    "    Merge is based on the shared PROP_ID column.\n",
    "    \"\"\"\n",
    "    # Define file paths\n",
    "    metadata_file = \"Data Input/Meta Data/Property_Closure_Analysis - Copy.xlsx\"\n",
    "    production_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "    \n",
    "    print(f\"Reading metadata file: {metadata_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the metadata file\n",
    "        metadata_df = pd.read_excel(metadata_file)\n",
    "        print(f\"Metadata file loaded: {len(metadata_df)} rows\")\n",
    "        \n",
    "        # Check required columns in metadata file\n",
    "        if 'PROP_ID' not in metadata_df.columns:\n",
    "            print(\"Error: PROP_ID column not found in metadata file\")\n",
    "            return\n",
    "        if 'PROJ_CLOSURE_YR_Updated' not in metadata_df.columns:\n",
    "            print(\"Error: PROJ_CLOSURE_YR_Updated column not found in metadata file\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Reading production file: {production_file}\")\n",
    "        \n",
    "        # Read the production file\n",
    "        production_df = pd.read_excel(production_file)\n",
    "        print(f\"Production file loaded: {len(production_df)} rows\")\n",
    "        \n",
    "        # Check required columns in production file\n",
    "        if 'PROP_ID' not in production_df.columns:\n",
    "            print(\"Error: PROP_ID column not found in production file\")\n",
    "            return\n",
    "        if 'PROJ_CLOSURE_YR' not in production_df.columns:\n",
    "            print(\"Error: PROJ_CLOSURE_YR column not found in production file\")\n",
    "            return\n",
    "        \n",
    "        # Convert closure year columns to numeric to validate years\n",
    "        metadata_df['PROJ_CLOSURE_YR_Updated'] = pd.to_numeric(metadata_df['PROJ_CLOSURE_YR_Updated'], errors='coerce')\n",
    "        production_df['PROJ_CLOSURE_YR'] = pd.to_numeric(production_df['PROJ_CLOSURE_YR'], errors='coerce')\n",
    "        \n",
    "        # Create lookup dictionary from metadata file - only include valid numerical values\n",
    "        # Filter for non-null values that are valid numbers\n",
    "        valid_closure_data = metadata_df[\n",
    "            (metadata_df['PROJ_CLOSURE_YR_Updated'].notna())\n",
    "        ]\n",
    "        \n",
    "        # Remove duplicates (keep first occurrence if there are multiple entries for same PROP_ID)\n",
    "        valid_closure_data = valid_closure_data.drop_duplicates(subset=['PROP_ID'])\n",
    "        \n",
    "        closure_lookup = valid_closure_data.set_index('PROP_ID')['PROJ_CLOSURE_YR_Updated'].to_dict()\n",
    "        print(f\"Valid PROJ_CLOSURE_YR_Updated values in metadata: {len(closure_lookup)}\")\n",
    "        \n",
    "        # Count existing closure data before merge\n",
    "        existing_closure_before = production_df['PROJ_CLOSURE_YR'].notna().sum()\n",
    "        print(f\"Existing PROJ_CLOSURE_YR values in production file: {existing_closure_before}\")\n",
    "        \n",
    "        # Track changes\n",
    "        rows_updated = 0\n",
    "        rows_added = 0\n",
    "        \n",
    "        # Update closure years in production file\n",
    "        for idx, row in production_df.iterrows():\n",
    "            prop_id = row['PROP_ID']\n",
    "            current_closure = row['PROJ_CLOSURE_YR']\n",
    "            \n",
    "            # Check if we have updated closure data for this property\n",
    "            if prop_id in closure_lookup:\n",
    "                new_closure = closure_lookup[prop_id]\n",
    "                \n",
    "                if pd.isna(current_closure):\n",
    "                    # Adding new closure year\n",
    "                    production_df.at[idx, 'PROJ_CLOSURE_YR'] = new_closure\n",
    "                    rows_added += 1\n",
    "                    print(f\"PROP_ID {prop_id}: Added closure year {int(new_closure)}\")\n",
    "                elif current_closure != new_closure:\n",
    "                    # Updating existing closure year\n",
    "                    production_df.at[idx, 'PROJ_CLOSURE_YR'] = new_closure\n",
    "                    rows_updated += 1\n",
    "                    print(f\"PROP_ID {prop_id}: Updated closure year from {int(current_closure)} to {int(new_closure)}\")\n",
    "        \n",
    "        # Count closure data after merge\n",
    "        existing_closure_after = production_df['PROJ_CLOSURE_YR'].notna().sum()\n",
    "        \n",
    "        # Save the updated production file\n",
    "        production_df.to_excel(production_file, index=False)\n",
    "        \n",
    "        # Report results\n",
    "        print(f\"\\n=== MERGE RESULTS ===\")\n",
    "        print(f\"PROJ_CLOSURE_YR values before merge: {existing_closure_before}\")\n",
    "        print(f\"PROJ_CLOSURE_YR values after merge: {existing_closure_after}\")\n",
    "        print(f\"Rows with new closure years added: {rows_added}\")\n",
    "        print(f\"Rows with closure years updated: {rows_updated}\")\n",
    "        print(f\"Total changes made: {rows_added + rows_updated}\")\n",
    "        print(f\"Net increase in closure data: {existing_closure_after - existing_closure_before}\")\n",
    "        \n",
    "        # Show matching statistics\n",
    "        metadata_prop_ids = set(metadata_df['PROP_ID'].dropna())\n",
    "        production_prop_ids = set(production_df['PROP_ID'].dropna())\n",
    "        matching_prop_ids = metadata_prop_ids.intersection(production_prop_ids)\n",
    "        \n",
    "        print(f\"\\nMatching statistics:\")\n",
    "        print(f\"PROP_IDs in metadata file: {len(metadata_prop_ids)}\")\n",
    "        print(f\"PROP_IDs in production file: {len(production_prop_ids)}\")\n",
    "        print(f\"Matching PROP_IDs: {len(matching_prop_ids)}\")\n",
    "        print(f\"PROP_IDs with valid closure updates: {len(closure_lookup)}\")\n",
    "        \n",
    "        print(f\"\\nUpdated production file saved: {production_file}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during merge process: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_closure_years()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c4c0323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n",
      "Found 44 year columns\n",
      "PROP_ID 56954: Changed startup year from 2014 to 2011\n",
      "PROP_ID 58083: Changed startup year from 2019 to 2010\n",
      "PROP_ID 62576: Changed startup year from 2006 to 1984\n",
      "PROP_ID 58076: Changed startup year from 2008 to 2002\n",
      "PROP_ID 58546: Changed startup year from 2015 to 2000\n",
      "PROP_ID 60813: Changed startup year from 2009 to 2008\n",
      "PROP_ID 58079: Changed startup year from 2008 to 2004\n",
      "PROP_ID 58534: Changed startup year from 2015 to 1996\n",
      "PROP_ID 55265: Changed startup year from 2013 to 2011\n",
      "PROP_ID 58535: Changed startup year from 2015 to 2002\n",
      "PROP_ID 52676: Changed startup year from 2018 to 1980\n",
      "PROP_ID 62582: Changed startup year from 2015 to 2000\n",
      "PROP_ID 58077: Changed startup year from 2008 to 2002\n",
      "PROP_ID 26483: Changed startup year from 2010 to 1990\n",
      "PROP_ID 30526: Changed startup year from 2018 to 2008\n",
      "PROP_ID 26908: Changed startup year from 2008 to 1986\n",
      "PROP_ID 28272: Changed startup year from 1993 to 1989\n",
      "PROP_ID 30284: Changed startup year from 2016 to 2015\n",
      "PROP_ID 33381: Changed startup year from 2011 to 2005\n",
      "PROP_ID 26672: Changed startup year from 2010 to 1991\n",
      "PROP_ID 33190: Changed startup year from 2010 to 2003\n",
      "PROP_ID 27530: Changed startup year from 1997 to 1996\n",
      "PROP_ID 30972: Changed startup year from 2011 to 2001\n",
      "PROP_ID 28756: Changed startup year from 2011 to 1992\n",
      "PROP_ID 25700: Changed startup year from 2009 to 2008\n",
      "PROP_ID 25700: Changed closure year from 2021 to 2023\n",
      "PROP_ID 31323: Changed closure year from 2021 to 2023\n",
      "PROP_ID 24733: Changed startup year from 2011 to 1987\n",
      "PROP_ID 27283: Changed startup year from 1999 to 1998\n",
      "PROP_ID 24673: Changed startup year from 2003 to 1988\n",
      "PROP_ID 26105: Changed startup year from 2011 to 1984\n",
      "PROP_ID 32931: Changed startup year from 2007 to 2006\n",
      "PROP_ID 33791: Changed startup year from 2012 to 2011\n",
      "PROP_ID 30277: Changed startup year from 2005 to 1997\n",
      "PROP_ID 29979: Changed startup year from 2012 to 1995\n",
      "PROP_ID 28703: Changed startup year from 2011 to 1996\n",
      "PROP_ID 36399: Changed startup year from 2006 to 2005\n",
      "PROP_ID 32917: Changed startup year from 2013 to 2007\n",
      "PROP_ID 26577: Changed startup year from 2004 to 1986\n",
      "PROP_ID 26575: Changed startup year from 1999 to 1987\n",
      "PROP_ID 25720: Changed startup year from 2004 to 1994\n",
      "PROP_ID 30656: Changed startup year from 2015 to 2007\n",
      "PROP_ID 28032: Changed startup year from 2017 to 2000\n",
      "PROP_ID 27685: Changed startup year from 2007 to 1990\n",
      "PROP_ID 26668: Changed startup year from 2005 to 2004\n",
      "PROP_ID 27249: Changed startup year from 1993 to 1988\n",
      "PROP_ID 27033: Changed startup year from 2021 to 1992\n",
      "PROP_ID 32142: Changed startup year from 2010 to 2007\n",
      "PROP_ID 29130: Changed startup year from 2014 to 1997\n",
      "PROP_ID 27030: Changed startup year from 1992 to 1988\n",
      "PROP_ID 26477: Changed startup year from 2004 to 1991\n",
      "PROP_ID 27674: Changed startup year from 2008 to 1999\n",
      "PROP_ID 27392: Changed closure year from 2015 to 2023\n",
      "PROP_ID 24482: Changed startup year from 1995 to 1980\n",
      "PROP_ID 29377: Changed startup year from 2012 to 1996\n",
      "PROP_ID 27446: Changed startup year from 1998 to 1988\n",
      "PROP_ID 24474: Changed startup year from 2007 to 1983\n",
      "PROP_ID 26984: Changed startup year from 2014 to 2013\n",
      "PROP_ID 26823: Changed startup year from 2005 to 1997\n",
      "PROP_ID 29792: Changed startup year from 2019 to 1996\n",
      "PROP_ID 27243: Changed startup year from 1990 to 1989\n",
      "PROP_ID 38654: Changed startup year from 2011 to 2010\n",
      "PROP_ID 28866: Changed startup year from 2004 to 1989\n",
      "PROP_ID 27752: Changed startup year from 2012 to 1995\n",
      "PROP_ID 24470: Changed startup year from 2012 to 1984\n",
      "PROP_ID 30469: Changed startup year from 2016 to 1996\n",
      "PROP_ID 26968: Changed startup year from 2004 to 1995\n",
      "PROP_ID 27523: Changed startup year from 1998 to 1997\n",
      "PROP_ID 27653: Changed startup year from 2004 to 1997\n",
      "PROP_ID 29830: Changed startup year from 2007 to 2006\n",
      "PROP_ID 25824: Changed startup year from 2003 to 1995\n",
      "PROP_ID 58786: Changed startup year from 2019 to 2016\n",
      "PROP_ID 28650: Changed startup year from 2015 to 2014\n",
      "PROP_ID 24880: Changed startup year from 1997 to 1985\n",
      "PROP_ID 28296: Changed startup year from 2006 to 1991\n",
      "PROP_ID 35321: Changed startup year from 2007 to 1998\n",
      "PROP_ID 29140: Changed startup year from 1999 to 1991\n",
      "PROP_ID 28680: Changed startup year from 2015 to 1996\n",
      "PROP_ID 28953: Changed startup year from 2015 to 1992\n",
      "PROP_ID 27065: Changed startup year from 2014 to 2009\n",
      "PROP_ID 27034: Changed startup year from 2006 to 1991\n",
      "PROP_ID 81511: Changed startup year from 2020 to 2017\n",
      "PROP_ID 74748: Changed startup year from 1992 to 1991\n",
      "PROP_ID 31975: Changed startup year from 2003 to 2000\n",
      "PROP_ID 59309: Changed startup year from 2014 to 2010\n",
      "PROP_ID 37403: Changed startup year from 2002 to 2000\n",
      "PROP_ID 32991: Changed startup year from 2006 to 2005\n",
      "PROP_ID 37170: Changed startup year from 2018 to 1992\n",
      "PROP_ID 82093: Changed closure year from 2020 to 2023\n",
      "PROP_ID 59109: Changed startup year from 2015 to 1984\n",
      "PROP_ID 33660: Changed startup year from 2018 to 2010\n",
      "PROP_ID 83105: Changed startup year from 2018 to 2017\n",
      "PROP_ID 83105: Changed closure year from 2021 to 2023\n",
      "PROP_ID 31974: Changed startup year from 2013 to 2008\n",
      "PROP_ID 31895: Changed startup year from 1992 to 1990\n",
      "PROP_ID 37405: Changed startup year from 2010 to 2008\n",
      "PROP_ID 31818: Changed startup year from 2012 to 2000\n",
      "PROP_ID 74741: Changed startup year from 2014 to 2000\n",
      "PROP_ID 74756: Changed startup year from 2003 to 1999\n",
      "PROP_ID 59116: Changed startup year from 2023 to 1995\n",
      "PROP_ID 60833: Changed closure year from 2016 to 2017\n",
      "PROP_ID 59119: Changed closure year from 2021 to 2023\n",
      "PROP_ID 70314: Changed closure year from 2022 to 2023\n",
      "PROP_ID 32598: Changed startup year from 2015 to 2000\n",
      "PROP_ID 82101: Changed closure year from 2019 to 2020\n",
      "PROP_ID 35007: Changed startup year from 2012 to 1984\n",
      "PROP_ID 68315: Changed closure year from 2022 to 2023\n",
      "PROP_ID 82082: Changed closure year from 2022 to 2023\n",
      "PROP_ID 31901: Changed startup year from 2009 to 2000\n",
      "PROP_ID 37544: Changed startup year from 2010 to 1992\n",
      "PROP_ID 74749: Changed startup year from 2005 to 2001\n",
      "PROP_ID 83415: Changed startup year from 2008 to 2006\n",
      "PROP_ID 82162: Changed startup year from 2009 to 2007\n",
      "PROP_ID 32888: Changed closure year from 2013 to 2014\n",
      "PROP_ID 82213: Changed closure year from 2022 to 2023\n",
      "PROP_ID 82022: Changed closure year from 2022 to 2023\n",
      "PROP_ID 70371: Changed startup year from 2011 to 2004\n",
      "PROP_ID 35182: Changed startup year from 2007 to 2006\n",
      "PROP_ID 33691: Changed startup year from 2007 to 2006\n",
      "PROP_ID 59158: Changed startup year from 2015 to 2001\n",
      "PROP_ID 82217: Changed closure year from 2022 to 2023\n",
      "PROP_ID 76436: Changed startup year from 2013 to 2010\n",
      "PROP_ID 82131: Changed closure year from 2021 to 2023\n",
      "PROP_ID 31949: Changed startup year from 2001 to 1990\n",
      "PROP_ID 32999: Changed startup year from 2013 to 2012\n",
      "PROP_ID 59175: Changed startup year from 2021 to 1997\n",
      "PROP_ID 34462: Changed startup year from 2011 to 2004\n",
      "PROP_ID 33003: Changed startup year from 2018 to 1990\n",
      "PROP_ID 32403: Changed startup year from 2019 to 1992\n",
      "PROP_ID 34509: Changed startup year from 2010 to 2007\n",
      "PROP_ID 82081: Changed closure year from 2020 to 2023\n",
      "PROP_ID 82165: Changed closure year from 2022 to 2023\n",
      "PROP_ID 32589: Changed startup year from 2008 to 2000\n",
      "PROP_ID 74802: Changed startup year from 2016 to 1997\n",
      "PROP_ID 60240: Changed startup year from 2012 to 1984\n",
      "PROP_ID 37816: Changed startup year from 2011 to 2006\n",
      "PROP_ID 74759: Changed startup year from 2006 to 1984\n",
      "PROP_ID 32413: Changed startup year from 2003 to 1992\n",
      "PROP_ID 33162: Changed startup year from 2006 to 2000\n",
      "PROP_ID 32882: Changed startup year from 2013 to 2010\n",
      "PROP_ID 31915: Changed startup year from 2015 to 1998\n",
      "PROP_ID 32178: Changed startup year from 2004 to 2000\n",
      "PROP_ID 82163: Changed closure year from 2021 to 2023\n",
      "PROP_ID 76471: Changed startup year from 2003 to 2000\n",
      "PROP_ID 82193: Changed closure year from 2020 to 2023\n",
      "PROP_ID 37804: Changed startup year from 2008 to 2000\n",
      "PROP_ID 59241: Changed startup year from 2019 to 1993\n",
      "PROP_ID 37827: Changed startup year from 2021 to 2019\n",
      "PROP_ID 36392: Changed startup year from 2013 to 2012\n",
      "PROP_ID 31833: Changed startup year from 2019 to 1980\n",
      "PROP_ID 75656: Changed startup year from 2014 to 2000\n",
      "PROP_ID 32743: Changed startup year from 2010 to 2000\n",
      "PROP_ID 35626: Changed startup year from 2016 to 2013\n",
      "PROP_ID 82058: Changed closure year from 2022 to 2023\n",
      "PROP_ID 82224: Changed closure year from 2020 to 2023\n",
      "PROP_ID 82215: Changed closure year from 2020 to 2023\n",
      "PROP_ID 68072: Changed startup year from 2008 to 2000\n",
      "PROP_ID 37218: Changed startup year from 2012 to 2008\n",
      "PROP_ID 32742: Changed startup year from 2001 to 2000\n",
      "PROP_ID 32742: Changed closure year from 2018 to 2023\n",
      "PROP_ID 82088: Changed closure year from 2022 to 2023\n",
      "PROP_ID 82069: Changed closure year from 2021 to 2023\n",
      "PROP_ID 59297: Changed closure year from 2019 to 2023\n",
      "PROP_ID 32747: Changed startup year from 2002 to 2000\n",
      "PROP_ID 68175: Changed startup year from 2013 to 2010\n",
      "PROP_ID 82096: Changed startup year from 2019 to 2013\n",
      "PROP_ID 82102: Changed closure year from 2018 to 2023\n",
      "PROP_ID 70310: Changed startup year from 2016 to 1992\n",
      "PROP_ID 38547: Changed startup year from 2019 to 2010\n",
      "PROP_ID 30688: Changed startup year from 2016 to 2013\n",
      "PROP_ID 52581: Changed startup year from 2013 to 2000\n",
      "PROP_ID 54228: Changed startup year from 1985 to 1984\n",
      "PROP_ID 54423: Changed closure year from 2021 to 2023\n",
      "PROP_ID 59777: Changed startup year from 2017 to 2000\n",
      "PROP_ID 68557: Changed startup year from 2016 to 1999\n",
      "PROP_ID 53686: Changed startup year from 2019 to 2000\n",
      "PROP_ID 37474: Changed startup year from 2010 to 2007\n",
      "PROP_ID 27213: Changed startup year from 2007 to 1993\n",
      "PROP_ID 27742: Changed startup year from 2011 to 2010\n",
      "PROP_ID 28316: Changed startup year from 2010 to 1993\n",
      "PROP_ID 29552: Changed startup year from 2000 to 1995\n",
      "PROP_ID 28739: Changed startup year from 2011 to 2001\n",
      "PROP_ID 27971: Changed startup year from 2005 to 1990\n",
      "PROP_ID 27023: Changed startup year from 2000 to 1990\n",
      "PROP_ID 28019: Changed startup year from 2001 to 2000\n",
      "PROP_ID 27453: Changed startup year from 2019 to 2008\n",
      "PROP_ID 35378: Changed startup year from 2022 to 2001\n",
      "PROP_ID 30053: Changed startup year from 2011 to 2008\n",
      "PROP_ID 27027: Changed startup year from 2007 to 1990\n",
      "PROP_ID 27027: Changed closure year from 2010 to 2012\n",
      "PROP_ID 33832: Changed startup year from 2019 to 2009\n",
      "PROP_ID 27713: Changed startup year from 2018 to 2004\n",
      "PROP_ID 27053: Changed startup year from 1990 to 1986\n",
      "PROP_ID 28363: Changed startup year from 2007 to 1990\n",
      "PROP_ID 31758: Changed startup year from 2022 to 2002\n",
      "PROP_ID 28364: Changed startup year from 2013 to 1990\n",
      "PROP_ID 57030: Changed startup year from 2016 to 1986\n",
      "PROP_ID 35422: Changed startup year from 2009 to 2006\n",
      "PROP_ID 35422: Changed closure year from 2008 to 2023\n",
      "PROP_ID 29404: Changed startup year from 2009 to 2008\n",
      "PROP_ID 27826: Changed startup year from 2020 to 2000\n",
      "PROP_ID 25820: Changed startup year from 2015 to 1990\n",
      "PROP_ID 27750: Changed startup year from 2018 to 2000\n",
      "PROP_ID 80256: Changed startup year from 2015 to 2009\n",
      "PROP_ID 24774: Changed startup year from 2010 to 1985\n",
      "PROP_ID 24774: Changed closure year from 2001 to 2023\n",
      "PROP_ID 31316: Changed startup year from 2018 to 1998\n",
      "PROP_ID 80259: Changed startup year from 2015 to 2011\n",
      "PROP_ID 27287: Changed startup year from 2019 to 2004\n",
      "PROP_ID 36012: Changed startup year from 2018 to 2000\n",
      "PROP_ID 36464: Changed startup year from 2016 to 2000\n",
      "PROP_ID 29980: Changed startup year from 1999 to 1997\n",
      "PROP_ID 27139: Changed startup year from 2012 to 1996\n",
      "PROP_ID 28229: Changed startup year from 2005 to 1999\n",
      "PROP_ID 24770: Changed startup year from 2017 to 1996\n",
      "PROP_ID 24770: Changed closure year from 2017 to 2023\n",
      "PROP_ID 24800: Changed startup year from 2014 to 2004\n",
      "PROP_ID 29073: Changed startup year from 2015 to 1993\n",
      "PROP_ID 29435: Changed closure year from 2006 to 2007\n",
      "PROP_ID 29665: Changed closure year from 2020 to 2021\n",
      "PROP_ID 25760: Changed startup year from 2015 to 1995\n",
      "PROP_ID 27167: Changed startup year from 2002 to 1988\n",
      "PROP_ID 25699: Changed startup year from 2017 to 1991\n",
      "PROP_ID 27388: Changed startup year from 1998 to 1989\n",
      "PROP_ID 30968: Changed startup year from 2016 to 2002\n",
      "PROP_ID 28798: Changed startup year from 2016 to 2000\n",
      "PROP_ID 34997: Changed startup year from 2013 to 2006\n",
      "\n",
      "Completed!\n",
      "Made 192 changes to startup years\n",
      "Made 35 changes to closure years\n",
      "File saved: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def correct_startup_years(file_path):\n",
    "    \"\"\"\n",
    "    Corrects the START_UP_YR and PROJ_CLOSURE_YR for each mine based on actual production data.\n",
    "    - If there's non-zero production before the recorded startup year, changes it to the year of first non-zero production.\n",
    "    - If there's non-zero production after the projected closure year, changes it to the year of last non-zero production.\n",
    "   \n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file\n",
    "    \"\"\"\n",
    "    print(f\"Reading file: {file_path}\")\n",
    "   \n",
    "    # Read the dataset\n",
    "    df = pd.read_excel(file_path)\n",
    "   \n",
    "    # Check if required columns exist\n",
    "    if 'START_UP_YR' not in df.columns:\n",
    "        print(\"Error: START_UP_YR column not found\")\n",
    "        return\n",
    "   \n",
    "    if 'PROP_ID' not in df.columns:\n",
    "        print(\"Error: PROP_ID column not found\")\n",
    "        return\n",
    "    \n",
    "    # Check if PROJ_CLOSURE_YR exists (optional for closure year correction)\n",
    "    has_closure_data = 'PROJ_CLOSURE_YR' in df.columns\n",
    "    if not has_closure_data:\n",
    "        print(\"Warning: PROJ_CLOSURE_YR column not found - will only correct startup years\")\n",
    "   \n",
    "    # Get year columns (1980-2023)\n",
    "    year_columns = [str(year) for year in range(1980, 2024)]\n",
    "    existing_year_columns = [col for col in year_columns if col in df.columns]\n",
    "    existing_year_columns.sort(key=int)  # Ensure chronological order\n",
    "   \n",
    "    print(f\"Found {len(existing_year_columns)} year columns\")\n",
    "   \n",
    "    # Convert year columns to numeric\n",
    "    for col in existing_year_columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Convert closure year to numeric if it exists\n",
    "    if has_closure_data:\n",
    "        df['PROJ_CLOSURE_YR'] = pd.to_numeric(df['PROJ_CLOSURE_YR'], errors='coerce')\n",
    "   \n",
    "    # Track changes\n",
    "    startup_changes_made = 0\n",
    "    closure_changes_made = 0\n",
    "   \n",
    "    # Process each mine\n",
    "    for idx, row in df.iterrows():\n",
    "        startup_year = row['START_UP_YR']\n",
    "        closure_year = row['PROJ_CLOSURE_YR'] if has_closure_data else None\n",
    "       \n",
    "        # Skip if no valid startup year\n",
    "        if pd.isna(startup_year):\n",
    "            continue\n",
    "       \n",
    "        startup_year = int(startup_year)\n",
    "       \n",
    "        # Find first year with non-zero production\n",
    "        first_production_year = None\n",
    "        for year_col in existing_year_columns:\n",
    "            year = int(year_col)\n",
    "            production = row[year_col]\n",
    "           \n",
    "            # Check if there's non-zero production\n",
    "            if not pd.isna(production) and production > 0:\n",
    "                first_production_year = year\n",
    "                break\n",
    "       \n",
    "        # If first production is before recorded startup year, update it\n",
    "        if first_production_year and first_production_year < startup_year:\n",
    "            df.at[idx, 'START_UP_YR'] = first_production_year\n",
    "            startup_changes_made += 1\n",
    "            print(f\"PROP_ID {row['PROP_ID']}: Changed startup year from {startup_year} to {first_production_year}\")\n",
    "        \n",
    "        # Process closure year correction if closure data exists\n",
    "        if has_closure_data and not pd.isna(closure_year):\n",
    "            closure_year = int(closure_year)\n",
    "            \n",
    "            # Find last year with non-zero production after closure year\n",
    "            last_production_after_closure = None\n",
    "            for year_col in reversed(existing_year_columns):  # Start from latest year\n",
    "                year = int(year_col)\n",
    "                production = row[year_col]\n",
    "                \n",
    "                # Check if there's non-zero production after closure year\n",
    "                if year > closure_year and not pd.isna(production) and production > 0:\n",
    "                    last_production_after_closure = year\n",
    "                    break  # We want the last (latest) year, so break on first match when going backwards\n",
    "            \n",
    "            # If production exists after closure year, update closure year\n",
    "            if last_production_after_closure:\n",
    "                df.at[idx, 'PROJ_CLOSURE_YR'] = last_production_after_closure\n",
    "                closure_changes_made += 1\n",
    "                print(f\"PROP_ID {row['PROP_ID']}: Changed closure year from {closure_year} to {last_production_after_closure}\")\n",
    "   \n",
    "    # Save the updated file\n",
    "    df.to_excel(file_path, index=False)\n",
    "   \n",
    "    print(f\"\\nCompleted!\")\n",
    "    print(f\"Made {startup_changes_made} changes to startup years\")\n",
    "    if has_closure_data:\n",
    "        print(f\"Made {closure_changes_made} changes to closure years\")\n",
    "    print(f\"File saved: {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define file path\n",
    "    file_path = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "   \n",
    "    # Run the correction\n",
    "    correct_startup_years(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5ec2f476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n",
      "Found 44 year columns\n",
      "Warning: Startup year 1970 for PROP_ID 62580 is outside data range\n",
      "Warning: Startup year 1940 for PROP_ID 55209 is outside data range\n",
      "Warning: Startup year 1963 for PROP_ID 66682 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 65193 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 52785 is outside data range\n",
      "Warning: Startup year 1958 for PROP_ID 66249 is outside data range\n",
      "Warning: Startup year 1963 for PROP_ID 66683 is outside data range\n",
      "Warning: Startup year 1974 for PROP_ID 58547 is outside data range\n",
      "Warning: Startup year 1962 for PROP_ID 57501 is outside data range\n",
      "Warning: Startup year 1965 for PROP_ID 57368 is outside data range\n",
      "Warning: Startup year 1979 for PROP_ID 53154 is outside data range\n",
      "Warning: Startup year 1948 for PROP_ID 69079 is outside data range\n",
      "Warning: Startup year 1934 for PROP_ID 65191 is outside data range\n",
      "Warning: Startup year 1965 for PROP_ID 64560 is outside data range\n",
      "Warning: Startup year 1965 for PROP_ID 66063 is outside data range\n",
      "Warning: Startup year 1973 for PROP_ID 52675 is outside data range\n",
      "Warning: Startup year 1963 for PROP_ID 57084 is outside data range\n",
      "Warning: Startup year 1952 for PROP_ID 54759 is outside data range\n",
      "PROP_ID 80820: Changed startup year from 2015 to 2016 (startup year had zero production)\n",
      "Warning: Startup year 1968 for PROP_ID 27291 is outside data range\n",
      "PROP_ID 81246: Changed startup year from 2015 to 2016 (startup year had zero production)\n",
      "Warning: Startup year 1970 for PROP_ID 27525 is outside data range\n",
      "Warning: Startup year 1928 for PROP_ID 24518 is outside data range\n",
      "Warning: Startup year 1960 for PROP_ID 28734 is outside data range\n",
      "Warning: Startup year 1950 for PROP_ID 80829 is outside data range\n",
      "Warning: Startup year 1904 for PROP_ID 24783 is outside data range\n",
      "Warning: Startup year 1900 for PROP_ID 27543 is outside data range\n",
      "Warning: Startup year 1979 for PROP_ID 29291 is outside data range\n",
      "Warning: Startup year 1969 for PROP_ID 27163 is outside data range\n",
      "Warning: Startup year 1979 for PROP_ID 27830 is outside data range\n",
      "Warning: Startup year 1938 for PROP_ID 28638 is outside data range\n",
      "Warning: Startup year 1954 for PROP_ID 27589 is outside data range\n",
      "Warning: Startup year 1976 for PROP_ID 27055 is outside data range\n",
      "PROP_ID 81644: Changed startup year from 2015 to 2017 (startup year had zero production)\n",
      "Warning: Startup year 1915 for PROP_ID 26697 is outside data range\n",
      "Warning: Startup year 1965 for PROP_ID 28004 is outside data range\n",
      "Warning: Startup year 1967 for PROP_ID 27345 is outside data range\n",
      "Warning: Startup year 1964 for PROP_ID 28250 is outside data range\n",
      "PROP_ID 33414: Changed startup year from 2014 to 2015 (startup year had zero production)\n",
      "Warning: Startup year 1976 for PROP_ID 26653 is outside data range\n",
      "PROP_ID 29744: Changed startup year from 2003 to 2016 (startup year had zero production)\n",
      "PROP_ID 37425: Changed startup year from 2004 to 2009 (startup year had zero production)\n",
      "PROP_ID 30531: Changed startup year from 2006 to 2007 (startup year had zero production)\n",
      "Warning: Startup year 1965 for PROP_ID 27385 is outside data range\n",
      "Warning: Startup year 1844 for PROP_ID 26722 is outside data range\n",
      "Warning: Startup year 1905 for PROP_ID 26696 is outside data range\n",
      "Warning: Startup year 1978 for PROP_ID 27495 is outside data range\n",
      "Warning: Startup year 1971 for PROP_ID 29286 is outside data range\n",
      "Warning: Startup year 1966 for PROP_ID 81205 is outside data range\n",
      "Warning: Startup year 1972 for PROP_ID 80697 is outside data range\n",
      "Warning: Startup year 1963 for PROP_ID 28092 is outside data range\n",
      "Warning: Startup year 1972 for PROP_ID 26568 is outside data range\n",
      "PROP_ID 80823: Changed startup year from 2015 to 2018 (startup year had zero production)\n",
      "PROP_ID 80801: Changed startup year from 2011 to 2016 (startup year had zero production)\n",
      "PROP_ID 36374: Changed startup year from 2015 to 2016 (startup year had zero production)\n",
      "PROP_ID 33083: Changed startup year from 2011 to 2012 (startup year had zero production)\n",
      "Warning: Startup year 1966 for PROP_ID 28257 is outside data range\n",
      "Warning: Startup year 1957 for PROP_ID 29352 is outside data range\n",
      "Warning: Startup year 1979 for PROP_ID 27342 is outside data range\n",
      "Warning: Startup year 1976 for PROP_ID 30666 is outside data range\n",
      "Warning: Startup year 1925 for PROP_ID 26723 is outside data range\n",
      "Warning: Startup year 1975 for PROP_ID 28955 is outside data range\n",
      "Warning: Startup year 1961 for PROP_ID 27616 is outside data range\n",
      "Warning: Startup year 1915 for PROP_ID 24469 is outside data range\n",
      "Warning: Startup year 1935 for PROP_ID 29445 is outside data range\n",
      "PROP_ID 28704: Changed startup year from 2010 to 2013 (startup year had zero production)\n",
      "Warning: Startup year 1954 for PROP_ID 24505 is outside data range\n",
      "PROP_ID 33810: Changed startup year from 2016 to 2017 (startup year had zero production)\n",
      "Warning: Startup year 1939 for PROP_ID 28264 is outside data range\n",
      "PROP_ID 66154: Changed startup year from 2010 to 2016 (startup year had zero production)\n",
      "Warning: Startup year 1932 for PROP_ID 28270 is outside data range\n",
      "Warning: Startup year 1958 for PROP_ID 27918 is outside data range\n",
      "Warning: Startup year 1966 for PROP_ID 27868 is outside data range\n",
      "Warning: Startup year 1959 for PROP_ID 28010 is outside data range\n",
      "Warning: Startup year 1974 for PROP_ID 27619 is outside data range\n",
      "PROP_ID 81197: Changed startup year from 2015 to 2016 (startup year had zero production)\n",
      "PROP_ID 81649: Changed startup year from 2015 to 2017 (startup year had zero production)\n",
      "Warning: Startup year 1955 for PROP_ID 28091 is outside data range\n",
      "Warning: Startup year 1959 for PROP_ID 24503 is outside data range\n",
      "PROP_ID 30665: Changed startup year from 2015 to 2016 (startup year had zero production)\n",
      "Warning: Startup year 1972 for PROP_ID 36775 is outside data range\n",
      "PROP_ID 81647: Changed startup year from 2015 to 2017 (startup year had zero production)\n",
      "Warning: Startup year 1960 for PROP_ID 30327 is outside data range\n",
      "PROP_ID 81642: Changed startup year from 2015 to 2017 (startup year had zero production)\n",
      "Warning: Startup year 1972 for PROP_ID 29924 is outside data range\n",
      "Warning: Startup year 1971 for PROP_ID 80699 is outside data range\n",
      "Warning: Startup year 1956 for PROP_ID 29739 is outside data range\n",
      "Warning: Startup year 1959 for PROP_ID 26654 is outside data range\n",
      "Warning: Startup year 1967 for PROP_ID 24732 is outside data range\n",
      "Warning: Startup year 1968 for PROP_ID 30738 is outside data range\n",
      "PROP_ID 35851: Changed startup year from 2007 to 2016 (startup year had zero production)\n",
      "PROP_ID 81199: Changed startup year from 2015 to 2016 (startup year had zero production)\n",
      "PROP_ID 33964: Changed startup year from 2007 to 2017 (startup year had zero production)\n",
      "Warning: Startup year 1958 for PROP_ID 35755 is outside data range\n",
      "PROP_ID 29743: Changed startup year from 2008 to 2010 (startup year had zero production)\n",
      "Warning: Startup year 1953 for PROP_ID 29283 is outside data range\n",
      "Warning: Startup year 1960 for PROP_ID 40496 is outside data range\n",
      "Warning: Startup year 1957 for PROP_ID 83375 is outside data range\n",
      "Warning: Startup year 1964 for PROP_ID 32202 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 82072 is outside data range\n",
      "Warning: Startup year 1960 for PROP_ID 37169 is outside data range\n",
      "Warning: Startup year 1913 for PROP_ID 32401 is outside data range\n",
      "Warning: Startup year 1942 for PROP_ID 74736 is outside data range\n",
      "Warning: Startup year 1957 for PROP_ID 32204 is outside data range\n",
      "Warning: Startup year 1968 for PROP_ID 31984 is outside data range\n",
      "Warning: Startup year 1969 for PROP_ID 82208 is outside data range\n",
      "Warning: Startup year 1907 for PROP_ID 32593 is outside data range\n",
      "Warning: Startup year 1978 for PROP_ID 31923 is outside data range\n",
      "Warning: Startup year 1966 for PROP_ID 60833 is outside data range\n",
      "Warning: Startup year 1916 for PROP_ID 70314 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 82245 is outside data range\n",
      "Warning: Startup year 1960 for PROP_ID 68306 is outside data range\n",
      "Warning: Startup year 1941 for PROP_ID 39426 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 82235 is outside data range\n",
      "Warning: Startup year 1958 for PROP_ID 68315 is outside data range\n",
      "Warning: Startup year 1977 for PROP_ID 32414 is outside data range\n",
      "Warning: Startup year 1977 for PROP_ID 32735 is outside data range\n",
      "Warning: Startup year 1963 for PROP_ID 31827 is outside data range\n",
      "Warning: Startup year 1976 for PROP_ID 74750 is outside data range\n",
      "Warning: Startup year 1950 for PROP_ID 31921 is outside data range\n",
      "Warning: Startup year 1954 for PROP_ID 68398 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 68518 is outside data range\n",
      "Warning: Startup year 1919 for PROP_ID 37171 is outside data range\n",
      "Warning: Startup year 1958 for PROP_ID 40336 is outside data range\n",
      "Warning: Startup year 1966 for PROP_ID 31896 is outside data range\n",
      "Warning: Startup year 1976 for PROP_ID 31826 is outside data range\n",
      "Warning: Startup year 1965 for PROP_ID 31953 is outside data range\n",
      "Warning: Startup year 1954 for PROP_ID 31823 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 35627 is outside data range\n",
      "Warning: Startup year 1957 for PROP_ID 76464 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 59166 is outside data range\n",
      "Warning: Startup year 1958 for PROP_ID 82211 is outside data range\n",
      "Warning: Startup year 1960 for PROP_ID 30484 is outside data range\n",
      "Warning: Startup year 1966 for PROP_ID 37172 is outside data range\n",
      "Warning: Startup year 1934 for PROP_ID 68530 is outside data range\n",
      "Warning: Startup year 1967 for PROP_ID 32197 is outside data range\n",
      "Warning: Startup year 1933 for PROP_ID 68538 is outside data range\n",
      "Warning: Startup year 1881 for PROP_ID 37168 is outside data range\n",
      "Warning: Startup year 1971 for PROP_ID 68073 is outside data range\n",
      "Warning: Startup year 1965 for PROP_ID 31910 is outside data range\n",
      "Warning: Startup year 1933 for PROP_ID 31914 is outside data range\n",
      "Warning: Startup year 1934 for PROP_ID 32201 is outside data range\n",
      "Warning: Startup year 1960 for PROP_ID 59242 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 82157 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 31942 is outside data range\n",
      "Warning: Startup year 1976 for PROP_ID 37784 is outside data range\n",
      "Warning: Startup year 1972 for PROP_ID 31950 is outside data range\n",
      "Warning: Startup year 1977 for PROP_ID 82216 is outside data range\n",
      "Warning: Startup year 1965 for PROP_ID 82165 is outside data range\n",
      "Warning: Startup year 1975 for PROP_ID 82190 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 32740 is outside data range\n",
      "Warning: Startup year 1976 for PROP_ID 50230 is outside data range\n",
      "Warning: Startup year 1973 for PROP_ID 54755 is outside data range\n",
      "Warning: Startup year 1942 for PROP_ID 59350 is outside data range\n",
      "Warning: Startup year 1952 for PROP_ID 31822 is outside data range\n",
      "Warning: Startup year 1977 for PROP_ID 31907 is outside data range\n",
      "Warning: Startup year 1976 for PROP_ID 31819 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 59223 is outside data range\n",
      "Warning: Startup year 1969 for PROP_ID 31904 is outside data range\n",
      "Warning: Startup year 1955 for PROP_ID 31906 is outside data range\n",
      "Warning: Startup year 1963 for PROP_ID 31911 is outside data range\n",
      "Warning: Startup year 1959 for PROP_ID 82201 is outside data range\n",
      "Warning: Startup year 1971 for PROP_ID 83381 is outside data range\n",
      "Warning: Startup year 1974 for PROP_ID 37782 is outside data range\n",
      "Warning: Startup year 1942 for PROP_ID 59243 is outside data range\n",
      "Warning: Startup year 1974 for PROP_ID 31934 is outside data range\n",
      "Warning: Startup year 1977 for PROP_ID 31893 is outside data range\n",
      "Warning: Startup year 1942 for PROP_ID 74752 is outside data range\n",
      "Warning: Startup year 1965 for PROP_ID 31631 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 36372 is outside data range\n",
      "Warning: Startup year 1974 for PROP_ID 32411 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 59253 is outside data range\n",
      "Warning: Startup year 1958 for PROP_ID 82092 is outside data range\n",
      "Warning: Startup year 1953 for PROP_ID 31835 is outside data range\n",
      "Warning: Startup year 1955 for PROP_ID 32267 is outside data range\n",
      "Warning: Startup year 1958 for PROP_ID 59258 is outside data range\n",
      "Warning: Startup year 1957 for PROP_ID 70039 is outside data range\n",
      "Warning: Startup year 1892 for PROP_ID 31857 is outside data range\n",
      "Warning: Startup year 1968 for PROP_ID 31969 is outside data range\n",
      "Warning: Startup year 1885 for PROP_ID 35181 is outside data range\n",
      "Warning: Startup year 1956 for PROP_ID 32749 is outside data range\n",
      "Warning: Startup year 1921 for PROP_ID 37803 is outside data range\n",
      "Warning: Startup year 1950 for PROP_ID 32254 is outside data range\n",
      "Warning: Startup year 1961 for PROP_ID 59270 is outside data range\n",
      "Warning: Startup year 1961 for PROP_ID 82215 is outside data range\n",
      "Warning: Startup year 1974 for PROP_ID 31834 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 32753 is outside data range\n",
      "Warning: Startup year 1965 for PROP_ID 31902 is outside data range\n",
      "Warning: Startup year 1942 for PROP_ID 76468 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 59285 is outside data range\n",
      "Warning: Startup year 1885 for PROP_ID 69426 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 82219 is outside data range\n",
      "Warning: Startup year 1979 for PROP_ID 32732 is outside data range\n",
      "Warning: Startup year 1970 for PROP_ID 32175 is outside data range\n",
      "Warning: Startup year 1979 for PROP_ID 82094 is outside data range\n",
      "Warning: Startup year 1950 for PROP_ID 38557 is outside data range\n",
      "Warning: Startup year 1967 for PROP_ID 37600 is outside data range\n",
      "Warning: Startup year 1965 for PROP_ID 52783 is outside data range\n",
      "Warning: Startup year 1975 for PROP_ID 59345 is outside data range\n",
      "Warning: Startup year 1962 for PROP_ID 54303 is outside data range\n",
      "Warning: Startup year 1962 for PROP_ID 74801 is outside data range\n",
      "Warning: Startup year 1960 for PROP_ID 52934 is outside data range\n",
      "Warning: Startup year 1902 for PROP_ID 56047 is outside data range\n",
      "Warning: Startup year 1923 for PROP_ID 59919 is outside data range\n",
      "Warning: Startup year 1958 for PROP_ID 69416 is outside data range\n",
      "Warning: Startup year 1976 for PROP_ID 32611 is outside data range\n",
      "Warning: Startup year 1950 for PROP_ID 52849 is outside data range\n",
      "Warning: Startup year 1971 for PROP_ID 26634 is outside data range\n",
      "Warning: Startup year 1963 for PROP_ID 27136 is outside data range\n",
      "Warning: Startup year 1938 for PROP_ID 27378 is outside data range\n",
      "Warning: Startup year 1959 for PROP_ID 28629 is outside data range\n",
      "Warning: Startup year 1979 for PROP_ID 27077 is outside data range\n",
      "Warning: Startup year 1885 for PROP_ID 27672 is outside data range\n",
      "Warning: Startup year 1939 for PROP_ID 27169 is outside data range\n",
      "Warning: Startup year 1938 for PROP_ID 31089 is outside data range\n",
      "Warning: Startup year 1975 for PROP_ID 34257 is outside data range\n",
      "Warning: Startup year 1977 for PROP_ID 27363 is outside data range\n",
      "Warning: Startup year 1929 for PROP_ID 27146 is outside data range\n",
      "Warning: Startup year 1936 for PROP_ID 29057 is outside data range\n",
      "Warning: Startup year 1925 for PROP_ID 27166 is outside data range\n",
      "Warning: Startup year 1978 for PROP_ID 27539 is outside data range\n",
      "Warning: Startup year 1964 for PROP_ID 33172 is outside data range\n",
      "Warning: Startup year 1956 for PROP_ID 27346 is outside data range\n",
      "Warning: Startup year 1911 for PROP_ID 28283 is outside data range\n",
      "Warning: Startup year 1950 for PROP_ID 31651 is outside data range\n",
      "Warning: Startup year 1958 for PROP_ID 37445 is outside data range\n",
      "Warning: Startup year 1907 for PROP_ID 29246 is outside data range\n",
      "Warning: Startup year 1948 for PROP_ID 26495 is outside data range\n",
      "Warning: Startup year 1949 for PROP_ID 28596 is outside data range\n",
      "Warning: Startup year 1968 for PROP_ID 29347 is outside data range\n",
      "Warning: Startup year 1972 for PROP_ID 29632 is outside data range\n",
      "Warning: Startup year 1968 for PROP_ID 27124 is outside data range\n",
      "Warning: Startup year 1966 for PROP_ID 27145 is outside data range\n",
      "PROP_ID 80402: Changed startup year from 2015 to 2016 (startup year had zero production)\n",
      "Warning: Startup year 1924 for PROP_ID 77877 is outside data range\n",
      "Warning: Startup year 1966 for PROP_ID 25831 is outside data range\n",
      "Warning: Startup year 1951 for PROP_ID 28358 is outside data range\n",
      "Warning: Startup year 1936 for PROP_ID 27542 is outside data range\n",
      "Warning: Startup year 1969 for PROP_ID 28349 is outside data range\n",
      "Warning: Startup year 1942 for PROP_ID 27729 is outside data range\n",
      "Warning: Startup year 1958 for PROP_ID 29435 is outside data range\n",
      "Warning: Startup year 1961 for PROP_ID 25727 is outside data range\n",
      "Warning: Startup year 1966 for PROP_ID 27383 is outside data range\n",
      "Warning: Startup year 1972 for PROP_ID 27303 is outside data range\n",
      "Warning: Startup year 1918 for PROP_ID 28585 is outside data range\n",
      "PROP_ID 80288: Changed startup year from 2015 to 2016 (startup year had zero production)\n",
      "Warning: Startup year 1976 for PROP_ID 30732 is outside data range\n",
      "Warning: Startup year 1953 for PROP_ID 28362 is outside data range\n",
      "\n",
      "=== RESULTS ===\n",
      "Total mines processed: 991\n",
      "Mines with startup year corrections: 25\n",
      "\n",
      "List of PROP_IDs that were changed:\n",
      "  1. 80820\n",
      "  2. 81246\n",
      "  3. 81644\n",
      "  4. 33414\n",
      "  5. 29744\n",
      "  6. 37425\n",
      "  7. 30531\n",
      "  8. 80823\n",
      "  9. 80801\n",
      " 10. 36374\n",
      " 11. 33083\n",
      " 12. 28704\n",
      " 13. 33810\n",
      " 14. 66154\n",
      " 15. 81197\n",
      " 16. 81649\n",
      " 17. 30665\n",
      " 18. 81647\n",
      " 19. 81642\n",
      " 20. 35851\n",
      " 21. 81199\n",
      " 22. 33964\n",
      " 23. 29743\n",
      " 24. 80402\n",
      " 25. 80288\n",
      "\n",
      "File saved: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fix_zero_startup_years(file_path):\n",
    "    \"\"\"\n",
    "    Modifies the START_UP_YR for mines that have zero production in their startup year.\n",
    "    If a mine's startup year shows 0 production, changes it to the first year with non-zero production.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file\n",
    "    \"\"\"\n",
    "    print(f\"Reading file: {file_path}\")\n",
    "    \n",
    "    # Read the dataset\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    if 'START_UP_YR' not in df.columns:\n",
    "        print(\"Error: START_UP_YR column not found\")\n",
    "        return\n",
    "    \n",
    "    if 'PROP_ID' not in df.columns:\n",
    "        print(\"Error: PROP_ID column not found\")\n",
    "        return\n",
    "    \n",
    "    # Get year columns (1980-2023)\n",
    "    year_columns = [str(year) for year in range(1980, 2024)]\n",
    "    existing_year_columns = [col for col in year_columns if col in df.columns]\n",
    "    existing_year_columns.sort(key=int)  # Ensure chronological order\n",
    "    \n",
    "    print(f\"Found {len(existing_year_columns)} year columns\")\n",
    "    \n",
    "    # Convert year columns to numeric\n",
    "    for col in existing_year_columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Convert START_UP_YR to numeric\n",
    "    df['START_UP_YR'] = pd.to_numeric(df['START_UP_YR'], errors='coerce')\n",
    "    \n",
    "    # Track changes\n",
    "    changes_made = 0\n",
    "    changed_prop_ids = []\n",
    "    \n",
    "    # Process each mine\n",
    "    for idx, row in df.iterrows():\n",
    "        startup_year = row['START_UP_YR']\n",
    "        prop_id = row['PROP_ID']\n",
    "        \n",
    "        # Skip if no valid startup year\n",
    "        if pd.isna(startup_year):\n",
    "            continue\n",
    "        \n",
    "        startup_year = int(startup_year)\n",
    "        startup_year_str = str(startup_year)\n",
    "        \n",
    "        # Check if startup year column exists in our data\n",
    "        if startup_year_str not in existing_year_columns:\n",
    "            print(f\"Warning: Startup year {startup_year} for PROP_ID {prop_id} is outside data range\")\n",
    "            continue\n",
    "        \n",
    "        # Get production value for the startup year\n",
    "        startup_production = row[startup_year_str]\n",
    "        \n",
    "        # Check if production in startup year is exactly 0 (excluding NaN/empty cells)\n",
    "        if not pd.isna(startup_production) and startup_production == 0:\n",
    "            \n",
    "            # Find first year with non-zero production\n",
    "            first_production_year = None\n",
    "            for year_col in existing_year_columns:\n",
    "                year = int(year_col)\n",
    "                production = row[year_col]\n",
    "                \n",
    "                # Check if there's non-zero production\n",
    "                if not pd.isna(production) and production > 0:\n",
    "                    first_production_year = year\n",
    "                    break\n",
    "            \n",
    "            # If we found a year with non-zero production, update startup year\n",
    "            if first_production_year:\n",
    "                df.at[idx, 'START_UP_YR'] = first_production_year\n",
    "                changes_made += 1\n",
    "                changed_prop_ids.append(prop_id)\n",
    "                print(f\"PROP_ID {prop_id}: Changed startup year from {startup_year} to {first_production_year} (startup year had zero production)\")\n",
    "            else:\n",
    "                print(f\"Warning: PROP_ID {prop_id} has zero production in startup year {startup_year} but no non-zero production found in any year\")\n",
    "    \n",
    "    # Save the updated file\n",
    "    df.to_excel(file_path, index=False)\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"\\n=== RESULTS ===\")\n",
    "    print(f\"Total mines processed: {len(df)}\")\n",
    "    print(f\"Mines with startup year corrections: {changes_made}\")\n",
    "    \n",
    "    if changed_prop_ids:\n",
    "        print(f\"\\nList of PROP_IDs that were changed:\")\n",
    "        for i, prop_id in enumerate(changed_prop_ids, 1):\n",
    "            print(f\"{i:3d}. {prop_id}\")\n",
    "    else:\n",
    "        print(\"\\nNo mines required startup year corrections.\")\n",
    "    \n",
    "    print(f\"\\nFile saved: {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define file path\n",
    "    file_path = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "    \n",
    "    # Run the correction\n",
    "    fix_zero_startup_years(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d041c2e",
   "metadata": {},
   "source": [
    "Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c4ec97e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx...\n",
      "Adding REGION column...\n",
      "Calculating mine lifetimes...\n",
      "Missing PROJ_CLOSURE_YR values before imputation: 400\n",
      "Number of mines with valid lifetimes (for median calculation): 591\n",
      "\n",
      "Level 1 (Commodity+Country+MineType) combinations: 136\n",
      "Level 2 (Commodity+Region+MineType) combinations: 68\n",
      "Level 3 (Commodity+MineType) combinations: 14\n",
      "\n",
      "Applying hierarchical imputation based on median lifetimes...\n",
      "Using criteria: Check next level if lifetime < 30 or > 50 years, or if sample count < 5\n",
      "\n",
      "Imputation results:\n",
      "Total imputed values: 398\n",
      "  Level 1 (Commodity+Country+MineType): 90\n",
      "  Level 2 (Commodity+Region+MineType): 203\n",
      "  Level 3 (Commodity+MineType): 105\n",
      "\n",
      "Skipped imputation at Level 1 (used lower level instead):\n",
      "  Due to small sample size (< 5): 36\n",
      "  Due to low lifetime (< 30 years): 157\n",
      "  Due to high lifetime (> 50 years): 38\n",
      "\n",
      "Skipped imputation at Level 2 (used lower level instead):\n",
      "  Due to small sample size (< 5): 31\n",
      "  Due to low lifetime (< 30 years): 16\n",
      "  Due to high lifetime (> 50 years): 22\n",
      "\n",
      "Skipped imputation at Level 3 (left missing):\n",
      "  Due to small sample size (< 3): 1\n",
      "\n",
      "Missing PROJ_CLOSURE_YR values after imputation: 2\n",
      "Imputation coverage: 99.50%\n",
      "\n",
      "Checking for inconsistencies where imputed closure year is earlier than the latest production year...\n",
      "\n",
      "Found 98 mines where the imputed closure year was earlier than the latest production year\n",
      "Corrected all 98 mines by setting closure year to latest production year\n",
      "\n",
      "Corrections by original imputation level:\n",
      "  Commodity+Region+MineType: 41 mines\n",
      "  Commodity+Global+MineType: 37 mines\n",
      "  Commodity+Country+MineType: 20 mines\n",
      "\n",
      "=== MINES TO BE DROPPED (Missing Closure Years) ===\n",
      "Total mines to be dropped: 2\n",
      "\n",
      "Detailed list of dropped mines:\n",
      "--------------------------------------------------------------------------------\n",
      "PROP_ID         PRIMARY_COMMODITY         MINE_TYPE1                    \n",
      "--------------------------------------------------------------------------------\n",
      "70310           Lithium                   Underground                   \n",
      "74801           Manganese                 Underground                   \n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF DROPPED MINES BY COMMODITY:\n",
      "  Lithium: 1 mines\n",
      "  Manganese: 1 mines\n",
      "\n",
      "SUMMARY OF DROPPED MINES BY MINE TYPE:\n",
      "  Underground: 2 mines\n",
      "\n",
      "=== FINAL CLEANUP ===\n",
      "Removed 2 rows with missing closure years after imputation\n",
      "Final dataset: 989 rows (started with 991 rows)\n",
      "Total rows removed: 2\n",
      "\n",
      "Saving cleaned and imputed data to Data Output/Commodity Production/Commodity_Production-1980_2023_imputed.xlsx...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the mapping of countries to regions\n",
    "COUNTRY_TO_REGION = {\n",
    "    # Africa\n",
    "    'Ghana': 'Africa',\n",
    "    'Guinea': 'Africa',\n",
    "    'Sierra Leone': 'Africa',\n",
    "    'South Africa': 'Africa',\n",
    "    'Mozambique': 'Africa',\n",
    "    'Zimbabwe': 'Africa',\n",
    "    'Botswana': 'Africa',\n",
    "    'Tanzania': 'Africa',\n",
    "    'Morocco': 'Africa',\n",
    "    'Dem. Rep. Congo': 'Africa',\n",
    "    'Zambia': 'Africa',\n",
    "    'Namibia': 'Africa',\n",
    "    'Mauritania': 'Africa',\n",
    "    'Madagascar': 'Africa',\n",
    "    'Liberia': 'Africa',\n",
    "    'Algeria': 'Africa',\n",
    "    'Gabon': 'Africa',\n",
    "    'Rwanda': 'Africa',\n",
    "    'Eritrea': 'Africa',\n",
    "    'Burkina Faso': 'Africa',\n",
    "   \n",
    "    # Asia\n",
    "    'Saudi Arabia': 'Middle East',\n",
    "    'India': 'Asia',\n",
    "    'China': 'Asia',\n",
    "    'Kazakhstan': 'Asia',\n",
    "    'Russia': 'Eurasia',\n",
    "    'Indonesia': 'Asia',\n",
    "    'Mongolia': 'Asia',\n",
    "    'Uzbekistan': 'Asia',\n",
    "    'Bangladesh': 'Asia',\n",
    "    'Philippines': 'Asia',\n",
    "    'Türkiye': 'Eurasia',\n",
    "    'Pakistan': 'Asia',\n",
    "    'Thailand': 'Asia',\n",
    "    'Myanmar': 'Asia',\n",
    "    'Laos': 'Asia',\n",
    "    'Iran': 'Middle East',\n",
    "    'Armenia': 'Eurasia',\n",
    "    'Sri Lanka': 'Asia',\n",
    "    'Malaysia': 'Asia',\n",
    "    'Tajikistan': 'Asia',\n",
    "    'South Korea': 'Asia',\n",
    "    'Vietnam': 'Asia',\n",
    "   \n",
    "    # Europe\n",
    "    'Greece': 'Europe',\n",
    "    'Hungary': 'Europe',\n",
    "    'Montenegro': 'Europe',\n",
    "    'Ukraine': 'Europe',\n",
    "    'Norway': 'Europe',\n",
    "    'Czechia': 'Europe',\n",
    "    'Poland': 'Europe',\n",
    "    'Slovenia': 'Europe',\n",
    "    'Spain': 'Europe',\n",
    "    'Serbia': 'Europe',\n",
    "    'Germany': 'Europe',\n",
    "    'United Kingdom': 'Europe',\n",
    "    'Bulgaria': 'Europe',\n",
    "    'Sweden': 'Europe',\n",
    "    'Portugal': 'Europe',\n",
    "    'Romania': 'Europe',\n",
    "    'North Macedonia': 'Europe',\n",
    "    'Albania': 'Europe',\n",
    "    'Georgia': 'Eurasia',\n",
    "    'Cyprus': 'Europe',\n",
    "    'Austria': 'Europe',\n",
    "    'Bosnia & Herzegovina': 'Europe',\n",
    "    'Finland': 'Europe',\n",
    "    'Ireland': 'Europe',\n",
    "   \n",
    "    # North America\n",
    "    'USA': 'North America',\n",
    "    'Canada': 'North America',\n",
    "    'Mexico': 'North America',\n",
    "    'Dominican Republic': 'Caribbean',\n",
    "    'Jamaica': 'Caribbean',\n",
    "    'Panama': 'Central America',\n",
    "    'Guatemala': 'Central America',\n",
    "    'Cuba': 'Caribbean',\n",
    "    'Honduras': 'Central America',\n",
    "   \n",
    "    # South America\n",
    "    'Guyana': 'South America',\n",
    "    'Brazil': 'South America',\n",
    "    'Venezuela': 'South America',\n",
    "    'Colombia': 'South America',\n",
    "    'Argentina': 'South America',\n",
    "    'Chile': 'South America',\n",
    "    'Peru': 'South America',\n",
    "    'Ecuador': 'South America',\n",
    "    'Bolivia': 'South America',\n",
    "   \n",
    "    # Oceania\n",
    "    'Australia': 'Oceania',\n",
    "    'New Zealand': 'Oceania',\n",
    "    'Papua New Guinea': 'Oceania',\n",
    "    'New Caledonia': 'Oceania'\n",
    "}\n",
    "\n",
    "def main():\n",
    "    # Path to the input and output files\n",
    "    input_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "    output_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_imputed.xlsx\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Read the Excel file\n",
    "    print(f\"Reading data from {input_file}...\")\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # Store initial row count\n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # Add a REGION column based on COUNTRY_NAME\n",
    "    print(\"Adding REGION column...\")\n",
    "    df['REGION'] = df['COUNTRY_NAME'].map(COUNTRY_TO_REGION)\n",
    "    \n",
    "    # Calculate mine lifetimes for mines with both START_UP_YR and PROJ_CLOSURE_YR\n",
    "    print(\"Calculating mine lifetimes...\")\n",
    "    df['MINE_LIFETIME'] = None\n",
    "    mask = (df['START_UP_YR'].notna()) & (df['PROJ_CLOSURE_YR'].notna())\n",
    "    df.loc[mask, 'MINE_LIFETIME'] = df.loc[mask, 'PROJ_CLOSURE_YR'] - df.loc[mask, 'START_UP_YR']\n",
    "    \n",
    "    # Create a copy of the original PROJ_CLOSURE_YR column to compare later\n",
    "    df['ORIGINAL_PROJ_CLOSURE_YR'] = df['PROJ_CLOSURE_YR']\n",
    "    \n",
    "    # Count missing values before imputation\n",
    "    missing_before = df['PROJ_CLOSURE_YR'].isna().sum()\n",
    "    print(f\"Missing PROJ_CLOSURE_YR values before imputation: {missing_before}\")\n",
    "    \n",
    "    # Filter rows with valid mine lifetimes for calculating medians\n",
    "    lifetime_df = df[(df['MINE_LIFETIME'].notna()) & (df['MINE_LIFETIME'] > 0)]\n",
    "    \n",
    "    print(f\"Number of mines with valid lifetimes (for median calculation): {len(lifetime_df)}\")\n",
    "    \n",
    "    # Calculate median lifetimes and counts for each level of aggregation\n",
    "    # Level 1: PRIMARY_COMMODITY + COUNTRY_NAME + MINE_TYPE1\n",
    "    level1_medians = lifetime_df.groupby(['PRIMARY_COMMODITY', 'COUNTRY_NAME', 'MINE_TYPE1'])['MINE_LIFETIME'].agg(['median', 'count'])\n",
    "    # Level 2: PRIMARY_COMMODITY + REGION + MINE_TYPE1\n",
    "    level2_medians = lifetime_df.groupby(['PRIMARY_COMMODITY', 'REGION', 'MINE_TYPE1'])['MINE_LIFETIME'].agg(['median', 'count'])\n",
    "    # Level 3: PRIMARY_COMMODITY + MINE_TYPE1 (global)\n",
    "    level3_medians = lifetime_df.groupby(['PRIMARY_COMMODITY', 'MINE_TYPE1'])['MINE_LIFETIME'].agg(['median', 'count'])\n",
    "    \n",
    "    # Print summary of available combinations\n",
    "    print(f\"\\nLevel 1 (Commodity+Country+MineType) combinations: {len(level1_medians)}\")\n",
    "    print(f\"Level 2 (Commodity+Region+MineType) combinations: {len(level2_medians)}\")\n",
    "    print(f\"Level 3 (Commodity+MineType) combinations: {len(level3_medians)}\")\n",
    "    \n",
    "    # Apply hierarchical imputation with the new rules\n",
    "    print(\"\\nApplying hierarchical imputation based on median lifetimes...\")\n",
    "    print(\"Using criteria: Check next level if lifetime < 30 or > 50 years, or if sample count < 5\")\n",
    "    \n",
    "    # Create a column to track imputation level\n",
    "    if 'IMPUTATION_LEVEL' not in df.columns:\n",
    "        df['IMPUTATION_LEVEL'] = None\n",
    "    \n",
    "    # Create counters for each imputation case\n",
    "    level1_count = 0\n",
    "    level2_count = 0\n",
    "    level3_count = 0\n",
    "    skipped_level1_low_lifetime = 0\n",
    "    skipped_level1_high_lifetime = 0\n",
    "    skipped_level1_small_sample = 0\n",
    "    skipped_level2_low_lifetime = 0\n",
    "    skipped_level2_high_lifetime = 0\n",
    "    skipped_level2_small_sample = 0\n",
    "    skipped_level3_small_sample = 0 \n",
    "\n",
    "    # Track the mines that will be imputed to help with consistency check later\n",
    "    imputed_mines = []\n",
    "    \n",
    "    # Loop through rows with missing PROJ_CLOSURE_YR but valid START_UP_YR\n",
    "    for idx, row in df[(df['PROJ_CLOSURE_YR'].isna()) & (df['START_UP_YR'].notna())].iterrows():\n",
    "        key1 = (row['PRIMARY_COMMODITY'], row['COUNTRY_NAME'], row['MINE_TYPE1'])\n",
    "        key2 = (row['PRIMARY_COMMODITY'], row['REGION'], row['MINE_TYPE1']) if pd.notna(row['REGION']) else None\n",
    "        key3 = (row['PRIMARY_COMMODITY'], row['MINE_TYPE1'])\n",
    "        \n",
    "        # Try level 1 first\n",
    "        use_level1 = False\n",
    "        if key1 in level1_medians.index:\n",
    "            lifetime = level1_medians.loc[key1, 'median']\n",
    "            count = level1_medians.loc[key1, 'count']\n",
    "            \n",
    "            # Check if we should use this level\n",
    "            if count >= 3 and 30 <= lifetime <= 50:\n",
    "                use_level1 = True\n",
    "            else:\n",
    "                if count < 3:\n",
    "                    skipped_level1_small_sample += 1\n",
    "                elif lifetime < 30:\n",
    "                    skipped_level1_low_lifetime += 1\n",
    "                elif lifetime > 50:\n",
    "                    skipped_level1_high_lifetime += 1\n",
    "        \n",
    "        if use_level1:\n",
    "            # Use level 1 for imputation\n",
    "            lifetime = int(round(lifetime))\n",
    "            df.at[idx, 'PROJ_CLOSURE_YR'] = row['START_UP_YR'] + lifetime\n",
    "            df.at[idx, 'IMPUTATION_LEVEL'] = 'Commodity+Country+MineType'\n",
    "            df.at[idx, 'MINE_LIFETIME'] = lifetime\n",
    "            imputed_mines.append(idx)\n",
    "            level1_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Try level 2 if level 1 doesn't meet criteria\n",
    "        use_level2 = False\n",
    "        if key2 is not None and key2 in level2_medians.index:\n",
    "            lifetime = level2_medians.loc[key2, 'median']\n",
    "            count = level2_medians.loc[key2, 'count']\n",
    "            \n",
    "            # Check if we should use this level\n",
    "            if count >= 3 and 30 <= lifetime <= 50:\n",
    "                use_level2 = True\n",
    "            else:\n",
    "                if count < 3:\n",
    "                    skipped_level2_small_sample += 1\n",
    "                elif lifetime < 30:\n",
    "                    skipped_level2_low_lifetime += 1\n",
    "                elif lifetime > 50:\n",
    "                    skipped_level2_high_lifetime += 1\n",
    "        \n",
    "        if use_level2:\n",
    "            # Use level 2 for imputation\n",
    "            lifetime = int(round(lifetime))\n",
    "            df.at[idx, 'PROJ_CLOSURE_YR'] = row['START_UP_YR'] + lifetime\n",
    "            df.at[idx, 'IMPUTATION_LEVEL'] = 'Commodity+Region+MineType'\n",
    "            df.at[idx, 'MINE_LIFETIME'] = lifetime\n",
    "            imputed_mines.append(idx)\n",
    "            level2_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Try level 3 if level 2 doesn't meet criteria\n",
    "        use_level3 = False\n",
    "        if key3 in level3_medians.index:\n",
    "            lifetime = level3_medians.loc[key3, 'median']\n",
    "            count = level3_medians.loc[key3, 'count']\n",
    "\n",
    "            # Level 3: only require at least 3 samples; no lifetime guardrails\n",
    "            if count >= 3:\n",
    "                use_level3 = True\n",
    "            else:\n",
    "                skipped_level3_small_sample += 1\n",
    "\n",
    "        if use_level3:\n",
    "            lifetime = int(round(lifetime))\n",
    "            df.at[idx, 'PROJ_CLOSURE_YR'] = row['START_UP_YR'] + lifetime\n",
    "            df.at[idx, 'IMPUTATION_LEVEL'] = 'Commodity+Global+MineType'\n",
    "            df.at[idx, 'MINE_LIFETIME'] = lifetime\n",
    "            imputed_mines.append(idx)\n",
    "            level3_count += 1\n",
    "     \n",
    "    # Ensure all PROJ_CLOSURE_YR values are integers (for both original and imputed values)\n",
    "    mask = df['PROJ_CLOSURE_YR'].notna()\n",
    "    df.loc[mask, 'PROJ_CLOSURE_YR'] = df.loc[mask, 'PROJ_CLOSURE_YR'].apply(lambda x: int(round(x)))\n",
    "    \n",
    "    # Count missing values after imputation\n",
    "    missing_after = df['PROJ_CLOSURE_YR'].isna().sum()\n",
    "    imputed_count = level1_count + level2_count + level3_count\n",
    "    print(f\"\\nImputation results:\")\n",
    "    print(f\"Total imputed values: {imputed_count}\")\n",
    "    print(f\"  Level 1 (Commodity+Country+MineType): {level1_count}\")\n",
    "    print(f\"  Level 2 (Commodity+Region+MineType): {level2_count}\")\n",
    "    print(f\"  Level 3 (Commodity+MineType): {level3_count}\")\n",
    "    \n",
    "    print(f\"\\nSkipped imputation at Level 1 (used lower level instead):\")\n",
    "    print(f\"  Due to small sample size (< 5): {skipped_level1_small_sample}\")\n",
    "    print(f\"  Due to low lifetime (< 30 years): {skipped_level1_low_lifetime}\")\n",
    "    print(f\"  Due to high lifetime (> 50 years): {skipped_level1_high_lifetime}\")\n",
    "    \n",
    "    print(f\"\\nSkipped imputation at Level 2 (used lower level instead):\")\n",
    "    print(f\"  Due to small sample size (< 5): {skipped_level2_small_sample}\")\n",
    "    print(f\"  Due to low lifetime (< 30 years): {skipped_level2_low_lifetime}\")\n",
    "    print(f\"  Due to high lifetime (> 50 years): {skipped_level2_high_lifetime}\")\n",
    "\n",
    "    print(f\"\\nSkipped imputation at Level 3 (left missing):\")\n",
    "    print(f\"  Due to small sample size (< 3): {skipped_level3_small_sample}\")\n",
    "\n",
    "    print(f\"\\nMissing PROJ_CLOSURE_YR values after imputation: {missing_after}\")\n",
    "    \n",
    "    if missing_before > 0:\n",
    "        print(f\"Imputation coverage: {100 * (missing_before - missing_after) / missing_before:.2f}%\")\n",
    "    else:\n",
    "        print(\"No missing values were present before imputation.\")\n",
    "    \n",
    "    # Find the year columns in the dataset\n",
    "    year_columns = [col for col in df.columns if str(col).isdigit() and 1980 <= int(col) <= 2023]\n",
    "    \n",
    "    if year_columns:\n",
    "        print(\"\\nChecking for inconsistencies where imputed closure year is earlier than the latest production year...\")\n",
    "        \n",
    "        inconsistent_mines = []\n",
    "        corrected_mines = []\n",
    "        \n",
    "        # For each imputed mine, find the latest year with production data\n",
    "        for idx in imputed_mines:\n",
    "            row = df.loc[idx]\n",
    "            \n",
    "            # Extract the production data for years\n",
    "            production_data = row[year_columns]\n",
    "            \n",
    "            # Find the latest year with a non-zero, non-NaN production value\n",
    "            latest_production_year = None\n",
    "            for year in sorted(year_columns, reverse=True):\n",
    "                if pd.notna(row[year]) and row[year] > 0:\n",
    "                    latest_production_year = int(year)\n",
    "                    break\n",
    "            \n",
    "            # If there is production data and the imputed closure year is earlier\n",
    "            if latest_production_year is not None and row['PROJ_CLOSURE_YR'] < latest_production_year:\n",
    "                inconsistent_mines.append({\n",
    "                    'PROP_ID': row['PROP_ID'],\n",
    "                    'PRIMARY_COMMODITY': row['PRIMARY_COMMODITY'],\n",
    "                    'COUNTRY_NAME': row['COUNTRY_NAME'],\n",
    "                    'MINE_TYPE1': row['MINE_TYPE1'],\n",
    "                    'START_UP_YR': row['START_UP_YR'],\n",
    "                    'ORIGINAL_IMPUTED_CLOSURE_YEAR': row['PROJ_CLOSURE_YR'],\n",
    "                    'LATEST_PRODUCTION_YEAR': latest_production_year,\n",
    "                    'MINE_LIFETIME': row['MINE_LIFETIME'],\n",
    "                    'IMPUTATION_LEVEL': row['IMPUTATION_LEVEL']\n",
    "                })\n",
    "                \n",
    "                # Correct the closure year to the latest production year\n",
    "                df.at[idx, 'PROJ_CLOSURE_YR'] = latest_production_year\n",
    "                df.at[idx, 'MINE_LIFETIME'] = latest_production_year - row['START_UP_YR']\n",
    "                corrected_mines.append(idx)\n",
    "        \n",
    "        # Report inconsistencies and corrections\n",
    "        if inconsistent_mines:\n",
    "            print(f\"\\nFound {len(inconsistent_mines)} mines where the imputed closure year was earlier than the latest production year\")\n",
    "            print(f\"Corrected all {len(corrected_mines)} mines by setting closure year to latest production year\")\n",
    "            \n",
    "            # Group inconsistencies by imputation level\n",
    "            inconsistent_df = pd.DataFrame(inconsistent_mines)\n",
    "            inconsistent_by_level = inconsistent_df['IMPUTATION_LEVEL'].value_counts()\n",
    "            print(\"\\nCorrections by original imputation level:\")\n",
    "            for level, count in inconsistent_by_level.items():\n",
    "                print(f\"  {level}: {count} mines\")\n",
    "        else:\n",
    "            print(\"No inconsistencies found. All imputed closure years are after the latest production year.\")\n",
    "    else:\n",
    "        print(\"\\nCould not find year columns to check for production consistency.\")\n",
    "    \n",
    "    # Remove rows with missing closure years after imputation\n",
    "    rows_before_removal = len(df)\n",
    "    \n",
    "    # Identify rows that will be dropped (missing closure years)\n",
    "    dropped_rows = df[df['PROJ_CLOSURE_YR'].isna()]\n",
    "    \n",
    "    # Save information about dropped mines\n",
    "    if len(dropped_rows) > 0:\n",
    "        dropped_info = dropped_rows[['PROP_ID', 'PRIMARY_COMMODITY', 'MINE_TYPE1']].copy()\n",
    "        print(f\"\\n=== MINES TO BE DROPPED (Missing Closure Years) ===\")\n",
    "        print(f\"Total mines to be dropped: {len(dropped_info)}\")\n",
    "        print(\"\\nDetailed list of dropped mines:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'PROP_ID':<15} {'PRIMARY_COMMODITY':<25} {'MINE_TYPE1':<30}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for idx, row in dropped_info.iterrows():\n",
    "            prop_id = str(row['PROP_ID'])[:14]  # Truncate if too long\n",
    "            commodity = str(row['PRIMARY_COMMODITY'])[:24]  # Truncate if too long\n",
    "            mine_type = str(row['MINE_TYPE1'])[:29]  # Truncate if too long\n",
    "            print(f\"{prop_id:<15} {commodity:<25} {mine_type:<30}\")\n",
    "        \n",
    "        # Summary by commodity and mine type\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"SUMMARY OF DROPPED MINES BY COMMODITY:\")\n",
    "        commodity_summary = dropped_info['PRIMARY_COMMODITY'].value_counts()\n",
    "        for commodity, count in commodity_summary.items():\n",
    "            print(f\"  {commodity}: {count} mines\")\n",
    "        \n",
    "        print(f\"\\nSUMMARY OF DROPPED MINES BY MINE TYPE:\")\n",
    "        mine_type_summary = dropped_info['MINE_TYPE1'].value_counts()\n",
    "        for mine_type, count in mine_type_summary.items():\n",
    "            print(f\"  {mine_type}: {count} mines\")\n",
    "    \n",
    "    # Remove the rows with missing closure years\n",
    "    df = df[df['PROJ_CLOSURE_YR'].notna()]\n",
    "    rows_after_removal = len(df)\n",
    "    rows_removed = rows_before_removal - rows_after_removal\n",
    "    \n",
    "    print(f\"\\n=== FINAL CLEANUP ===\")\n",
    "    print(f\"Removed {rows_removed} rows with missing closure years after imputation\")\n",
    "    print(f\"Final dataset: {rows_after_removal} rows (started with {initial_rows} rows)\")\n",
    "    print(f\"Total rows removed: {initial_rows - rows_after_removal}\")\n",
    "    \n",
    "    # Save the imputed dataframe to Excel\n",
    "    print(f\"\\nSaving cleaned and imputed data to {output_file}...\")\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1e251ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading imputed dataset: Data Output/Commodity Production/Commodity_Production-1980_2023_imputed.xlsx\n",
      "Imputed dataset loaded: 989 rows\n",
      "Reading original dataset: Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\n",
      "Original dataset loaded: 991 rows\n",
      "PROJ_CLOSURE_YR values in original dataset: 591\n",
      "Valid PROJ_CLOSURE_YR values in imputed dataset: 989\n",
      "\n",
      "=== MERGE RESULTS ===\n",
      "PROJ_CLOSURE_YR values before merge: 591\n",
      "PROJ_CLOSURE_YR values after merge: 989\n",
      "Rows with new closure years added: 398\n",
      "Rows with closure years updated: 0\n",
      "Total changes made: 398\n",
      "\n",
      "=== MINES TO BE DROPPED (Missing Closure Years) ===\n",
      "Total mines to be dropped: 2\n",
      "\n",
      "Detailed list of dropped mines:\n",
      "--------------------------------------------------------------------------------\n",
      "PROP_ID         PRIMARY_COMMODITY         MINE_TYPE1                    \n",
      "--------------------------------------------------------------------------------\n",
      "70310           Lithium                   Underground                   \n",
      "74801           Manganese                 Underground                   \n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF DROPPED MINES BY COMMODITY:\n",
      "  Lithium: 1 mines\n",
      "  Manganese: 1 mines\n",
      "\n",
      "SUMMARY OF DROPPED MINES BY MINE TYPE:\n",
      "  Underground: 2 mines\n",
      "\n",
      "=== FINAL DATASET SUMMARY ===\n",
      "Original dataset rows: 991\n",
      "Rows removed due to missing closure years: 2\n",
      "Final dataset rows: 989\n",
      "Data retention rate: 99.8%\n",
      "\n",
      "Matching statistics:\n",
      "PROP_IDs in imputed dataset: 989\n",
      "PROP_IDs in original dataset: 991\n",
      "Matching PROP_IDs: 989\n",
      "PROP_IDs with valid closure data transferred: 989\n",
      "\n",
      "Final dataset saved: Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def merge_closure_years_final():\n",
    "    \"\"\"\n",
    "    Merges PROJ_CLOSURE_YR from the imputed dataset into the original dataset\n",
    "    based on PROP_ID, removes rows without closure years, and saves as final dataset.\n",
    "    \"\"\"\n",
    "    # Define file paths\n",
    "    imputed_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_imputed.xlsx\"\n",
    "    original_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023.xlsx\"\n",
    "    output_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "    \n",
    "    print(f\"Reading imputed dataset: {imputed_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the imputed dataset (source of closure years)\n",
    "        imputed_df = pd.read_excel(imputed_file)\n",
    "        print(f\"Imputed dataset loaded: {len(imputed_df)} rows\")\n",
    "        \n",
    "        # Check required columns in imputed file\n",
    "        if 'PROP_ID' not in imputed_df.columns:\n",
    "            print(\"Error: PROP_ID column not found in imputed file\")\n",
    "            return\n",
    "        if 'PROJ_CLOSURE_YR' not in imputed_df.columns:\n",
    "            print(\"Error: PROJ_CLOSURE_YR column not found in imputed file\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Reading original dataset: {original_file}\")\n",
    "        \n",
    "        # Read the original dataset (target)\n",
    "        original_df = pd.read_excel(original_file)\n",
    "        print(f\"Original dataset loaded: {len(original_df)} rows\")\n",
    "        \n",
    "        # Check required columns in original file\n",
    "        if 'PROP_ID' not in original_df.columns:\n",
    "            print(\"Error: PROP_ID column not found in original file\")\n",
    "            return\n",
    "        if 'PROJ_CLOSURE_YR' not in original_df.columns:\n",
    "            print(\"Error: PROJ_CLOSURE_YR column not found in original file\")\n",
    "            return\n",
    "        \n",
    "        # Count closure years before merge\n",
    "        closure_before = original_df['PROJ_CLOSURE_YR'].notna().sum()\n",
    "        print(f\"PROJ_CLOSURE_YR values in original dataset: {closure_before}\")\n",
    "        \n",
    "        # Create lookup dictionary from imputed dataset\n",
    "        # Only include non-null closure years\n",
    "        closure_lookup = imputed_df[imputed_df['PROJ_CLOSURE_YR'].notna()].set_index('PROP_ID')['PROJ_CLOSURE_YR'].to_dict()\n",
    "        print(f\"Valid PROJ_CLOSURE_YR values in imputed dataset: {len(closure_lookup)}\")\n",
    "        \n",
    "        # Track changes\n",
    "        rows_updated = 0\n",
    "        rows_added = 0\n",
    "        \n",
    "        # Update closure years in original dataset\n",
    "        for idx, row in original_df.iterrows():\n",
    "            prop_id = row['PROP_ID']\n",
    "            current_closure = row['PROJ_CLOSURE_YR']\n",
    "            \n",
    "            # Check if we have closure data for this property\n",
    "            if prop_id in closure_lookup:\n",
    "                new_closure = closure_lookup[prop_id]\n",
    "                \n",
    "                if pd.isna(current_closure):\n",
    "                    # Adding new closure year\n",
    "                    original_df.at[idx, 'PROJ_CLOSURE_YR'] = new_closure\n",
    "                    rows_added += 1\n",
    "                elif current_closure != new_closure:\n",
    "                    # Updating existing closure year\n",
    "                    original_df.at[idx, 'PROJ_CLOSURE_YR'] = new_closure\n",
    "                    rows_updated += 1\n",
    "        \n",
    "        # Count closure years after merge\n",
    "        closure_after = original_df['PROJ_CLOSURE_YR'].notna().sum()\n",
    "        \n",
    "        print(f\"\\n=== MERGE RESULTS ===\")\n",
    "        print(f\"PROJ_CLOSURE_YR values before merge: {closure_before}\")\n",
    "        print(f\"PROJ_CLOSURE_YR values after merge: {closure_after}\")\n",
    "        print(f\"Rows with new closure years added: {rows_added}\")\n",
    "        print(f\"Rows with closure years updated: {rows_updated}\")\n",
    "        print(f\"Total changes made: {rows_added + rows_updated}\")\n",
    "        \n",
    "        # Identify rows that will be dropped (missing closure years)\n",
    "        rows_before_removal = len(original_df)\n",
    "        dropped_rows = original_df[original_df['PROJ_CLOSURE_YR'].isna()]\n",
    "        \n",
    "        # Save information about dropped mines\n",
    "        if len(dropped_rows) > 0:\n",
    "            dropped_info = dropped_rows[['PROP_ID', 'PRIMARY_COMMODITY', 'MINE_TYPE1']].copy()\n",
    "            print(f\"\\n=== MINES TO BE DROPPED (Missing Closure Years) ===\")\n",
    "            print(f\"Total mines to be dropped: {len(dropped_info)}\")\n",
    "            print(\"\\nDetailed list of dropped mines:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{'PROP_ID':<15} {'PRIMARY_COMMODITY':<25} {'MINE_TYPE1':<30}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for idx, row in dropped_info.iterrows():\n",
    "                prop_id = str(row['PROP_ID'])[:14]  # Truncate if too long\n",
    "                commodity = str(row['PRIMARY_COMMODITY'])[:24]  # Truncate if too long\n",
    "                mine_type = str(row['MINE_TYPE1'])[:29]  # Truncate if too long\n",
    "                print(f\"{prop_id:<15} {commodity:<25} {mine_type:<30}\")\n",
    "            \n",
    "            # Summary by commodity and mine type\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"SUMMARY OF DROPPED MINES BY COMMODITY:\")\n",
    "            commodity_summary = dropped_info['PRIMARY_COMMODITY'].value_counts()\n",
    "            for commodity, count in commodity_summary.items():\n",
    "                print(f\"  {commodity}: {count} mines\")\n",
    "            \n",
    "            print(f\"\\nSUMMARY OF DROPPED MINES BY MINE TYPE:\")\n",
    "            mine_type_summary = dropped_info['MINE_TYPE1'].value_counts()\n",
    "            for mine_type, count in mine_type_summary.items():\n",
    "                print(f\"  {mine_type}: {count} mines\")\n",
    "        \n",
    "        # Remove rows with missing closure years\n",
    "        final_df = original_df[original_df['PROJ_CLOSURE_YR'].notna()]\n",
    "        rows_after_removal = len(final_df)\n",
    "        rows_removed = rows_before_removal - rows_after_removal\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        \n",
    "        # Save the final dataset\n",
    "        final_df.to_excel(output_file, index=False)\n",
    "        \n",
    "        # Final reporting\n",
    "        print(f\"\\n=== FINAL DATASET SUMMARY ===\")\n",
    "        print(f\"Original dataset rows: {rows_before_removal}\")\n",
    "        print(f\"Rows removed due to missing closure years: {rows_removed}\")\n",
    "        print(f\"Final dataset rows: {rows_after_removal}\")\n",
    "        print(f\"Data retention rate: {rows_after_removal/rows_before_removal*100:.1f}%\")\n",
    "        \n",
    "        # Show matching statistics\n",
    "        imputed_prop_ids = set(imputed_df['PROP_ID'].dropna())\n",
    "        original_prop_ids = set(original_df['PROP_ID'].dropna())\n",
    "        matching_prop_ids = imputed_prop_ids.intersection(original_prop_ids)\n",
    "        \n",
    "        print(f\"\\nMatching statistics:\")\n",
    "        print(f\"PROP_IDs in imputed dataset: {len(imputed_prop_ids)}\")\n",
    "        print(f\"PROP_IDs in original dataset: {len(original_prop_ids)}\")\n",
    "        print(f\"Matching PROP_IDs: {len(matching_prop_ids)}\")\n",
    "        print(f\"PROP_IDs with valid closure data transferred: {len(closure_lookup)}\")\n",
    "        \n",
    "        print(f\"\\nFinal dataset saved: {output_file}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during merge process: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_closure_years_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a734d7f",
   "metadata": {},
   "source": [
    "Preperation for CumProd\n",
    "1) If there are greater than or equal to 3 consecutive zeros after the last non-zero production value and the projected closure year is past 2023 then drop the mine\n",
    "2) If there are greater than or equal to 1 consecutive zeros after the last non-zero produciton value and the projected closure year is less than or equal to 2023, then change the projected closure year to the last year of non-zero production\n",
    "3) If there are greater than or equal to 4 consectuvie zeros and not a total of greater than or equal to 5 non-zero production values after the consecutive zeros then drop the mine. \n",
    "4) If there are greater than or equal to 5 non-zero prroduction values after the greater than or equal to 3 consecutive zeros then flag the mine and print its PROP_ID.\n",
    "5) Flag mines that are flagged in criteria 4 that have multiple greater than or equal to 3 consecutive zeroes\n",
    "\n",
    "For the flagged mines\n",
    "1) for the flagged mines of criteria 4, if the mine has greater than or equal to 1 consecutive zeros then change the start up year to the year of the first non-zero production. \n",
    "2) for the flagged mines of criteria 4, if the mine has greater than or equal to 5 consecutive zeros after the start up year then change the start up year to the year of the first non-zero production after the consecutvie zeros --> do this only for mines whose projected closure is greater than 2023, for the ones less than or equal to 2023, drop the mine. do this only after running the first criteria. \n",
    "2) leave the flagged mines of criteria 5 the same\n",
    "\n",
    "*for the flagged mines of criteria 5, I will edit it manually through visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "455aa5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset: Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\n",
      "Dataset loaded: 989 rows\n",
      "Found 44 year columns\n",
      "\n",
      "=== PROCESSING RESULTS ===\n",
      "\n",
      "Criteria 2 - Closure year changes: 8\n",
      "  PROP_ID 29477: Changed closure from 2019 to 2013 (10 trailing zeros)\n",
      "  PROP_ID 28245: Changed closure from 2022 to 2016 (7 trailing zeros)\n",
      "  PROP_ID 35734: Changed closure from 2021 to 2020 (3 trailing zeros)\n",
      "  PROP_ID 24474: Changed closure from 2022 to 2001 (22 trailing zeros)\n",
      "  PROP_ID 28264: Changed closure from 2004 to 2000 (20 trailing zeros)\n",
      "  PROP_ID 28866: Changed closure from 2022 to 2006 (17 trailing zeros)\n",
      "  PROP_ID 26968: Changed closure from 2023 to 2020 (3 trailing zeros)\n",
      "  PROP_ID 30189: Changed closure from 2022 to 2004 (19 trailing zeros)\n",
      "\n",
      "Criteria 4 - Flagged mines (≥5 non-zero values after ≥3 consecutive zeros): 135\n",
      "  PROP_ID 30897: Zero stretch 2002-2006 (length: 4), 13 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28811: Zero stretch 2000-2014 (length: 15), 9 non-zero values after, 1 total stretches\n",
      "  PROP_ID 81246: Zero stretch 2000-2015 (length: 16), 6 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27467: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 36915: Zero stretch 2000-2011 (length: 12), 12 non-zero values after, 1 total stretches\n",
      "  PROP_ID 35780: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30340: Zero stretch 2000-2014 (length: 15), 9 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30526: Zero stretch 2000-2007 (length: 8), 12 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28228: Zero stretch 2000-2003 (length: 4), 20 non-zero values after, 1 total stretches\n",
      "  PROP_ID 26908: Zero stretch 2000-2007 (length: 8), 16 non-zero values after, 1 total stretches\n",
      "  PROP_ID 80829: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28272: Zero stretch 2001-2003 (length: 3), 16 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30284: Zero stretch 2000-2014 (length: 15), 9 non-zero values after, 1 total stretches\n",
      "  PROP_ID 33381: Zero stretch 2000-2004 (length: 5), 19 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29312: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 33190: Zero stretch 2000-2002 (length: 3), 17 non-zero values after, 2 total stretches\n",
      "  PROP_ID 33413: Zero stretch 2000-2017 (length: 18), 6 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27163: Zero stretch 2014-2016 (length: 3), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28022: Zero stretch 2000-2007 (length: 8), 16 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29152: Zero stretch 2000-2012 (length: 13), 11 non-zero values after, 1 total stretches\n",
      "  PROP_ID 25700: Zero stretch 2000-2007 (length: 8), 15 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28268: Zero stretch 2000-2002 (length: 3), 19 non-zero values after, 1 total stretches\n",
      "  PROP_ID 81644: Zero stretch 2000-2016 (length: 17), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29357: Zero stretch 2000-2005 (length: 6), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 31323: Zero stretch 2000-2005 (length: 5), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 34468: Zero stretch 2000-2018 (length: 19), 5 non-zero values after, 1 total stretches\n",
      "  PROP_ID 33414: Zero stretch 2000-2014 (length: 15), 9 non-zero values after, 1 total stretches\n",
      "  PROP_ID 24673: Zero stretch 2001-2003 (length: 3), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 26105: Zero stretch 2000-2010 (length: 11), 12 non-zero values after, 1 total stretches\n",
      "  PROP_ID 32931: Zero stretch 2000-2005 (length: 6), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29744: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 37425: Zero stretch 2000-2008 (length: 9), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 35291: Zero stretch 2000-2005 (length: 4), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30531: Zero stretch 2000-2006 (length: 7), 17 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29285: Zero stretch 2000-2003 (length: 4), 20 non-zero values after, 1 total stretches\n",
      "  PROP_ID 33791: Zero stretch 2000-2010 (length: 11), 13 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28750: Zero stretch 2000-2004 (length: 5), 19 non-zero values after, 1 total stretches\n",
      "  PROP_ID 36399: Zero stretch 2000-2004 (length: 5), 19 non-zero values after, 1 total stretches\n",
      "  PROP_ID 78627: Zero stretch 2000-2017 (length: 18), 6 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29286: Zero stretch 2002-2015 (length: 14), 6 non-zero values after, 1 total stretches\n",
      "  PROP_ID 81205: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 80697: Zero stretch 2000-2005 (length: 6), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 35428: Zero stretch 2000-2008 (length: 9), 15 non-zero values after, 1 total stretches\n",
      "  PROP_ID 32917: Zero stretch 2000-2006 (length: 7), 15 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30555: Zero stretch 2000-2007 (length: 8), 16 non-zero values after, 1 total stretches\n",
      "  PROP_ID 26577: Zero stretch 2000-2003 (length: 4), 20 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28286: Zero stretch 2000-2005 (length: 6), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30656: Zero stretch 2000-2006 (length: 7), 9 non-zero values after, 2 total stretches\n",
      "  PROP_ID 25698: Zero stretch 2000-2014 (length: 15), 9 non-zero values after, 1 total stretches\n",
      "  PROP_ID 32856: Zero stretch 2000-2010 (length: 10), 13 non-zero values after, 1 total stretches\n",
      "  PROP_ID 80823: Zero stretch 2000-2017 (length: 16), 5 non-zero values after, 1 total stretches\n",
      "  PROP_ID 80801: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 36374: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27685: Zero stretch 2000-2007 (length: 8), 15 non-zero values after, 1 total stretches\n",
      "  PROP_ID 26668: Zero stretch 2000-2003 (length: 4), 20 non-zero values after, 1 total stretches\n",
      "  PROP_ID 33340: Zero stretch 2000-2006 (length: 7), 17 non-zero values after, 1 total stretches\n",
      "  PROP_ID 78612: Zero stretch 2000-2016 (length: 17), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27033: Zero stretch 2000-2004 (length: 4), 5 non-zero values after, 2 total stretches\n",
      "  PROP_ID 32142: Zero stretch 2000-2006 (length: 7), 13 non-zero values after, 1 total stretches\n",
      "  PROP_ID 61027: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27804: Zero stretch 2000-2014 (length: 15), 9 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29461: Zero stretch 2000-2008 (length: 9), 15 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27352: Zero stretch 2000-2005 (length: 6), 17 non-zero values after, 1 total stretches\n",
      "  PROP_ID 38217: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28837: Zero stretch 2000-2011 (length: 12), 12 non-zero values after, 1 total stretches\n",
      "  PROP_ID 32169: Zero stretch 2000-2005 (length: 6), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 26477: Zero stretch 2000-2003 (length: 4), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27674: Zero stretch 2002-2008 (length: 7), 15 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27392: Zero stretch 2016-2018 (length: 3), 5 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30254: Zero stretch 2000-2012 (length: 13), 11 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29815: Zero stretch 2000-2005 (length: 6), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29377: Zero stretch 2000-2013 (length: 11), 10 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30678: Zero stretch 2000-2017 (length: 18), 6 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28704: Zero stretch 2000-2012 (length: 13), 11 non-zero values after, 1 total stretches\n",
      "  PROP_ID 31300: Zero stretch 2000-2018 (length: 19), 5 non-zero values after, 1 total stretches\n",
      "  PROP_ID 26984: Zero stretch 2000-2012 (length: 13), 11 non-zero values after, 1 total stretches\n",
      "  PROP_ID 26823: Zero stretch 2002-2004 (length: 3), 17 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30969: Zero stretch 2000-2011 (length: 12), 12 non-zero values after, 1 total stretches\n",
      "  PROP_ID 38654: Zero stretch 2000-2009 (length: 9), 13 non-zero values after, 1 total stretches\n",
      "  PROP_ID 33810: Zero stretch 2000-2016 (length: 17), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 66154: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 31528: Zero stretch 2000-2012 (length: 13), 11 non-zero values after, 1 total stretches\n",
      "  PROP_ID 33208: Zero stretch 2000-2007 (length: 8), 16 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28240: Zero stretch 2000-2005 (length: 6), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 32233: Zero stretch 2000-2008 (length: 9), 15 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30469: Zero stretch 2000-2014 (length: 15), 9 non-zero values after, 1 total stretches\n",
      "  PROP_ID 35707: Zero stretch 2000-2016 (length: 17), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 24697: Zero stretch 2000-2018 (length: 19), 5 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28908: Zero stretch 2000-2014 (length: 15), 9 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27653: Zero stretch 2000-2003 (length: 4), 20 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29830: Zero stretch 2000-2005 (length: 6), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27280: Zero stretch 2000-2006 (length: 7), 17 non-zero values after, 1 total stretches\n",
      "  PROP_ID 25824: Zero stretch 2000-2002 (length: 3), 21 non-zero values after, 1 total stretches\n",
      "  PROP_ID 25761: Zero stretch 2000-2011 (length: 12), 12 non-zero values after, 1 total stretches\n",
      "  PROP_ID 37670: Zero stretch 2000-2016 (length: 17), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 81197: Zero stretch 2000-2015 (length: 16), 6 non-zero values after, 1 total stretches\n",
      "  PROP_ID 81649: Zero stretch 2000-2016 (length: 17), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 58786: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28650: Zero stretch 2000-2013 (length: 14), 10 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30665: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 36775: Zero stretch 2000-2004 (length: 5), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 31223: Zero stretch 2000-2003 (length: 4), 20 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30025: Zero stretch 2000-2005 (length: 6), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 32354: Zero stretch 2000-2003 (length: 4), 19 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28249: Zero stretch 2000-2005 (length: 6), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 81647: Zero stretch 2000-2016 (length: 17), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 35321: Zero stretch 2004-2006 (length: 3), 17 non-zero values after, 1 total stretches\n",
      "  PROP_ID 81642: Zero stretch 2000-2016 (length: 17), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28371: Zero stretch 2000-2008 (length: 9), 15 non-zero values after, 1 total stretches\n",
      "  PROP_ID 62900: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 80699: Zero stretch 2000-2005 (length: 6), 18 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29739: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 30654: Zero stretch 2000-2011 (length: 12), 12 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28680: Zero stretch 2000-2013 (length: 14), 10 non-zero values after, 1 total stretches\n",
      "  PROP_ID 35185: Zero stretch 2000-2009 (length: 10), 14 non-zero values after, 1 total stretches\n",
      "  PROP_ID 36127: Zero stretch 2000-2014 (length: 15), 9 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29544: Zero stretch 2000-2004 (length: 5), 19 non-zero values after, 1 total stretches\n",
      "  PROP_ID 28953: Zero stretch 2000-2006 (length: 7), 7 non-zero values after, 2 total stretches\n",
      "  PROP_ID 35851: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27065: Zero stretch 2000-2008 (length: 9), 11 non-zero values after, 2 total stretches\n",
      "  PROP_ID 29742: Zero stretch 2000-2008 (length: 9), 15 non-zero values after, 1 total stretches\n",
      "  PROP_ID 81199: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 36137: Zero stretch 2000-2016 (length: 17), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 40771: Zero stretch 2000-2006 (length: 7), 16 non-zero values after, 1 total stretches\n",
      "  PROP_ID 33964: Zero stretch 2000-2016 (length: 17), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 27818: Zero stretch 2000-2015 (length: 16), 8 non-zero values after, 1 total stretches\n",
      "  PROP_ID 35755: Zero stretch 2000-2002 (length: 3), 21 non-zero values after, 1 total stretches\n",
      "  PROP_ID 29743: Zero stretch 2000-2009 (length: 10), 14 non-zero values after, 1 total stretches\n",
      "  PROP_ID 81511: Zero stretch 2000-2016 (length: 17), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 59350: Zero stretch 2000-2002 (length: 3), 16 non-zero values after, 1 total stretches\n",
      "  PROP_ID 76485: Zero stretch 2000-2013 (length: 14), 10 non-zero values after, 1 total stretches\n",
      "  PROP_ID 36877: Zero stretch 2005-2007 (length: 3), 12 non-zero values after, 1 total stretches\n",
      "  PROP_ID 36071: Zero stretch 2000-2013 (length: 6), 10 non-zero values after, 1 total stretches\n",
      "  PROP_ID 80402: Zero stretch 2000-2015 (length: 16), 7 non-zero values after, 1 total stretches\n",
      "  PROP_ID 80288: Zero stretch 2000-2015 (length: 16), 7 non-zero values after, 1 total stretches\n",
      "\n",
      "Criteria 5 - Multi-gap flagged mines (flagged in criteria 4 with multiple ≥3 zero stretches): 5\n",
      "  PROP_ID 33190: 2 zero stretches\n",
      "    - 2000-2002 (length: 3)\n",
      "    - 2007-2009 (length: 3)\n",
      "  PROP_ID 30656: 2 zero stretches\n",
      "    - 2000-2006 (length: 7)\n",
      "    - 2008-2015 (length: 8)\n",
      "  PROP_ID 27033: 2 zero stretches\n",
      "    - 2000-2004 (length: 4)\n",
      "    - 2009-2021 (length: 13)\n",
      "  PROP_ID 28953: 2 zero stretches\n",
      "    - 2000-2006 (length: 7)\n",
      "    - 2010-2014 (length: 5)\n",
      "  PROP_ID 27065: 2 zero stretches\n",
      "    - 2000-2008 (length: 9)\n",
      "    - 2010-2013 (length: 4)\n",
      "\n",
      "Mines to be dropped: 34\n",
      "  Criteria 1 (≥2 zeros after last production, closure > 2023): 31\n",
      "  Criteria 3 (≥4 consecutive zeros, insufficient restart): 3\n",
      "\n",
      "Examples of dropped mines:\n",
      "  26483: Criteria 1: ≥2 zeros after last production, closure > 2023\n",
      "  25669: Criteria 1: ≥2 zeros after last production, closure > 2023\n",
      "  28765: Criteria 1: ≥2 zeros after last production, closure > 2023\n",
      "  28756: Criteria 1: ≥2 zeros after last production, closure > 2023\n",
      "  27589: Criteria 1: ≥2 zeros after last production, closure > 2023\n",
      "  29979: Criteria 1: ≥2 zeros after last production, closure > 2023\n",
      "  28032: Criteria 1: ≥2 zeros after last production, closure > 2023\n",
      "  26525: Criteria 1: ≥2 zeros after last production, closure > 2023\n",
      "  33083: Criteria 1: ≥2 zeros after last production, closure > 2023\n",
      "  29432: Criteria 1: ≥2 zeros after last production, closure > 2023\n",
      "  ... and 24 more\n",
      "\n",
      "=== FINAL SUMMARY ===\n",
      "Initial rows: 989\n",
      "Rows dropped: 34\n",
      "Final rows: 955\n",
      "Data retention rate: 96.6%\n",
      "Closure year changes made: 8\n",
      "Mines flagged for review (Criteria 4): 135\n",
      "Multi-gap flagged mines (Criteria 5): 5\n",
      "\n",
      "Edited dataset saved: Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def edit_production_dataset():\n",
    "    \"\"\"\n",
    "    Edits the production dataset based on consecutive zero patterns and closure years.\n",
    "    \n",
    "    Criteria:\n",
    "    1) ≥2 consecutive zeros after last production AND closure > 2023 → Drop mine\n",
    "    2) ≥1 consecutive zeros after last production AND closure ≤ 2023 → Change closure to last production year\n",
    "    3) ≥4 consecutive zeros AND not ≥5 non-zero values after zeros → Drop mine\n",
    "    4) ≥5 non-zero values after ≥3 consecutive zeros → Flag mine\n",
    "    5) Flag mines that are flagged in criteria 4 that have multiple ≥3 consecutive zero stretches\n",
    "    \"\"\"\n",
    "    # Define file path\n",
    "    input_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "    output_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "    \n",
    "    print(f\"Reading dataset: {input_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the dataset\n",
    "        df = pd.read_excel(input_file)\n",
    "        print(f\"Dataset loaded: {len(df)} rows\")\n",
    "        \n",
    "        # Check required columns\n",
    "        if 'PROP_ID' not in df.columns:\n",
    "            print(\"Error: PROP_ID column not found\")\n",
    "            return\n",
    "        if 'PROJ_CLOSURE_YR' not in df.columns:\n",
    "            print(\"Error: PROJ_CLOSURE_YR column not found\")\n",
    "            return\n",
    "        \n",
    "        # Get year columns (1980-2023)\n",
    "        year_columns = [str(year) for year in range(1980, 2024)]\n",
    "        existing_year_columns = [col for col in year_columns if col in df.columns]\n",
    "        existing_year_columns.sort(key=int)  # Ensure chronological order\n",
    "        \n",
    "        print(f\"Found {len(existing_year_columns)} year columns\")\n",
    "        \n",
    "        # Convert year columns to numeric\n",
    "        for col in existing_year_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Convert closure year to numeric\n",
    "        df['PROJ_CLOSURE_YR'] = pd.to_numeric(df['PROJ_CLOSURE_YR'], errors='coerce')\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        initial_rows = len(df)\n",
    "        mines_to_drop = []\n",
    "        closure_changes = []\n",
    "        flagged_mines = []\n",
    "        multi_gap_flagged_mines = []\n",
    "        \n",
    "        # Process each mine\n",
    "        for idx, row in df.iterrows():\n",
    "            prop_id = row['PROP_ID']\n",
    "            closure_year = row['PROJ_CLOSURE_YR']\n",
    "            \n",
    "            # Get production values for all years, keeping NaN as NaN\n",
    "            production_values = []\n",
    "            for year_col in existing_year_columns:\n",
    "                value = row[year_col]\n",
    "                production_values.append(value)  # Keep NaN as NaN, don't convert to 0\n",
    "            \n",
    "            # Find last non-zero production year (value > 0, not NaN)\n",
    "            last_nonzero_idx = None\n",
    "            for i in range(len(production_values) - 1, -1, -1):\n",
    "                value = production_values[i]\n",
    "                if not pd.isna(value) and value > 0:\n",
    "                    last_nonzero_idx = i\n",
    "                    break\n",
    "            \n",
    "            if last_nonzero_idx is None:\n",
    "                # No non-zero production found, skip this mine\n",
    "                continue\n",
    "            \n",
    "            last_nonzero_year = int(existing_year_columns[last_nonzero_idx])\n",
    "            \n",
    "            # Count consecutive ACTUAL zeros (not NaN) after last non-zero production\n",
    "            consecutive_zeros_after = 0\n",
    "            for i in range(last_nonzero_idx + 1, len(production_values)):\n",
    "                value = production_values[i]\n",
    "                if not pd.isna(value) and value == 0:  # Only count actual zeros\n",
    "                    consecutive_zeros_after += 1\n",
    "                elif not pd.isna(value) and value > 0:  # Stop if we hit non-zero production\n",
    "                    break\n",
    "                # If it's NaN, we skip it (don't count as zero, don't break the streak)\n",
    "            \n",
    "            # Check criteria 1 and 2 based on consecutive zeros after last production\n",
    "            if pd.notna(closure_year):\n",
    "                if closure_year > 2023 and consecutive_zeros_after >= 2:\n",
    "                    # Criteria 1: ≥2 consecutive zeros after last production AND closure > 2023 → Drop mine\n",
    "                    mines_to_drop.append({\n",
    "                        'idx': idx,\n",
    "                        'prop_id': prop_id,\n",
    "                        'reason': 'Criteria 1: ≥2 zeros after last production, closure > 2023',\n",
    "                        'consecutive_zeros': consecutive_zeros_after,\n",
    "                        'last_production_year': last_nonzero_year,\n",
    "                        'closure_year': closure_year\n",
    "                    })\n",
    "                    continue\n",
    "                elif closure_year <= 2023 and consecutive_zeros_after >= 1:\n",
    "                    # Criteria 2: ≥1 consecutive zeros after last production AND closure ≤ 2023 → Change closure to last production year\n",
    "                    if closure_year != last_nonzero_year:\n",
    "                        closure_changes.append({\n",
    "                            'prop_id': prop_id,\n",
    "                            'old_closure': closure_year,\n",
    "                            'new_closure': last_nonzero_year,\n",
    "                            'consecutive_zeros': consecutive_zeros_after\n",
    "                        })\n",
    "                        df.at[idx, 'PROJ_CLOSURE_YR'] = last_nonzero_year\n",
    "                    continue\n",
    "            \n",
    "            # Find all stretches of ≥3 consecutive ACTUAL zeros in the entire timeline\n",
    "            zero_stretches = []\n",
    "            current_zeros = 0\n",
    "            stretch_start = None\n",
    "            \n",
    "            for i in range(len(production_values)):\n",
    "                value = production_values[i]\n",
    "                if not pd.isna(value) and value == 0:  # Only count actual zeros\n",
    "                    if current_zeros == 0:\n",
    "                        stretch_start = i\n",
    "                    current_zeros += 1\n",
    "                elif not pd.isna(value) and value > 0:  # Hit non-zero production\n",
    "                    if current_zeros >= 3 and stretch_start is not None:\n",
    "                        # Record this zero stretch\n",
    "                        zero_stretches.append({\n",
    "                            'start': stretch_start,\n",
    "                            'end': i - 1,\n",
    "                            'length': current_zeros\n",
    "                        })\n",
    "                    current_zeros = 0\n",
    "                    stretch_start = None\n",
    "                # If NaN, we skip it (don't count as zero, don't reset counter)\n",
    "            \n",
    "            # Check for a zero stretch that extends to the end\n",
    "            if current_zeros >= 3 and stretch_start is not None:\n",
    "                zero_stretches.append({\n",
    "                    'start': stretch_start,\n",
    "                    'end': len(production_values) - 1,\n",
    "                    'length': current_zeros\n",
    "                })\n",
    "            \n",
    "            # Process zero stretches for criteria 3 and 4\n",
    "            mine_flagged_criteria_4 = False\n",
    "            \n",
    "            for stretch in zero_stretches:\n",
    "                if stretch['length'] >= 3:\n",
    "                    # Count NON-ZERO values (> 0, not NaN) after this zero stretch\n",
    "                    nonzero_after_stretch = 0\n",
    "                    \n",
    "                    for i in range(stretch['end'] + 1, len(production_values)):\n",
    "                        value = production_values[i]\n",
    "                        if not pd.isna(value) and value > 0:  # Only count actual non-zero values\n",
    "                            nonzero_after_stretch += 1\n",
    "                    \n",
    "                    if nonzero_after_stretch >= 5:\n",
    "                        # Criteria 4: Flag mine\n",
    "                        if not mine_flagged_criteria_4:  # Only add once per mine\n",
    "                            flagged_mines.append({\n",
    "                                'prop_id': prop_id,\n",
    "                                'zero_stretch_start_year': int(existing_year_columns[stretch['start']]),\n",
    "                                'zero_stretch_end_year': int(existing_year_columns[stretch['end']]),\n",
    "                                'zero_stretch_length': stretch['length'],\n",
    "                                'nonzero_after_stretch': nonzero_after_stretch,\n",
    "                                'total_zero_stretches': len([s for s in zero_stretches if s['length'] >= 3])\n",
    "                            })\n",
    "                            mine_flagged_criteria_4 = True\n",
    "                    elif stretch['length'] >= 4:  # Only apply criteria 3 if stretch is ≥4 years\n",
    "                        # Criteria 3: Drop mine (only for ≥4 consecutive zeros)\n",
    "                        mines_to_drop.append({\n",
    "                            'idx': idx,\n",
    "                            'prop_id': prop_id,\n",
    "                            'reason': f'Criteria 3: ≥4 consecutive zeros, only {nonzero_after_stretch} non-zero values after',\n",
    "                            'zero_stretch_start_year': int(existing_year_columns[stretch['start']]),\n",
    "                            'zero_stretch_end_year': int(existing_year_columns[stretch['end']]),\n",
    "                            'zero_stretch_length': stretch['length'],\n",
    "                            'nonzero_after_stretch': nonzero_after_stretch\n",
    "                        })\n",
    "                        break  # Don't process other stretches for this mine since we're dropping it\n",
    "            \n",
    "            # Criteria 5: Check if mine flagged in criteria 4 has multiple ≥3 zero stretches\n",
    "            if mine_flagged_criteria_4:\n",
    "                multiple_stretches_count = len([s for s in zero_stretches if s['length'] >= 3])\n",
    "                if multiple_stretches_count > 1:\n",
    "                    multi_gap_flagged_mines.append({\n",
    "                        'prop_id': prop_id,\n",
    "                        'total_zero_stretches': multiple_stretches_count,\n",
    "                        'stretches_details': [{\n",
    "                            'start_year': int(existing_year_columns[s['start']]),\n",
    "                            'end_year': int(existing_year_columns[s['end']]),\n",
    "                            'length': s['length']\n",
    "                        } for s in zero_stretches if s['length'] >= 3]\n",
    "                    })\n",
    "        \n",
    "        # Apply changes\n",
    "        print(f\"\\n=== PROCESSING RESULTS ===\")\n",
    "        \n",
    "        # Report closure year changes\n",
    "        if closure_changes:\n",
    "            print(f\"\\nCriteria 2 - Closure year changes: {len(closure_changes)}\")\n",
    "            for change in closure_changes:\n",
    "                print(f\"  PROP_ID {change['prop_id']}: Changed closure from {change['old_closure']} to {change['new_closure']} ({change['consecutive_zeros']} trailing zeros)\")\n",
    "        \n",
    "        # Report flagged mines\n",
    "        if flagged_mines:\n",
    "            print(f\"\\nCriteria 4 - Flagged mines (≥5 non-zero values after ≥3 consecutive zeros): {len(flagged_mines)}\")\n",
    "            for flag in flagged_mines:\n",
    "                print(f\"  PROP_ID {flag['prop_id']}: Zero stretch {flag['zero_stretch_start_year']}-{flag['zero_stretch_end_year']} (length: {flag['zero_stretch_length']}), {flag['nonzero_after_stretch']} non-zero values after, {flag['total_zero_stretches']} total stretches\")\n",
    "        \n",
    "        # Report multi-gap flagged mines\n",
    "        if multi_gap_flagged_mines:\n",
    "            print(f\"\\nCriteria 5 - Multi-gap flagged mines (flagged in criteria 4 with multiple ≥3 zero stretches): {len(multi_gap_flagged_mines)}\")\n",
    "            for multi_flag in multi_gap_flagged_mines:\n",
    "                print(f\"  PROP_ID {multi_flag['prop_id']}: {multi_flag['total_zero_stretches']} zero stretches\")\n",
    "                for stretch in multi_flag['stretches_details']:\n",
    "                    print(f\"    - {stretch['start_year']}-{stretch['end_year']} (length: {stretch['length']})\")\n",
    "        else:\n",
    "            print(f\"\\nCriteria 5 - Multi-gap flagged mines: 0\")\n",
    "        \n",
    "        # Drop mines\n",
    "        if mines_to_drop:\n",
    "            print(f\"\\nMines to be dropped: {len(mines_to_drop)}\")\n",
    "            \n",
    "            # Group by reason\n",
    "            criteria_1_count = len([m for m in mines_to_drop if 'Criteria 1' in m['reason']])\n",
    "            criteria_3_count = len([m for m in mines_to_drop if 'Criteria 3' in m['reason']])\n",
    "            \n",
    "            print(f\"  Criteria 1 (≥2 zeros after last production, closure > 2023): {criteria_1_count}\")\n",
    "            print(f\"  Criteria 3 (≥4 consecutive zeros, insufficient restart): {criteria_3_count}\")\n",
    "            \n",
    "            # Show some examples\n",
    "            print(f\"\\nExamples of dropped mines:\")\n",
    "            for i, mine in enumerate(mines_to_drop[:10]):  # Show first 10\n",
    "                print(f\"  {mine['prop_id']}: {mine['reason']}\")\n",
    "            if len(mines_to_drop) > 10:\n",
    "                print(f\"  ... and {len(mines_to_drop) - 10} more\")\n",
    "            \n",
    "            # Drop the mines\n",
    "            indices_to_drop = [mine['idx'] for mine in mines_to_drop]\n",
    "            df = df.drop(indices_to_drop).reset_index(drop=True)\n",
    "        \n",
    "        # Final statistics\n",
    "        final_rows = len(df)\n",
    "        rows_dropped = initial_rows - final_rows\n",
    "        \n",
    "        print(f\"\\n=== FINAL SUMMARY ===\")\n",
    "        print(f\"Initial rows: {initial_rows}\")\n",
    "        print(f\"Rows dropped: {rows_dropped}\")\n",
    "        print(f\"Final rows: {final_rows}\")\n",
    "        print(f\"Data retention rate: {final_rows/initial_rows*100:.1f}%\")\n",
    "        print(f\"Closure year changes made: {len(closure_changes)}\")\n",
    "        print(f\"Mines flagged for review (Criteria 4): {len(flagged_mines)}\")\n",
    "        print(f\"Multi-gap flagged mines (Criteria 5): {len(multi_gap_flagged_mines)}\")\n",
    "        \n",
    "        # Save the edited dataset\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"\\nEdited dataset saved: {output_file}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    edit_production_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4490ee3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset: Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\n",
      "Dataset loaded: 955 rows\n",
      "Found 44 year columns\n",
      "\n",
      "Re-identifying flagged mines...\n",
      "Identified 131 mines flagged in criteria 4\n",
      "Identified 6 mines flagged in criteria 5\n",
      "\n",
      "Step 1: Processing criteria 4 mines for startup year corrections (≥1 consecutive zeros)...\n",
      "\n",
      "Step 2: Processing criteria 4 mines for ≥5 consecutive zeros after startup year...\n",
      "Dropped 1 mines with ≥5 zeros after startup and closure ≤ 2023\n",
      "\n",
      "=== PROCESSING RESULTS ===\n",
      "\n",
      "Step 1 - Criteria 4 mines processed: 131\n",
      "Step 1 - Startup year changes made: 24\n",
      "\n",
      "Step 1 startup year corrections:\n",
      "  PROP_ID 30897: Changed startup from 2001 to 2007 (1 zero stretches)\n",
      "  PROP_ID 27467: Changed startup from 1990 to 2016 (1 zero stretches)\n",
      "  PROP_ID 80829: Changed startup from 1950 to 2016 (1 zero stretches)\n",
      "  PROP_ID 27163: Changed startup from 1969 to 1992 (1 zero stretches)\n",
      "  PROP_ID 31323: Changed startup from 2004 to 2006 (1 zero stretches)\n",
      "  PROP_ID 35291: Changed startup from 2004 to 2006 (1 zero stretches)\n",
      "  PROP_ID 28750: Changed startup from 1981 to 2005 (1 zero stretches)\n",
      "  PROP_ID 29286: Changed startup from 1971 to 1999 (1 zero stretches)\n",
      "  PROP_ID 81205: Changed startup from 1966 to 2016 (1 zero stretches)\n",
      "  PROP_ID 80697: Changed startup from 1972 to 2006 (1 zero stretches)\n",
      "  PROP_ID 32856: Changed startup from 2010 to 2011 (1 zero stretches)\n",
      "  PROP_ID 30666: Changed startup from 1976 to 2016 (1 zero stretches)\n",
      "  PROP_ID 38217: Changed startup from 1998 to 2016 (1 zero stretches)\n",
      "  PROP_ID 27392: Changed startup from 1991 to 1992 (1 zero stretches)\n",
      "  PROP_ID 36775: Changed startup from 1972 to 2005 (1 zero stretches)\n",
      "  PROP_ID 62900: Changed startup from 1989 to 2016 (1 zero stretches)\n",
      "  PROP_ID 80699: Changed startup from 1971 to 2006 (1 zero stretches)\n",
      "  PROP_ID 29739: Changed startup from 1956 to 2016 (1 zero stretches)\n",
      "  PROP_ID 30654: Changed startup from 1984 to 2012 (1 zero stretches)\n",
      "  PROP_ID 27818: Changed startup from 1991 to 2016 (1 zero stretches)\n",
      "  PROP_ID 35755: Changed startup from 1958 to 2003 (1 zero stretches)\n",
      "  PROP_ID 59350: Changed startup from 1942 to 2003 (1 zero stretches)\n",
      "  PROP_ID 76485: Changed startup from 1984 to 2014 (1 zero stretches)\n",
      "  PROP_ID 36071: Changed startup from 2013 to 2014 (1 zero stretches)\n",
      "\n",
      "Step 2 - Mines with ≥5 zeros after startup: 8\n",
      "Step 2 - Startup year changes (closure > 2023): 7\n",
      "Step 2 - Mines dropped (closure ≤ 2023): 1\n",
      "\n",
      "Step 2 startup year corrections (≥5 zeros after startup, closure > 2023):\n",
      "  PROP_ID 26908: Changed startup from 1986 to 2008 (8 zeros, closure: 2033)\n",
      "  PROP_ID 26105: Changed startup from 1984 to 2011 (11 zeros, closure: 2045)\n",
      "  PROP_ID 27685: Changed startup from 1990 to 2008 (8 zeros, closure: 2040)\n",
      "  PROP_ID 27674: Changed startup from 1999 to 2009 (7 zeros, closure: 2026)\n",
      "  PROP_ID 29377: Changed startup from 1996 to 2014 (14 zeros, closure: 2041)\n",
      "  PROP_ID 30469: Changed startup from 1996 to 2015 (15 zeros, closure: 2038)\n",
      "  PROP_ID 28680: Changed startup from 1996 to 2014 (14 zeros, closure: 2051)\n",
      "\n",
      "Step 2 mines dropped (≥5 zeros after startup, closure ≤ 2023):\n",
      "  PROP_ID 29286: 14 zeros (2002-2015), closure: 2023\n",
      "\n",
      "Criteria 5 mines: 6 (left unchanged)\n",
      "Criteria 5 mines (multiple gaps, no changes made):\n",
      "  PROP_ID 80820: 2 zero stretches\n",
      "  PROP_ID 33190: 2 zero stretches\n",
      "  PROP_ID 29477: 2 zero stretches\n",
      "  PROP_ID 30656: 2 zero stretches\n",
      "  PROP_ID 35734: 2 zero stretches\n",
      "  PROP_ID 27065: 2 zero stretches\n",
      "\n",
      "=== FINAL SUMMARY ===\n",
      "Total criteria 4 mines: 131\n",
      "Step 1 startup years corrected: 24\n",
      "Step 2 startup years corrected: 7\n",
      "Step 2 mines dropped: 1\n",
      "Criteria 5 mines (unchanged): 6\n",
      "Final dataset rows: 954\n",
      "Modified dataset saved: Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def modify_flagged_mines_startup():\n",
    "    \"\"\"\n",
    "    Modifies startup years for mines flagged in criteria 4 based on consecutive zero patterns.\n",
    "    \n",
    "    Logic:\n",
    "    1) For flagged mines in criteria 4: if mine has ≥1 consecutive zeros, change startup year to first non-zero production\n",
    "    2) For flagged mines in criteria 4: if mine has ≥5 consecutive zeros after startup year:\n",
    "       - If closure > 2023: change startup year to first non-zero production AFTER the consecutive zeros\n",
    "       - If closure ≤ 2023: drop the mine\n",
    "    3) Leave flagged mines in criteria 5 unchanged\n",
    "    \"\"\"\n",
    "    # Define file path\n",
    "    input_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "    output_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "    \n",
    "    print(f\"Reading dataset: {input_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the dataset\n",
    "        df = pd.read_excel(input_file)\n",
    "        print(f\"Dataset loaded: {len(df)} rows\")\n",
    "        \n",
    "        # Check required columns\n",
    "        if 'PROP_ID' not in df.columns:\n",
    "            print(\"Error: PROP_ID column not found\")\n",
    "            return\n",
    "        if 'START_UP_YR' not in df.columns:\n",
    "            print(\"Error: START_UP_YR column not found\")\n",
    "            return\n",
    "        \n",
    "        # Get year columns (1980-2023)\n",
    "        year_columns = [str(year) for year in range(1980, 2024)]\n",
    "        existing_year_columns = [col for col in year_columns if col in df.columns]\n",
    "        existing_year_columns.sort(key=int)  # Ensure chronological order\n",
    "        \n",
    "        print(f\"Found {len(existing_year_columns)} year columns\")\n",
    "        \n",
    "        # Convert year columns to numeric\n",
    "        for col in existing_year_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Convert startup year and closure year to numeric\n",
    "        df['START_UP_YR'] = pd.to_numeric(df['START_UP_YR'], errors='coerce')\n",
    "        df['PROJ_CLOSURE_YR'] = pd.to_numeric(df['PROJ_CLOSURE_YR'], errors='coerce')\n",
    "        \n",
    "        # Re-identify flagged mines (criteria 4 and 5)\n",
    "        print(\"\\nRe-identifying flagged mines...\")\n",
    "        \n",
    "        criteria_4_mines = []\n",
    "        criteria_5_mines = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            prop_id = row['PROP_ID']\n",
    "            \n",
    "            # Get production values for all years, keeping NaN as NaN\n",
    "            production_values = []\n",
    "            for year_col in existing_year_columns:\n",
    "                value = row[year_col]\n",
    "                production_values.append(value)\n",
    "            \n",
    "            # Find all stretches of ≥3 consecutive ACTUAL zeros\n",
    "            zero_stretches = []\n",
    "            current_zeros = 0\n",
    "            stretch_start = None\n",
    "            \n",
    "            for i in range(len(production_values)):\n",
    "                value = production_values[i]\n",
    "                if not pd.isna(value) and value == 0:  # Only count actual zeros\n",
    "                    if current_zeros == 0:\n",
    "                        stretch_start = i\n",
    "                    current_zeros += 1\n",
    "                elif not pd.isna(value) and value > 0:  # Hit non-zero production\n",
    "                    if current_zeros >= 3 and stretch_start is not None:\n",
    "                        # Record this zero stretch\n",
    "                        zero_stretches.append({\n",
    "                            'start': stretch_start,\n",
    "                            'end': i - 1,\n",
    "                            'length': current_zeros\n",
    "                        })\n",
    "                    current_zeros = 0\n",
    "                    stretch_start = None\n",
    "            \n",
    "            # Check for a zero stretch that extends to the end\n",
    "            if current_zeros >= 3 and stretch_start is not None:\n",
    "                zero_stretches.append({\n",
    "                    'start': stretch_start,\n",
    "                    'end': len(production_values) - 1,\n",
    "                    'length': current_zeros\n",
    "                })\n",
    "            \n",
    "            # Check if mine qualifies for criteria 4 (≥5 non-zero values after ≥3 consecutive zeros)\n",
    "            mine_flagged_criteria_4 = False\n",
    "            \n",
    "            for stretch in zero_stretches:\n",
    "                if stretch['length'] >= 3:\n",
    "                    # Count NON-ZERO values after this zero stretch\n",
    "                    nonzero_after_stretch = 0\n",
    "                    \n",
    "                    for i in range(stretch['end'] + 1, len(production_values)):\n",
    "                        value = production_values[i]\n",
    "                        if not pd.isna(value) and value > 0:\n",
    "                            nonzero_after_stretch += 1\n",
    "                    \n",
    "                    if nonzero_after_stretch >= 5:\n",
    "                        mine_flagged_criteria_4 = True\n",
    "                        break\n",
    "            \n",
    "            # Add to appropriate list\n",
    "            if mine_flagged_criteria_4:\n",
    "                multiple_stretches_count = len([s for s in zero_stretches if s['length'] >= 3])\n",
    "                \n",
    "                if multiple_stretches_count > 1:\n",
    "                    # Criteria 5: Multiple zero stretches\n",
    "                    criteria_5_mines.append({\n",
    "                        'idx': idx,\n",
    "                        'prop_id': prop_id,\n",
    "                        'zero_stretches': zero_stretches,\n",
    "                        'has_multiple_gaps': True\n",
    "                    })\n",
    "                else:\n",
    "                    # Criteria 4: Single zero stretch with restart\n",
    "                    criteria_4_mines.append({\n",
    "                        'idx': idx,\n",
    "                        'prop_id': prop_id,\n",
    "                        'zero_stretches': zero_stretches,\n",
    "                        'has_multiple_gaps': False\n",
    "                    })\n",
    "        \n",
    "        print(f\"Identified {len(criteria_4_mines)} mines flagged in criteria 4\")\n",
    "        print(f\"Identified {len(criteria_5_mines)} mines flagged in criteria 5\")\n",
    "        \n",
    "        # Process criteria 4 mines - modify startup years (Step 1)\n",
    "        startup_changes_step1 = []\n",
    "        \n",
    "        print(f\"\\nStep 1: Processing criteria 4 mines for startup year corrections (≥1 consecutive zeros)...\")\n",
    "        \n",
    "        for mine in criteria_4_mines:\n",
    "            idx = mine['idx']\n",
    "            prop_id = mine['prop_id']\n",
    "            row = df.loc[idx]\n",
    "            current_startup = row['START_UP_YR']\n",
    "            \n",
    "            # Get production values\n",
    "            production_values = []\n",
    "            for year_col in existing_year_columns:\n",
    "                value = row[year_col]\n",
    "                production_values.append(value)\n",
    "            \n",
    "            # Check if mine has ≥1 consecutive zeros\n",
    "            has_zero_stretch = any(stretch['length'] >= 1 for stretch in mine['zero_stretches'])\n",
    "            \n",
    "            if has_zero_stretch:\n",
    "                # Find first year with non-zero production\n",
    "                first_production_year = None\n",
    "                for i, year_col in enumerate(existing_year_columns):\n",
    "                    value = production_values[i]\n",
    "                    if not pd.isna(value) and value > 0:\n",
    "                        first_production_year = int(year_col)\n",
    "                        break\n",
    "                \n",
    "                if first_production_year and (pd.isna(current_startup) or first_production_year != current_startup):\n",
    "                    # Update startup year\n",
    "                    df.at[idx, 'START_UP_YR'] = first_production_year\n",
    "                    startup_changes_step1.append({\n",
    "                        'prop_id': prop_id,\n",
    "                        'old_startup': current_startup if not pd.isna(current_startup) else 'Empty',\n",
    "                        'new_startup': first_production_year,\n",
    "                        'zero_stretches_count': len(mine['zero_stretches'])\n",
    "                    })\n",
    "        \n",
    "        # Process criteria 4 mines - Step 2: Handle ≥5 consecutive zeros after startup year\n",
    "        startup_changes_step2 = []\n",
    "        mines_to_drop = []\n",
    "        \n",
    "        print(f\"\\nStep 2: Processing criteria 4 mines for ≥5 consecutive zeros after startup year...\")\n",
    "        \n",
    "        for mine in criteria_4_mines:\n",
    "            idx = mine['idx']\n",
    "            prop_id = mine['prop_id']\n",
    "            row = df.loc[idx]\n",
    "            current_startup = row['START_UP_YR']  # This might have been updated in Step 1\n",
    "            closure_year = row['PROJ_CLOSURE_YR']\n",
    "            \n",
    "            # Skip if no valid startup or closure year\n",
    "            if pd.isna(current_startup) or pd.isna(closure_year):\n",
    "                continue\n",
    "                \n",
    "            current_startup = int(current_startup)\n",
    "            closure_year = int(closure_year)\n",
    "            \n",
    "            # Get production values\n",
    "            production_values = []\n",
    "            for year_col in existing_year_columns:\n",
    "                value = row[year_col]\n",
    "                production_values.append(value)\n",
    "            \n",
    "            # Find startup year index\n",
    "            startup_idx = None\n",
    "            for i, year_col in enumerate(existing_year_columns):\n",
    "                if int(year_col) == current_startup:\n",
    "                    startup_idx = i\n",
    "                    break\n",
    "            \n",
    "            if startup_idx is None:\n",
    "                continue\n",
    "            \n",
    "            # Look for ANY ≥5 consecutive zeros that occur after startup year\n",
    "            found_long_zeros_after_startup = False\n",
    "            zeros_start_idx = None\n",
    "            zeros_end_idx = None\n",
    "            \n",
    "            # Check all zero stretches that occur after startup year\n",
    "            for stretch in mine['zero_stretches']:\n",
    "                stretch_start_year = int(existing_year_columns[stretch['start']])\n",
    "                \n",
    "                # Check if this stretch occurs after startup year and is ≥5 zeros\n",
    "                if stretch_start_year > current_startup and stretch['length'] >= 5:\n",
    "                    found_long_zeros_after_startup = True\n",
    "                    zeros_start_idx = stretch['start']\n",
    "                    zeros_end_idx = stretch['end']\n",
    "                    break  # Use the first qualifying stretch found\n",
    "            \n",
    "            if found_long_zeros_after_startup:\n",
    "                if closure_year > 2023:\n",
    "                    # Find first non-zero production AFTER the consecutive zeros\n",
    "                    first_production_after_zeros = None\n",
    "                    for i in range(zeros_end_idx + 1, len(production_values)):\n",
    "                        value = production_values[i]\n",
    "                        if not pd.isna(value) and value > 0:\n",
    "                            first_production_after_zeros = int(existing_year_columns[i])\n",
    "                            break\n",
    "                    \n",
    "                    if first_production_after_zeros:\n",
    "                        # Update startup year to first production after zeros\n",
    "                        df.at[idx, 'START_UP_YR'] = first_production_after_zeros\n",
    "                        startup_changes_step2.append({\n",
    "                            'prop_id': prop_id,\n",
    "                            'old_startup': current_startup,\n",
    "                            'new_startup': first_production_after_zeros,\n",
    "                            'zeros_length': zeros_end_idx - zeros_start_idx + 1,\n",
    "                            'closure_year': closure_year\n",
    "                        })\n",
    "                else:  # closure_year <= 2023\n",
    "                    # Drop the mine\n",
    "                    mines_to_drop.append({\n",
    "                        'idx': idx,\n",
    "                        'prop_id': prop_id,\n",
    "                        'closure_year': closure_year,\n",
    "                        'zeros_length': zeros_end_idx - zeros_start_idx + 1,\n",
    "                        'zeros_start_year': int(existing_year_columns[zeros_start_idx]),\n",
    "                        'zeros_end_year': int(existing_year_columns[zeros_end_idx])\n",
    "                    })\n",
    "        \n",
    "        # Drop mines identified in Step 2\n",
    "        if mines_to_drop:\n",
    "            indices_to_drop = [mine['idx'] for mine in mines_to_drop]\n",
    "            df = df.drop(indices_to_drop).reset_index(drop=True)\n",
    "            print(f\"Dropped {len(mines_to_drop)} mines with ≥5 zeros after startup and closure ≤ 2023\")\n",
    "        \n",
    "        # Report results\n",
    "        print(f\"\\n=== PROCESSING RESULTS ===\")\n",
    "        \n",
    "        print(f\"\\nStep 1 - Criteria 4 mines processed: {len(criteria_4_mines)}\")\n",
    "        print(f\"Step 1 - Startup year changes made: {len(startup_changes_step1)}\")\n",
    "        \n",
    "        if startup_changes_step1:\n",
    "            print(f\"\\nStep 1 startup year corrections:\")\n",
    "            for change in startup_changes_step1:\n",
    "                print(f\"  PROP_ID {change['prop_id']}: Changed startup from {change['old_startup']} to {change['new_startup']} ({change['zero_stretches_count']} zero stretches)\")\n",
    "        \n",
    "        print(f\"\\nStep 2 - Mines with ≥5 zeros after startup: {len(startup_changes_step2) + len(mines_to_drop)}\")\n",
    "        print(f\"Step 2 - Startup year changes (closure > 2023): {len(startup_changes_step2)}\")\n",
    "        print(f\"Step 2 - Mines dropped (closure ≤ 2023): {len(mines_to_drop)}\")\n",
    "        \n",
    "        if startup_changes_step2:\n",
    "            print(f\"\\nStep 2 startup year corrections (≥5 zeros after startup, closure > 2023):\")\n",
    "            for change in startup_changes_step2:\n",
    "                print(f\"  PROP_ID {change['prop_id']}: Changed startup from {change['old_startup']} to {change['new_startup']} ({change['zeros_length']} zeros, closure: {change['closure_year']})\")\n",
    "        \n",
    "        if mines_to_drop:\n",
    "            print(f\"\\nStep 2 mines dropped (≥5 zeros after startup, closure ≤ 2023):\")\n",
    "            for mine in mines_to_drop:\n",
    "                print(f\"  PROP_ID {mine['prop_id']}: {mine['zeros_length']} zeros ({mine['zeros_start_year']}-{mine['zeros_end_year']}), closure: {mine['closure_year']}\")\n",
    "        \n",
    "        print(f\"\\nCriteria 5 mines: {len(criteria_5_mines)} (left unchanged)\")\n",
    "        if criteria_5_mines:\n",
    "            print(\"Criteria 5 mines (multiple gaps, no changes made):\")\n",
    "            for mine in criteria_5_mines[:10]:  # Show first 10\n",
    "                print(f\"  PROP_ID {mine['prop_id']}: {len(mine['zero_stretches'])} zero stretches\")\n",
    "            if len(criteria_5_mines) > 10:\n",
    "                print(f\"  ... and {len(criteria_5_mines) - 10} more\")\n",
    "        \n",
    "        # Save the modified dataset\n",
    "        df.to_excel(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\n=== FINAL SUMMARY ===\")\n",
    "        print(f\"Total criteria 4 mines: {len(criteria_4_mines)}\")\n",
    "        print(f\"Step 1 startup years corrected: {len(startup_changes_step1)}\")\n",
    "        print(f\"Step 2 startup years corrected: {len(startup_changes_step2)}\")\n",
    "        print(f\"Step 2 mines dropped: {len(mines_to_drop)}\")\n",
    "        print(f\"Criteria 5 mines (unchanged): {len(criteria_5_mines)}\")\n",
    "        print(f\"Final dataset rows: {len(df)}\")\n",
    "        print(f\"Modified dataset saved: {output_file}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    modify_flagged_mines_startup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb068e",
   "metadata": {},
   "source": [
    "Merge with Maus Spatial Data\n",
    "1) Merge the cluster id of each mine into the dataset based on the mines PROP_ID and the column 'id_data_source' in the spatial data xlsx file\n",
    "2) drop any mines that have no cluster id\n",
    "3) if there are more than 1 mine sharing the same cluster id and they are different mine types (open pit and underground), drop them because you cannot allocate area based on cumulative production between underground and open pit mines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d1f2a888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cluster data from Data Input/Maus_Spa_Data/cluster_points_concordance.csv...\n",
      "Found 42799 rows in cluster data\n",
      "Reading Excel file from Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx...\n",
      "Found 763 rows in Excel file\n",
      "Merging data...\n",
      "Removed 0 rows with no id_cluster assigned (0.0% of total)\n",
      "Saving merged data back to Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx...\n",
      "Done! Final dataset has 763 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_cluster_data(csv_path, excel_path):\n",
    "    \"\"\"\n",
    "    Merge cluster IDs from a CSV file into an Excel file based on property IDs,\n",
    "    removing rows without assigned cluster IDs, and save back to the original Excel file.\n",
    "    The id_cluster column will be added as the rightmost column.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        Path to the CSV file containing cluster data\n",
    "    excel_path : str\n",
    "        Path to the Excel file to be updated\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        The merged and filtered DataFrame\n",
    "    \"\"\"\n",
    "    # Read the CSV file with cluster data\n",
    "    print(f\"Reading cluster data from {csv_path}...\")\n",
    "    cluster_df = pd.read_csv(csv_path)\n",
    "    print(f\"Found {len(cluster_df)} rows in cluster data\")\n",
    "    \n",
    "    # Read the Excel file\n",
    "    print(f\"Reading Excel file from {excel_path}...\")\n",
    "    excel_df = pd.read_excel(excel_path)\n",
    "    print(f\"Found {len(excel_df)} rows in Excel file\")\n",
    "    \n",
    "    # Verify column names exist\n",
    "    if 'id_data_source' not in cluster_df.columns:\n",
    "        raise ValueError(f\"Column 'id_data_source' not found in the CSV file. Available columns: {cluster_df.columns.tolist()}\")\n",
    "    if 'id_cluster' not in cluster_df.columns:\n",
    "        raise ValueError(f\"Column 'id_cluster' not found in the CSV file. Available columns: {cluster_df.columns.tolist()}\")\n",
    "    if 'PROP_ID' not in excel_df.columns:\n",
    "        raise ValueError(f\"Column 'PROP_ID' not found in the Excel file. Available columns: {excel_df.columns.tolist()}\")\n",
    "    \n",
    "    # Convert both ID columns to strings to ensure proper matching\n",
    "    cluster_df['id_data_source'] = cluster_df['id_data_source'].astype(str)\n",
    "    excel_df['PROP_ID'] = excel_df['PROP_ID'].astype(str)\n",
    "    \n",
    "    # Create a subset of the cluster data with only the necessary columns\n",
    "    cluster_subset = cluster_df[['id_data_source', 'id_cluster']].copy()\n",
    "    \n",
    "    # Rename column to match Excel for merging\n",
    "    cluster_subset = cluster_subset.rename(columns={'id_data_source': 'PROP_ID'})\n",
    "    \n",
    "    # Merge the data\n",
    "    print(\"Merging data...\")\n",
    "    merged_df = pd.merge(\n",
    "        excel_df,\n",
    "        cluster_subset,\n",
    "        on='PROP_ID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # If id_cluster already exists in the original data, this will create a duplicate\n",
    "    # Ensure we have only one id_cluster column\n",
    "    if 'id_cluster_x' in merged_df.columns and 'id_cluster_y' in merged_df.columns:\n",
    "        # This happens when id_cluster already existed in excel_df\n",
    "        # Keep the newly merged column and drop the original\n",
    "        merged_df = merged_df.drop(columns=['id_cluster_x'])\n",
    "        merged_df = merged_df.rename(columns={'id_cluster_y': 'id_cluster'})\n",
    "    \n",
    "    # Ensure id_cluster is the last column\n",
    "    if 'id_cluster' in merged_df.columns:\n",
    "        # Get all columns except id_cluster\n",
    "        other_cols = [col for col in merged_df.columns if col != 'id_cluster']\n",
    "        # Reorder columns to put id_cluster at the end\n",
    "        merged_df = merged_df[other_cols + ['id_cluster']]\n",
    "    \n",
    "    # Count rows before filtering\n",
    "    total_rows = len(merged_df)\n",
    "    \n",
    "    # Filter out rows without an id_cluster\n",
    "    filtered_df = merged_df.dropna(subset=['id_cluster'])\n",
    "    \n",
    "    # Count removed rows\n",
    "    removed_rows = total_rows - len(filtered_df)\n",
    "    print(f\"Removed {removed_rows} rows with no id_cluster assigned ({removed_rows/total_rows:.1%} of total)\")\n",
    "    \n",
    "    # Save back to the original Excel file\n",
    "    print(f\"Saving merged data back to {excel_path}...\")\n",
    "    filtered_df.to_excel(excel_path, index=False)\n",
    "    \n",
    "    print(f\"Done! Final dataset has {len(filtered_df)} rows.\")\n",
    "    return filtered_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"Data Input/Maus_Spa_Data/cluster_points_concordance.csv\"\n",
    "    excel_path = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        merge_cluster_data(csv_path, excel_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0506b84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx...\n",
      "Dataset loaded with 763 rows and 61 columns\n",
      "Warning: Missing optional columns for detailed info: ['COUNTRY']\n",
      "Will proceed with available columns only.\n",
      "After removing rows with missing values: 763 rows (removed 0 rows)\n",
      "\n",
      "============================================================\n",
      "ID CLUSTER ANALYSIS RESULTS\n",
      "============================================================\n",
      "\n",
      "Basic Statistics:\n",
      "- Total unique ID clusters: 640\n",
      "- Total unique mines (PROP_ID): 763\n",
      "\n",
      "Mines per Cluster Distribution:\n",
      "- 567 clusters have 1 mine(s)\n",
      "- 53 clusters have 2 mine(s)\n",
      "- 9 clusters have 3 mine(s)\n",
      "- 7 clusters have 4 mine(s)\n",
      "- 2 clusters have 5 mine(s)\n",
      "- 1 clusters have 6 mine(s)\n",
      "- 1 clusters have 19 mine(s)\n",
      "\n",
      "Clusters with multiple mines: 73\n",
      "\n",
      "Top 10 clusters with most mines:\n",
      "- Cluster H0027425: 19 mines\n",
      "- Cluster H0027430: 6 mines\n",
      "- Cluster H0022218: 5 mines\n",
      "- Cluster H0043023: 5 mines\n",
      "- Cluster H0027205: 4 mines\n",
      "- Cluster H0035427: 4 mines\n",
      "- Cluster H0035565: 4 mines\n",
      "- Cluster H0040706: 4 mines\n",
      "- Cluster H0042323: 4 mines\n",
      "- Cluster H0045007: 4 mines\n",
      "\n",
      "============================================================\n",
      "MINE TYPE ANALYSIS WITHIN CLUSTERS\n",
      "============================================================\n",
      "\n",
      "Overall mine type distribution:\n",
      "- Open Pit: 453 mines\n",
      "- Underground: 303 mines\n",
      "- Brine: 7 mines\n",
      "\n",
      "Analyzing clusters with mixed mine types...\n",
      "\n",
      "Clusters with mixed mine types: 17\n",
      "\n",
      "Detailed breakdown of mixed-type clusters:\n",
      "- Cluster H0010025: 2 mines (Underground, Open Pit)\n",
      "- Cluster H0024134: 2 mines (Open Pit, Underground)\n",
      "- Cluster H0027205: 4 mines (Open Pit, Underground)\n",
      "- Cluster H0027311: 2 mines (Open Pit, Underground)\n",
      "- Cluster H0029319: 2 mines (Underground, Open Pit)\n",
      "- Cluster H0035414: 2 mines (Underground, Open Pit)\n",
      "- Cluster H0035420: 2 mines (Underground, Open Pit)\n",
      "- Cluster H0038694: 2 mines (Open Pit, Underground)\n",
      "- Cluster H0038801: 2 mines (Open Pit, Underground)\n",
      "- Cluster H0041600: 2 mines (Underground, Open Pit)\n",
      "- Cluster H0042323: 4 mines (Open Pit, Underground)\n",
      "- Cluster H0044707: 2 mines (Open Pit, Underground)\n",
      "- Cluster H0046147: 2 mines (Open Pit, Underground)\n",
      "- Cluster H0046150: 3 mines (Open Pit, Underground)\n",
      "- Cluster H0048259: 3 mines (Underground, Open Pit)\n",
      "- Cluster H0052142: 2 mines (Open Pit, Underground)\n",
      "- Cluster H0057449: 2 mines (Underground, Open Pit)\n",
      "\n",
      "--------------------------------------------------\n",
      "CLUSTERS WITH BOTH OPEN PIT AND UNDERGROUND MINES\n",
      "--------------------------------------------------\n",
      "Found 17 clusters with both Open Pit and Underground mines:\n",
      "\n",
      "Cluster H0010025:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0010025:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 59306\n",
      "      Name: Zhongguan | Commodity: Iron Ore\n",
      "    Underground - PROP_ID: 68387\n",
      "      Name: Beiminghe | Commodity: Iron Ore\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0024134:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0024134:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 59165\n",
      "      Name: Jingshansi | Commodity: Iron Ore\n",
      "    Underground - PROP_ID: 59291\n",
      "      Name: Wugang | Commodity: Iron Ore\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0027205:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 3 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0027205:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 27244\n",
      "      Name: Candelaria | Commodity: Copper\n",
      "    Underground - PROP_ID: 28369\n",
      "      Name: Punta del Cobre | Commodity: Copper\n",
      "    Underground - PROP_ID: 28866\n",
      "      Name: Ojos del Salado | Commodity: Copper\n",
      "    Underground - PROP_ID: 30189\n",
      "      Name: Trinidad | Commodity: Copper\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0027311:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0027311:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 27530\n",
      "      Name: Carmen de Andacollo | Commodity: Copper\n",
      "    Underground - PROP_ID: 78627\n",
      "      Name: Farellon | Commodity: Copper\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0029319:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0029319:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 65191\n",
      "      Name: Parnassos | Commodity: Bauxite\n",
      "    Underground - PROP_ID: 65193\n",
      "      Name: Delphi-Distomon | Commodity: Bauxite\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0035414:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0035414:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 33810\n",
      "      Name: Mwambashi | Commodity: Copper\n",
      "    Underground - PROP_ID: 28268\n",
      "      Name: Chambishi | Commodity: Copper\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0035420:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0035420:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 30969\n",
      "      Name: Muliashi North | Commodity: Copper\n",
      "    Underground - PROP_ID: 28272\n",
      "      Name: Baluba | Commodity: Copper\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0038694:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0038694:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 59297\n",
      "      Name: Yangshan | Commodity: Iron Ore\n",
      "    Underground - PROP_ID: 80279\n",
      "      Name: Rongfeng | Commodity: Zinc\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0038801:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0038801:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 27385\n",
      "      Name: Dexing Complex | Commodity: Copper\n",
      "    Underground - PROP_ID: 27818\n",
      "      Name: Yinshan | Commodity: Copper\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0041600:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0041600:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 30469\n",
      "      Name: Proyecto de Rio Tinto | Commodity: Copper\n",
      "    Underground - PROP_ID: 27674\n",
      "      Name: MATSA | Commodity: Copper\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0042323:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 3 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0042323:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 28680\n",
      "      Name: Toromocho | Commodity: Copper\n",
      "    Underground - PROP_ID: 26574\n",
      "      Name: Yauli | Commodity: Zinc\n",
      "    Underground - PROP_ID: 27728\n",
      "      Name: Yauliyacu | Commodity: Zinc\n",
      "    Underground - PROP_ID: 27729\n",
      "      Name: San Cristobal | Commodity: Zinc\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0044707:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0044707:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 27115\n",
      "      Name: Antamina | Commodity: Copper\n",
      "    Underground - PROP_ID: 25747\n",
      "      Name: Contonga | Commodity: Zinc\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0046147:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0046147:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 32174\n",
      "      Name: Kryvyi Rih | Commodity: Iron Ore\n",
      "    Underground - PROP_ID: 32201\n",
      "      Name: Kryvyi Rih State Combine | Commodity: Iron Ore\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0046150:\n",
      "  - Open Pit: 2 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0046150:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 31911\n",
      "      Name: Northern GOK | Commodity: Iron Ore\n",
      "    Open Pit - PROP_ID: 32204\n",
      "      Name: Central GOK | Commodity: Iron Ore\n",
      "    Underground - PROP_ID: 35181\n",
      "      Name: Sukha Balka | Commodity: Iron Ore\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0048259:\n",
      "  - Open Pit: 2 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0048259:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 31950\n",
      "      Name: Lebedinsky | Commodity: Iron Ore\n",
      "    Open Pit - PROP_ID: 31969\n",
      "      Name: Stoylensky GOK | Commodity: Iron Ore\n",
      "    Underground - PROP_ID: 33003\n",
      "      Name: KMAruda | Commodity: Iron Ore\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0052142:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0052142:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 31923\n",
      "      Name: Corumba | Commodity: Iron Ore\n",
      "    Underground - PROP_ID: 32611\n",
      "      Name: Urucum | Commodity: Manganese\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "Cluster H0057449:\n",
      "  - Open Pit: 1 mines\n",
      "  - Underground: 1 mines\n",
      "\n",
      "  Detailed mine information for Cluster H0057449:\n",
      "  --------------------------------------------------------------------------------\n",
      "    Open Pit - PROP_ID: 32592\n",
      "      Name: Dahongshan | Commodity: Iron Ore\n",
      "    Underground - PROP_ID: 29744\n",
      "      Name: Dahongshan | Commodity: Copper\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "DATA CLEANING - REMOVING MIXED-TYPE CLUSTERS\n",
      "============================================================\n",
      "\n",
      "Removing 17 clusters with mixed mine types...\n",
      "- Clusters to remove: ['H0010025', 'H0024134', 'H0027205', 'H0027311', 'H0029319', 'H0035414', 'H0035420', 'H0038694', 'H0038801', 'H0041600', 'H0042323', 'H0044707', 'H0046147', 'H0046150', 'H0048259', 'H0052142', 'H0057449']\n",
      "- Unique mines being removed: 40\n",
      "- Total rows being removed: 40\n",
      "\n",
      "Dataset size before cleaning: 763 rows\n",
      "Dataset size after cleaning: 723 rows\n",
      "Rows removed: 40\n",
      "\n",
      "Cleaned dataset saved back to: Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\n",
      "✓ Data cleaning completed successfully!\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "- Total clusters (before cleaning): 640\n",
      "- Clusters with single mine: 567 (88.6%)\n",
      "- Clusters with multiple mines: 73 (11.4%)\n",
      "- Clusters with mixed mine types: 17\n",
      "- Clusters with Open Pit + Underground: 17\n",
      "- Clusters removed from dataset: 17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_and_clean_id_clusters():\n",
    "    \"\"\"\n",
    "    Analyze the number of mines (PROP_ID) that share the same id_cluster,\n",
    "    check for open pit vs underground mines sharing the same cluster,\n",
    "    and remove problematic clusters from the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the Excel file\n",
    "    file_path = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "    #file_path = \"Data Output/Final_Dataset.xlsx\"\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        print(f\"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    required_columns = ['PROP_ID', 'id_cluster', 'MINE_TYPE1']\n",
    "    optional_columns = ['PROP_NAME', 'COUNTRY', 'PRIMARY_COMMODITY']\n",
    "    \n",
    "    missing_required = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_required:\n",
    "        print(f\"Missing required columns: {missing_required}\")\n",
    "        return\n",
    "    \n",
    "    missing_optional = [col for col in optional_columns if col not in df.columns]\n",
    "    if missing_optional:\n",
    "        print(f\"Warning: Missing optional columns for detailed info: {missing_optional}\")\n",
    "        print(\"Will proceed with available columns only.\")\n",
    "    \n",
    "    available_detail_columns = [col for col in optional_columns if col in df.columns]\n",
    "    \n",
    "    # Remove rows with missing values in key columns\n",
    "    original_count = len(df)\n",
    "    df_clean = df.dropna(subset=required_columns).copy()\n",
    "    print(f\"After removing rows with missing values: {len(df_clean)} rows (removed {original_count - len(df_clean)} rows)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ID CLUSTER ANALYSIS RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Basic statistics about clusters\n",
    "    total_clusters = df_clean['id_cluster'].nunique()\n",
    "    total_mines = df_clean['PROP_ID'].nunique()\n",
    "    \n",
    "    print(f\"\\nBasic Statistics:\")\n",
    "    print(f\"- Total unique ID clusters: {total_clusters}\")\n",
    "    print(f\"- Total unique mines (PROP_ID): {total_mines}\")\n",
    "    \n",
    "    # 2. Analyze mines per cluster\n",
    "    cluster_mine_counts = df_clean.groupby('id_cluster')['PROP_ID'].nunique().reset_index()\n",
    "    cluster_mine_counts.columns = ['id_cluster', 'mine_count']\n",
    "    \n",
    "    print(f\"\\nMines per Cluster Distribution:\")\n",
    "    mine_count_distribution = cluster_mine_counts['mine_count'].value_counts().sort_index()\n",
    "    for count, frequency in mine_count_distribution.items():\n",
    "        print(f\"- {frequency} clusters have {count} mine(s)\")\n",
    "    \n",
    "    # 3. Find clusters with multiple mines\n",
    "    multi_mine_clusters = cluster_mine_counts[cluster_mine_counts['mine_count'] > 1]\n",
    "    print(f\"\\nClusters with multiple mines: {len(multi_mine_clusters)}\")\n",
    "    \n",
    "    if len(multi_mine_clusters) > 0:\n",
    "        print(\"\\nTop 10 clusters with most mines:\")\n",
    "        top_clusters = multi_mine_clusters.nlargest(10, 'mine_count')\n",
    "        for _, row in top_clusters.iterrows():\n",
    "            print(f\"- Cluster {row['id_cluster']}: {row['mine_count']} mines\")\n",
    "    \n",
    "    # 4. Analyze mine types within clusters\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"MINE TYPE ANALYSIS WITHIN CLUSTERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get unique mine types\n",
    "    mine_types = df_clean['MINE_TYPE1'].value_counts()\n",
    "    print(f\"\\nOverall mine type distribution:\")\n",
    "    for mine_type, count in mine_types.items():\n",
    "        print(f\"- {mine_type}: {count} mines\")\n",
    "    \n",
    "    # Analyze mine types per cluster\n",
    "    cluster_mine_types = df_clean.groupby('id_cluster').agg({\n",
    "        'PROP_ID': 'nunique',\n",
    "        'MINE_TYPE1': lambda x: list(x.unique())\n",
    "    }).reset_index()\n",
    "    cluster_mine_types.columns = ['id_cluster', 'mine_count', 'mine_types']\n",
    "    \n",
    "    # 5. Find clusters with mixed mine types\n",
    "    print(f\"\\nAnalyzing clusters with mixed mine types...\")\n",
    "    \n",
    "    mixed_type_clusters = []\n",
    "    clusters_to_remove = []\n",
    "    \n",
    "    for _, row in cluster_mine_types.iterrows():\n",
    "        if len(row['mine_types']) > 1:\n",
    "            mixed_type_clusters.append({\n",
    "                'cluster': row['id_cluster'],\n",
    "                'mine_count': row['mine_count'],\n",
    "                'mine_types': row['mine_types']\n",
    "            })\n",
    "            # Add this cluster to removal list\n",
    "            clusters_to_remove.append(row['id_cluster'])\n",
    "    \n",
    "    print(f\"\\nClusters with mixed mine types: {len(mixed_type_clusters)}\")\n",
    "    \n",
    "    if len(mixed_type_clusters) > 0:\n",
    "        print(f\"\\nDetailed breakdown of mixed-type clusters:\")\n",
    "        for cluster_info in mixed_type_clusters:\n",
    "            types_str = \", \".join(cluster_info['mine_types'])\n",
    "            print(f\"- Cluster {cluster_info['cluster']}: {cluster_info['mine_count']} mines ({types_str})\")\n",
    "        \n",
    "        # 6. Specifically check for Open Pit + Underground combinations\n",
    "        open_underground_clusters = []\n",
    "        for cluster_info in mixed_type_clusters:\n",
    "            types = cluster_info['mine_types']\n",
    "            if 'Open Pit' in types and 'Underground' in types:\n",
    "                open_underground_clusters.append(cluster_info)\n",
    "        \n",
    "        print(f\"\\n\" + \"-\"*50)\n",
    "        print(f\"CLUSTERS WITH BOTH OPEN PIT AND UNDERGROUND MINES\")\n",
    "        print(f\"-\"*50)\n",
    "        print(f\"Found {len(open_underground_clusters)} clusters with both Open Pit and Underground mines:\")\n",
    "        \n",
    "        if len(open_underground_clusters) > 0:\n",
    "            for cluster_info in open_underground_clusters:\n",
    "                cluster_id = cluster_info['cluster']\n",
    "                print(f\"\\nCluster {cluster_id}:\")\n",
    "                \n",
    "                # Get detailed breakdown for this cluster\n",
    "                cluster_data = df_clean[df_clean['id_cluster'] == cluster_id]\n",
    "                mine_type_breakdown = cluster_data.groupby('MINE_TYPE1')['PROP_ID'].nunique()\n",
    "                \n",
    "                for mine_type, count in mine_type_breakdown.items():\n",
    "                    print(f\"  - {mine_type}: {count} mines\")\n",
    "                \n",
    "                # Show detailed information for all mines in this cluster\n",
    "                print(f\"\\n  Detailed mine information for Cluster {cluster_id}:\")\n",
    "                print(f\"  {'-'*80}\")\n",
    "                \n",
    "                # Sort by mine type for better organization\n",
    "                cluster_mines = cluster_data.sort_values(['MINE_TYPE1', 'PROP_ID'])\n",
    "                \n",
    "                for _, mine_row in cluster_mines.iterrows():\n",
    "                    mine_id = mine_row['PROP_ID']\n",
    "                    mine_type = mine_row['MINE_TYPE1']\n",
    "                    \n",
    "                    # Build detail string with available information\n",
    "                    details = []\n",
    "                    \n",
    "                    if 'PROP_NAME' in available_detail_columns:\n",
    "                        prop_name = mine_row['PROP_NAME'] if pd.notna(mine_row['PROP_NAME']) else 'N/A'\n",
    "                        details.append(f\"Name: {prop_name}\")\n",
    "                    \n",
    "                    if 'COUNTRY' in available_detail_columns:\n",
    "                        country = mine_row['COUNTRY'] if pd.notna(mine_row['COUNTRY']) else 'N/A'\n",
    "                        details.append(f\"Country: {country}\")\n",
    "                    \n",
    "                    if 'PRIMARY_COMMODITY' in available_detail_columns:\n",
    "                        commodity = mine_row['PRIMARY_COMMODITY'] if pd.notna(mine_row['PRIMARY_COMMODITY']) else 'N/A'\n",
    "                        details.append(f\"Commodity: {commodity}\")\n",
    "                    \n",
    "                    detail_string = \" | \".join(details) if details else \"No additional details available\"\n",
    "                    \n",
    "                    print(f\"    {mine_type} - PROP_ID: {mine_id}\")\n",
    "                    print(f\"      {detail_string}\")\n",
    "                \n",
    "                print(f\"  {'-'*80}\")\n",
    "        else:\n",
    "            print(\"No clusters found with both Open Pit and Underground mines.\")\n",
    "    else:\n",
    "        print(\"No clusters found with mixed mine types - all clusters contain only one type of mine.\")\n",
    "    \n",
    "    # 7. Remove problematic clusters from the dataset\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA CLEANING - REMOVING MIXED-TYPE CLUSTERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if len(clusters_to_remove) > 0:\n",
    "        print(f\"\\nRemoving {len(clusters_to_remove)} clusters with mixed mine types...\")\n",
    "        \n",
    "        # Count mines to be removed\n",
    "        mines_to_remove = df_clean[df_clean['id_cluster'].isin(clusters_to_remove)]['PROP_ID'].nunique()\n",
    "        rows_to_remove = len(df_clean[df_clean['id_cluster'].isin(clusters_to_remove)])\n",
    "        \n",
    "        print(f\"- Clusters to remove: {clusters_to_remove}\")\n",
    "        print(f\"- Unique mines being removed: {mines_to_remove}\")\n",
    "        print(f\"- Total rows being removed: {rows_to_remove}\")\n",
    "        \n",
    "        # Remove the problematic clusters from the original dataframe\n",
    "        df_cleaned = df[~df['id_cluster'].isin(clusters_to_remove)].copy()\n",
    "        \n",
    "        print(f\"\\nDataset size before cleaning: {len(df)} rows\")\n",
    "        print(f\"Dataset size after cleaning: {len(df_cleaned)} rows\")\n",
    "        print(f\"Rows removed: {len(df) - len(df_cleaned)}\")\n",
    "        \n",
    "        # Save the cleaned dataset\n",
    "        try:\n",
    "            df_cleaned.to_excel(file_path, index=False)\n",
    "            print(f\"\\nCleaned dataset saved back to: {file_path}\")\n",
    "            print(\"✓ Data cleaning completed successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving cleaned dataset: {e}\")\n",
    "            print(\"The analysis was completed but the file could not be saved.\")\n",
    "    else:\n",
    "        print(\"\\nNo mixed-type clusters found - no data cleaning required.\")\n",
    "        print(\"Dataset remains unchanged.\")\n",
    "    \n",
    "    # 8. Summary statistics\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    single_mine_clusters = len(cluster_mine_counts[cluster_mine_counts['mine_count'] == 1])\n",
    "    multi_mine_clusters_count = len(cluster_mine_counts[cluster_mine_counts['mine_count'] > 1])\n",
    "    \n",
    "    print(f\"- Total clusters (before cleaning): {total_clusters}\")\n",
    "    print(f\"- Clusters with single mine: {single_mine_clusters} ({single_mine_clusters/total_clusters*100:.1f}%)\")\n",
    "    print(f\"- Clusters with multiple mines: {multi_mine_clusters_count} ({multi_mine_clusters_count/total_clusters*100:.1f}%)\")\n",
    "    print(f\"- Clusters with mixed mine types: {len(mixed_type_clusters)}\")\n",
    "    if len(mixed_type_clusters) > 0:\n",
    "        print(f\"- Clusters with Open Pit + Underground: {len(open_underground_clusters)}\")\n",
    "        print(f\"- Clusters removed from dataset: {len(clusters_to_remove)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_and_clean_id_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7e438",
   "metadata": {},
   "source": [
    "Conversion to long format for cumulative production use\n",
    "1) convert to long format\n",
    "2) delete all rows before start up year and after closure year if applicable to reduce number of empty cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3fb56998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\n",
      "Original data shape: (723, 61)\n",
      "Found 44 year columns: ['1980', '1981', '1982', '1983', '1984']...['2019', '2020', '2021', '2022', '2023']\n",
      "Converting to long format...\n",
      "Processing coal commodities...\n",
      "Sorting data by PROP_ID, coal type, and Year...\n",
      "Long format data shape: (31812, 19)\n",
      "Saving to: Data Output/Commodity Production/Commodity_Production-1980_2023_long.xlsx\n",
      "Conversion completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def convert_to_long_format(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    Convert a wide-format Excel file to long format by transforming year columns\n",
    "    into a single Year column with corresponding values for each mine property.\n",
    "    For coal commodities with the same PROP_ID, groups all thermal coal records together\n",
    "    and all metallurgical coal records together, with each group ordered by year.\n",
    "   \n",
    "    Args:\n",
    "        input_file (str): Path to the wide-format Excel file\n",
    "        output_file (str, optional): Path for saving the long-format output file.\n",
    "                                    If None, will use input_file with \"_long\" appended.\n",
    "    \"\"\"\n",
    "    # Set default output file if not provided\n",
    "    if output_file is None:\n",
    "        base_name, ext = os.path.splitext(input_file)\n",
    "        output_file = f\"{base_name}_long{ext}\"\n",
    "   \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "   \n",
    "    print(f\"Reading file: {input_file}\")\n",
    "   \n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(input_file)\n",
    "       \n",
    "        # Check if PROP_ID column exists\n",
    "        if 'PROP_ID' not in df.columns:\n",
    "            print(\"Error: PROP_ID column not found in the input file.\")\n",
    "            return\n",
    "       \n",
    "        print(f\"Original data shape: {df.shape}\")\n",
    "       \n",
    "        # Identify year columns (assuming they are named like \"1980\", \"1981\", etc.)\n",
    "        year_pattern = re.compile(r'^(19|20)\\d{2}$')  # Match years from 1900-2099\n",
    "        year_columns = [col for col in df.columns if isinstance(col, str) and year_pattern.match(col)]\n",
    "       \n",
    "        if not year_columns:\n",
    "            # Try to identify if years might be stored as integers or floats\n",
    "            year_columns = [col for col in df.columns if isinstance(col, (int, float)) and 1900 <= col <= 2099]\n",
    "       \n",
    "        if not year_columns:\n",
    "            print(\"No year columns found in the data. Please check the column names.\")\n",
    "            return\n",
    "       \n",
    "        print(f\"Found {len(year_columns)} year columns: {year_columns[:5]}...\"\n",
    "              f\"{year_columns[-5:]}\" if len(year_columns) > 10 else year_columns)\n",
    "       \n",
    "        # Identify ID columns and attribute columns\n",
    "        # Assuming all non-year columns are attributes to keep\n",
    "        id_and_attribute_columns = [col for col in df.columns if col not in year_columns]\n",
    "       \n",
    "        print(\"Converting to long format...\")\n",
    "       \n",
    "        # Use pandas melt to convert to long format\n",
    "        long_df = pd.melt(\n",
    "            df,\n",
    "            id_vars=id_and_attribute_columns,\n",
    "            value_vars=year_columns,\n",
    "            var_name='Year',\n",
    "            value_name='Production'\n",
    "        )\n",
    "       \n",
    "        # Convert Year column to integer if needed\n",
    "        long_df['Year'] = pd.to_numeric(long_df['Year'], errors='coerce').astype('Int64')\n",
    "        \n",
    "        # Check if PRIMARY_COMMODITY column exists\n",
    "        if 'PRIMARY_COMMODITY' in long_df.columns:\n",
    "            print(\"Processing coal commodities...\")\n",
    "            \n",
    "            # Create a coal type indicator with proper sorting order\n",
    "            def get_coal_type(commodity):\n",
    "                if not isinstance(commodity, str):\n",
    "                    return 2  # Non-coal or non-string types last\n",
    "                \n",
    "                commodity = commodity.lower()\n",
    "                if 'coal (thermal)' in commodity:\n",
    "                    return 0  # Thermal coal first\n",
    "                elif 'coal (metallurgical)' in commodity:\n",
    "                    return 1  # Metallurgical coal second\n",
    "                else:\n",
    "                    return 2  # Non-coal types last\n",
    "            \n",
    "            # Add a temporary coal type indicator column\n",
    "            long_df['coal_type'] = long_df['PRIMARY_COMMODITY'].apply(get_coal_type)\n",
    "            \n",
    "            # Sort first by PROP_ID to group all records for the same property\n",
    "            # Then by coal_type to group all thermal coal before metallurgical\n",
    "            # Then by Year to ensure chronological order within each coal type\n",
    "            print(\"Sorting data by PROP_ID, coal type, and Year...\")\n",
    "            long_df = long_df.sort_values(by=['PROP_ID', 'coal_type', 'Year'])\n",
    "            \n",
    "            # Remove the temporary column\n",
    "            long_df = long_df.drop(columns=['coal_type'])\n",
    "        else:\n",
    "            # If PRIMARY_COMMODITY doesn't exist, just sort by PROP_ID and Year\n",
    "            print(\"PRIMARY_COMMODITY column not found. Sorting by PROP_ID and Year only...\")\n",
    "            long_df = long_df.sort_values(by=['PROP_ID', 'Year'])\n",
    "       \n",
    "        print(f\"Long format data shape: {long_df.shape}\")\n",
    "       \n",
    "        # Save to Excel\n",
    "        print(f\"Saving to: {output_file}\")\n",
    "        long_df.to_excel(output_file, index=False)\n",
    "       \n",
    "        print(\"Conversion completed successfully!\")\n",
    "        return long_df\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_final.xlsx\"\n",
    "    output_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_long.xlsx\"\n",
    "    convert_to_long_format(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e6c09798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from Data Output/Commodity Production/Commodity_Production-1980_2023_long.xlsx...\n",
      "Initial dataset: 31812 rows, 723 unique properties\n",
      "After START_UP_YR filter: 19042 rows\n",
      "Removed 12770 rows (40.14%) that were before START_UP_YR\n",
      "After PROJ_CLOSURE_YR filter: 18493 rows\n",
      "Removed 549 rows (2.88%) that were after PROJ_CLOSURE_YR\n",
      "Total filtered dataset: 18493 rows, 723 unique properties\n",
      "Total removed: 13319 rows (41.87% of original data)\n",
      "Saving filtered data to Data Output/Commodity Production/Commodity_Production-1980_2023_long.xlsx...\n",
      "Done!\n",
      "\n",
      "Summary Statistics:\n",
      "   Total Rows  Unique Properties               Description Percent of Original\n",
      "0       31812                723             Original data                100%\n",
      "1       19042                723  After START_UP_YR filter              59.86%\n",
      "2       18493                723        After both filters              58.13%\n",
      "3       13319                  0        Total rows removed              41.87%\n",
      "\n",
      "Sample of filtered data (first 5 rows):\n",
      "   PROP_ID PROP_NAME PRIMARY_COMMODITY COMMODITIES_LIST COUNTRY_NAME  \\\n",
      "0    24469     Miami            Copper           Copper          USA   \n",
      "1    24469     Miami            Copper           Copper          USA   \n",
      "2    24469     Miami            Copper           Copper          USA   \n",
      "3    24469     Miami            Copper           Copper          USA   \n",
      "4    24469     Miami            Copper           Copper          USA   \n",
      "\n",
      "   LATITUDE  LONGITUDE           DEV_STAGE ACTV_STATUS MINE_TYPE1  \\\n",
      "0    33.393   -110.897  Limited Production      Active   Open Pit   \n",
      "1    33.393   -110.897  Limited Production      Active   Open Pit   \n",
      "2    33.393   -110.897  Limited Production      Active   Open Pit   \n",
      "3    33.393   -110.897  Limited Production      Active   Open Pit   \n",
      "4    33.393   -110.897  Limited Production      Active   Open Pit   \n",
      "\n",
      "  GEOLOGIC_ORE_BODY_TYPE  Reserves_Tonnage  Reserves_Contained  Ore_Grade  \\\n",
      "0       Porphyry Deposit       498000000.0           412791.56        NaN   \n",
      "1       Porphyry Deposit       498000000.0           412791.56        NaN   \n",
      "2       Porphyry Deposit       498000000.0           412791.56        NaN   \n",
      "3       Porphyry Deposit       498000000.0           412791.56        NaN   \n",
      "4       Porphyry Deposit       498000000.0           412791.56        NaN   \n",
      "\n",
      "   START_UP_YR  PROJ_CLOSURE_YR id_cluster  Year  Production  \n",
      "0         1915             2025   H0040443  1980         NaN  \n",
      "1         1915             2025   H0040443  1981         NaN  \n",
      "2         1915             2025   H0040443  1982         NaN  \n",
      "3         1915             2025   H0040443  1983         NaN  \n",
      "4         1915             2025   H0040443  1984         NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def filter_mines_by_operation_years(input_file, create_backup=True):\n",
    "    \"\"\"\n",
    "    Filter the commodity production dataset to keep only rows where:\n",
    "    1. Year >= START_UP_YR for each mine, and\n",
    "    2. Year <= PROJ_CLOSURE_YR (if PROJ_CLOSURE_YR is not null)\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the Excel file to filter\n",
    "        create_backup (bool): Whether to create a backup of the original file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The filtered data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading data from {input_file}...\")\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(input_file)\n",
    "        \n",
    "        # Check if required columns exist\n",
    "        required_columns = ['Year', 'START_UP_YR', 'PROP_ID', 'PROJ_CLOSURE_YR']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "        \n",
    "        # Get the initial row count and unique properties\n",
    "        initial_count = len(df)\n",
    "        initial_properties = df['PROP_ID'].nunique()\n",
    "        print(f\"Initial dataset: {initial_count} rows, {initial_properties} unique properties\")\n",
    "        \n",
    "        # First filter: Keep only rows where Year >= START_UP_YR\n",
    "        filtered_startup = df[df['Year'] >= df['START_UP_YR']]\n",
    "        startup_filtered_count = len(filtered_startup)\n",
    "        startup_removed = initial_count - startup_filtered_count\n",
    "        \n",
    "        print(f\"After START_UP_YR filter: {startup_filtered_count} rows\")\n",
    "        print(f\"Removed {startup_removed} rows ({startup_removed/initial_count:.2%}) that were before START_UP_YR\")\n",
    "        \n",
    "        # Second filter: Keep only rows where Year <= PROJ_CLOSURE_YR (if PROJ_CLOSURE_YR is not null)\n",
    "        # Create a mask where either PROJ_CLOSURE_YR is NaN (meaning no closure date) \n",
    "        # or Year <= PROJ_CLOSURE_YR\n",
    "        closure_mask = filtered_startup['PROJ_CLOSURE_YR'].isna() | (filtered_startup['Year'] <= filtered_startup['PROJ_CLOSURE_YR'])\n",
    "        filtered_df = filtered_startup[closure_mask]\n",
    "        \n",
    "        # Get the filtered counts\n",
    "        filtered_count = len(filtered_df)\n",
    "        filtered_properties = filtered_df['PROP_ID'].nunique()\n",
    "        closure_removed = startup_filtered_count - filtered_count\n",
    "        total_removed = initial_count - filtered_count\n",
    "        \n",
    "        print(f\"After PROJ_CLOSURE_YR filter: {filtered_count} rows\")\n",
    "        print(f\"Removed {closure_removed} rows ({closure_removed/startup_filtered_count:.2%}) that were after PROJ_CLOSURE_YR\")\n",
    "        print(f\"Total filtered dataset: {filtered_count} rows, {filtered_properties} unique properties\")\n",
    "        print(f\"Total removed: {total_removed} rows ({total_removed/initial_count:.2%} of original data)\")\n",
    "        \n",
    "        # Save the filtered dataframe to a new file\n",
    "        output_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_long.xlsx\"\n",
    "        print(f\"Saving filtered data to {output_file}...\")\n",
    "        filtered_df.to_excel(output_file, index=False)\n",
    "        print(\"Done!\")\n",
    "        \n",
    "        # Generate a summary of data before and after filtering\n",
    "        summary = pd.DataFrame({\n",
    "            'Total Rows': [initial_count, startup_filtered_count, filtered_count, total_removed],\n",
    "            'Unique Properties': [\n",
    "                initial_properties, \n",
    "                filtered_startup['PROP_ID'].nunique(), \n",
    "                filtered_properties,\n",
    "                initial_properties - filtered_properties\n",
    "            ],\n",
    "            'Description': [\n",
    "                'Original data',\n",
    "                'After START_UP_YR filter', \n",
    "                'After both filters', \n",
    "                'Total rows removed'\n",
    "            ],\n",
    "            'Percent of Original': [\n",
    "                '100%',\n",
    "                f\"{startup_filtered_count/initial_count:.2%}\",\n",
    "                f\"{filtered_count/initial_count:.2%}\",\n",
    "                f\"{total_removed/initial_count:.2%}\"\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(summary)\n",
    "        \n",
    "        return filtered_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File path for your data\n",
    "    input_file = \"Data Output/Commodity Production/Commodity_Production-1980_2023_long.xlsx\"\n",
    "    \n",
    "    # Run the filtering function\n",
    "    filtered_data = filter_mines_by_operation_years(input_file)\n",
    "    \n",
    "    # Only display sample if data was successfully filtered\n",
    "    if filtered_data is not None:\n",
    "        print(\"\\nSample of filtered data (first 5 rows):\")\n",
    "        print(filtered_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
